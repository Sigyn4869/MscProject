{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-17T05:18:34.862359Z",
     "start_time": "2024-08-17T05:18:34.406921Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 数据读取"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "172535ad74d9e7c5"
  },
  {
   "cell_type": "code",
   "source": [
    "BH_tif_400_400 = pd.read_csv('useful_data/BH_tif_400_400.csv', header=None)\n",
    "CNM_tif_400_400 = pd.read_csv('useful_data/CNM_tif_400_400.csv', header=None)\n",
    "LAI_tif_400_400 = pd.read_csv('useful_data/LAI_tif_400_400.csv', header=None)\n",
    "DSM_tif_400_400 = pd.read_csv('useful_data/DSM_tif_400_400.csv', header=None)\n",
    "Air_Temperature_C_400_400 = pd.read_csv('useful_data/Air_Temperature_C_400_400.csv', header=None)\n",
    "Dew_Point_C_400_400 = pd.read_csv('useful_data/Dew_Point_C_400_400.csv', header=None)\n",
    "Relative_Humidity_400_400 = pd.read_csv('useful_data/Relative_Humidity_400_400.csv', header=None)\n",
    "Wind_Speed_m_s_400_400 = pd.read_csv('useful_data/Wind_Speed_kn_400_400.csv', header=None)\n",
    "weather_1795_6 = pd.read_csv('useful_data/weather_1795_6.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T05:15:05.187017Z",
     "start_time": "2024-08-17T05:15:04.998111Z"
    }
   },
   "id": "4f8001ec380311d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "BV_tif_400_400 = pd.read_csv('useful_data/BV_tif_400_400.csv', header=None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T05:15:06.042167Z",
     "start_time": "2024-08-17T05:15:06.012666Z"
    }
   },
   "id": "fa1e6c6dacdb1ab1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "               0              1              2              3    \\\n0    1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n1    1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n2    1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n3    1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n4    1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n..             ...            ...            ...            ...   \n395  1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n396  1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n397  1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n398  1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n399  1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n\n               4              5              6              7    \\\n0    1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n1    1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n2    1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n3    1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n4    1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n..             ...            ...            ...            ...   \n395  1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n396  1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n397  1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n398  1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n399  1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n\n               8              9    ...            390            391  \\\n0    1.790000e+308  1.790000e+308  ...  1.790000e+308  1.790000e+308   \n1    1.790000e+308  1.790000e+308  ...  1.790000e+308  1.790000e+308   \n2    1.790000e+308  1.790000e+308  ...  1.790000e+308   3.435001e+03   \n3    1.790000e+308  1.790000e+308  ...  1.790000e+308  1.790000e+308   \n4     1.672896e+04  1.790000e+308  ...  1.790000e+308  1.790000e+308   \n..             ...            ...  ...            ...            ...   \n395  1.790000e+308  1.790000e+308  ...  1.790000e+308  1.790000e+308   \n396  1.790000e+308  1.790000e+308  ...  1.790000e+308  1.790000e+308   \n397  1.790000e+308  1.790000e+308  ...  1.790000e+308  1.790000e+308   \n398  1.790000e+308  1.790000e+308  ...  1.790000e+308  1.790000e+308   \n399  1.790000e+308  1.790000e+308  ...  1.790000e+308  1.790000e+308   \n\n               392            393            394            395  \\\n0    1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n1    1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n2    1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n3    1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n4    1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n..             ...            ...            ...            ...   \n395  1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n396  1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n397  1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n398  1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n399  1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308   \n\n               396            397            398            399  \n0    1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308  \n1    1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308  \n2    1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308  \n3    1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308  \n4    1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308  \n..             ...            ...            ...            ...  \n395  1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308  \n396  1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308  \n397  1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308  \n398  1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308  \n399  1.790000e+308  1.790000e+308  1.790000e+308  1.790000e+308  \n\n[400 rows x 400 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>390</th>\n      <th>391</th>\n      <th>392</th>\n      <th>393</th>\n      <th>394</th>\n      <th>395</th>\n      <th>396</th>\n      <th>397</th>\n      <th>398</th>\n      <th>399</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>...</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>...</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>...</td>\n      <td>1.790000e+308</td>\n      <td>3.435001e+03</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>...</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.672896e+04</td>\n      <td>1.790000e+308</td>\n      <td>...</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>...</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>...</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>...</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>...</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>...</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n      <td>1.790000e+308</td>\n    </tr>\n  </tbody>\n</table>\n<p>400 rows × 400 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BV_tif_400_400"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:25:55.128523Z",
     "start_time": "2024-08-16T22:25:55.096765Z"
    }
   },
   "id": "320b4765fca2f40f",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "         0        1        2        3        4        5        6        7    \\\n0      181.0  65535.0  65535.0  65535.0  65535.0  65535.0  65535.0  65535.0   \n1      181.0    181.0    181.0  65535.0  65535.0  65535.0  65535.0  65535.0   \n2      181.0    181.0    181.0  65535.0  65535.0  65535.0  65535.0    312.0   \n3      181.0    181.0    181.0  65535.0  65535.0  65535.0  65535.0    312.0   \n4      181.0  65535.0  65535.0  65535.0  65535.0  65535.0    312.0    312.0   \n..       ...      ...      ...      ...      ...      ...      ...      ...   \n395  65535.0  65535.0  65535.0  65535.0  65535.0  65535.0  65535.0  65535.0   \n396  65535.0  65535.0  65535.0  65535.0  65535.0  65535.0  65535.0  65535.0   \n397  65535.0  65535.0  65535.0  65535.0  65535.0  65535.0  65535.0  65535.0   \n398  65535.0  65535.0  65535.0  65535.0  65535.0  65535.0  65535.0  65535.0   \n399  65535.0  65535.0  65535.0  65535.0  65535.0  65535.0  65535.0  65535.0   \n\n         8        9    ...      390      391      392      393      394  \\\n0    65535.0  65535.0  ...     48.0     48.0  65535.0  65535.0  65535.0   \n1    65535.0  65535.0  ...     48.0     48.0  65535.0  65535.0  65535.0   \n2      312.0    312.0  ...     48.0     48.0  65535.0  65535.0  65535.0   \n3      312.0    312.0  ...     48.0     48.0  65535.0  65535.0  65535.0   \n4      312.0    312.0  ...     48.0     48.0  65535.0  65535.0  65535.0   \n..       ...      ...  ...      ...      ...      ...      ...      ...   \n395  65535.0  65535.0  ...  65535.0  65535.0  65535.0  65535.0  65535.0   \n396  65535.0  65535.0  ...  65535.0  65535.0  65535.0  65535.0  65535.0   \n397  65535.0  65535.0  ...  65535.0  65535.0  65535.0  65535.0  65535.0   \n398  65535.0  65535.0  ...  65535.0  65535.0  65535.0  65535.0  65535.0   \n399  65535.0  65535.0  ...  65535.0  65535.0  65535.0  65535.0  65535.0   \n\n         395      396      397      398      399  \n0    65535.0  65535.0  65535.0  65535.0  65535.0  \n1    65535.0  65535.0  65535.0  65535.0  65535.0  \n2    65535.0  65535.0  65535.0  65535.0  65535.0  \n3    65535.0  65535.0  65535.0  65535.0  65535.0  \n4    65535.0  65535.0  65535.0  65535.0  65535.0  \n..       ...      ...      ...      ...      ...  \n395  65535.0  65535.0  65535.0  65535.0  65535.0  \n396  65535.0  65535.0  65535.0  65535.0  65535.0  \n397  65535.0  65535.0  65535.0  65535.0  65535.0  \n398  65535.0  65535.0  65535.0  65535.0  65535.0  \n399  65535.0  65535.0  65535.0  65535.0  65535.0  \n\n[400 rows x 400 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>390</th>\n      <th>391</th>\n      <th>392</th>\n      <th>393</th>\n      <th>394</th>\n      <th>395</th>\n      <th>396</th>\n      <th>397</th>\n      <th>398</th>\n      <th>399</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>181.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>...</td>\n      <td>48.0</td>\n      <td>48.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>181.0</td>\n      <td>181.0</td>\n      <td>181.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>...</td>\n      <td>48.0</td>\n      <td>48.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>181.0</td>\n      <td>181.0</td>\n      <td>181.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>312.0</td>\n      <td>312.0</td>\n      <td>312.0</td>\n      <td>...</td>\n      <td>48.0</td>\n      <td>48.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>181.0</td>\n      <td>181.0</td>\n      <td>181.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>312.0</td>\n      <td>312.0</td>\n      <td>312.0</td>\n      <td>...</td>\n      <td>48.0</td>\n      <td>48.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>181.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>312.0</td>\n      <td>312.0</td>\n      <td>312.0</td>\n      <td>312.0</td>\n      <td>...</td>\n      <td>48.0</td>\n      <td>48.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>...</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>...</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>...</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>...</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>...</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n      <td>65535.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>400 rows × 400 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BH_tif_400_400"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:25:55.144991Z",
     "start_time": "2024-08-16T22:25:55.129526Z"
    }
   },
   "id": "2cfaafd545105e30",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "           0          1          2          3         4          5    \\\n0    28.882736  28.408676  27.841417   2.758522  0.144400  10.691832   \n1    31.331669  30.637243  31.719954  27.747614  8.912071   8.793928   \n2    31.908613  31.804506  32.027122  29.398964  8.934540   8.833843   \n3    32.080490  31.087511  31.022667  28.868128  9.059328   9.106091   \n4    32.265759  30.051983  29.430004  28.671606  9.385540   9.447821   \n..         ...        ...        ...        ...       ...        ...   \n395   7.853981   8.364863   4.844347   7.725805  6.643098   6.228005   \n396   0.956340   6.135958   6.344071   7.694914  1.519167   0.188796   \n397   6.266613   6.323705   1.473933   0.197841  0.241845   0.285659   \n398   4.778746   1.479227   0.191199   0.190829  0.171260   7.967073   \n399   0.258677   0.151011   0.163286   7.857208  8.496970   8.306038   \n\n           6          7          8          9    ...        390        391  \\\n0     0.100922   0.106430   2.842615   2.551817  ...  11.154537  13.436749   \n1     8.830406  37.704353  10.114891   4.697506  ...  11.222446  13.472000   \n2     8.709167  37.670021  37.856037  37.932491  ...  11.245087  13.963539   \n3    37.942822  41.681610  45.117805  44.880417  ...  11.442204  13.703861   \n4    38.457752  43.525303  46.463539  44.709785  ...  11.744492  13.668510   \n..         ...        ...        ...        ...  ...        ...        ...   \n395   1.554339   1.125346   3.756186   7.582969  ...   0.149042   0.143802   \n396   1.230849   6.939294   8.895366  10.797201  ...   0.130824   0.207554   \n397   6.713283   9.136345  10.251402  11.310040  ...   0.118181   0.210786   \n398   9.116021   9.836994  10.378305  10.774410  ...   0.150107   5.015163   \n399   8.801956   9.219770   8.054457   1.710025  ...   0.174241   6.191231   \n\n           392        393        394        395        396        397  \\\n0    12.822575  14.938778  14.359684  11.296993  14.738819  15.216122   \n1    14.182007  15.719433  15.483654  12.895466  13.898914  13.888069   \n2    14.345043  17.081375  17.027512  13.588802   7.857258  10.486404   \n3    15.037663  16.741753  17.357056  13.962021  12.855732  13.410725   \n4    14.251072  16.781399  17.017300  11.888168  12.420208  13.361156   \n..         ...        ...        ...        ...        ...        ...   \n395   0.107100   0.109933   8.206887   8.274280  14.976748  18.604645   \n396   0.152059   6.976404   2.953644   7.994548  11.749454  19.201633   \n397   6.101124   3.854797   8.583788   0.183384   7.720595  17.836260   \n398   2.694671   5.216363   0.156149   0.168295   0.198293  12.996392   \n399   1.974123   5.390947   0.134482   0.100617   0.100279   0.083546   \n\n           398        399  \n0    14.058662   0.085865  \n1    12.631687   0.074139  \n2     9.954441   0.056244  \n3    12.163933   0.053967  \n4    11.836220   5.762657  \n..         ...        ...  \n395  16.940056  17.300076  \n396  18.808081  16.842148  \n397  18.706511  17.208393  \n398  14.958178  18.356520  \n399   0.134665  12.975633  \n\n[400 rows x 400 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>390</th>\n      <th>391</th>\n      <th>392</th>\n      <th>393</th>\n      <th>394</th>\n      <th>395</th>\n      <th>396</th>\n      <th>397</th>\n      <th>398</th>\n      <th>399</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>28.882736</td>\n      <td>28.408676</td>\n      <td>27.841417</td>\n      <td>2.758522</td>\n      <td>0.144400</td>\n      <td>10.691832</td>\n      <td>0.100922</td>\n      <td>0.106430</td>\n      <td>2.842615</td>\n      <td>2.551817</td>\n      <td>...</td>\n      <td>11.154537</td>\n      <td>13.436749</td>\n      <td>12.822575</td>\n      <td>14.938778</td>\n      <td>14.359684</td>\n      <td>11.296993</td>\n      <td>14.738819</td>\n      <td>15.216122</td>\n      <td>14.058662</td>\n      <td>0.085865</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>31.331669</td>\n      <td>30.637243</td>\n      <td>31.719954</td>\n      <td>27.747614</td>\n      <td>8.912071</td>\n      <td>8.793928</td>\n      <td>8.830406</td>\n      <td>37.704353</td>\n      <td>10.114891</td>\n      <td>4.697506</td>\n      <td>...</td>\n      <td>11.222446</td>\n      <td>13.472000</td>\n      <td>14.182007</td>\n      <td>15.719433</td>\n      <td>15.483654</td>\n      <td>12.895466</td>\n      <td>13.898914</td>\n      <td>13.888069</td>\n      <td>12.631687</td>\n      <td>0.074139</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>31.908613</td>\n      <td>31.804506</td>\n      <td>32.027122</td>\n      <td>29.398964</td>\n      <td>8.934540</td>\n      <td>8.833843</td>\n      <td>8.709167</td>\n      <td>37.670021</td>\n      <td>37.856037</td>\n      <td>37.932491</td>\n      <td>...</td>\n      <td>11.245087</td>\n      <td>13.963539</td>\n      <td>14.345043</td>\n      <td>17.081375</td>\n      <td>17.027512</td>\n      <td>13.588802</td>\n      <td>7.857258</td>\n      <td>10.486404</td>\n      <td>9.954441</td>\n      <td>0.056244</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>32.080490</td>\n      <td>31.087511</td>\n      <td>31.022667</td>\n      <td>28.868128</td>\n      <td>9.059328</td>\n      <td>9.106091</td>\n      <td>37.942822</td>\n      <td>41.681610</td>\n      <td>45.117805</td>\n      <td>44.880417</td>\n      <td>...</td>\n      <td>11.442204</td>\n      <td>13.703861</td>\n      <td>15.037663</td>\n      <td>16.741753</td>\n      <td>17.357056</td>\n      <td>13.962021</td>\n      <td>12.855732</td>\n      <td>13.410725</td>\n      <td>12.163933</td>\n      <td>0.053967</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>32.265759</td>\n      <td>30.051983</td>\n      <td>29.430004</td>\n      <td>28.671606</td>\n      <td>9.385540</td>\n      <td>9.447821</td>\n      <td>38.457752</td>\n      <td>43.525303</td>\n      <td>46.463539</td>\n      <td>44.709785</td>\n      <td>...</td>\n      <td>11.744492</td>\n      <td>13.668510</td>\n      <td>14.251072</td>\n      <td>16.781399</td>\n      <td>17.017300</td>\n      <td>11.888168</td>\n      <td>12.420208</td>\n      <td>13.361156</td>\n      <td>11.836220</td>\n      <td>5.762657</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>7.853981</td>\n      <td>8.364863</td>\n      <td>4.844347</td>\n      <td>7.725805</td>\n      <td>6.643098</td>\n      <td>6.228005</td>\n      <td>1.554339</td>\n      <td>1.125346</td>\n      <td>3.756186</td>\n      <td>7.582969</td>\n      <td>...</td>\n      <td>0.149042</td>\n      <td>0.143802</td>\n      <td>0.107100</td>\n      <td>0.109933</td>\n      <td>8.206887</td>\n      <td>8.274280</td>\n      <td>14.976748</td>\n      <td>18.604645</td>\n      <td>16.940056</td>\n      <td>17.300076</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>0.956340</td>\n      <td>6.135958</td>\n      <td>6.344071</td>\n      <td>7.694914</td>\n      <td>1.519167</td>\n      <td>0.188796</td>\n      <td>1.230849</td>\n      <td>6.939294</td>\n      <td>8.895366</td>\n      <td>10.797201</td>\n      <td>...</td>\n      <td>0.130824</td>\n      <td>0.207554</td>\n      <td>0.152059</td>\n      <td>6.976404</td>\n      <td>2.953644</td>\n      <td>7.994548</td>\n      <td>11.749454</td>\n      <td>19.201633</td>\n      <td>18.808081</td>\n      <td>16.842148</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>6.266613</td>\n      <td>6.323705</td>\n      <td>1.473933</td>\n      <td>0.197841</td>\n      <td>0.241845</td>\n      <td>0.285659</td>\n      <td>6.713283</td>\n      <td>9.136345</td>\n      <td>10.251402</td>\n      <td>11.310040</td>\n      <td>...</td>\n      <td>0.118181</td>\n      <td>0.210786</td>\n      <td>6.101124</td>\n      <td>3.854797</td>\n      <td>8.583788</td>\n      <td>0.183384</td>\n      <td>7.720595</td>\n      <td>17.836260</td>\n      <td>18.706511</td>\n      <td>17.208393</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>4.778746</td>\n      <td>1.479227</td>\n      <td>0.191199</td>\n      <td>0.190829</td>\n      <td>0.171260</td>\n      <td>7.967073</td>\n      <td>9.116021</td>\n      <td>9.836994</td>\n      <td>10.378305</td>\n      <td>10.774410</td>\n      <td>...</td>\n      <td>0.150107</td>\n      <td>5.015163</td>\n      <td>2.694671</td>\n      <td>5.216363</td>\n      <td>0.156149</td>\n      <td>0.168295</td>\n      <td>0.198293</td>\n      <td>12.996392</td>\n      <td>14.958178</td>\n      <td>18.356520</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>0.258677</td>\n      <td>0.151011</td>\n      <td>0.163286</td>\n      <td>7.857208</td>\n      <td>8.496970</td>\n      <td>8.306038</td>\n      <td>8.801956</td>\n      <td>9.219770</td>\n      <td>8.054457</td>\n      <td>1.710025</td>\n      <td>...</td>\n      <td>0.174241</td>\n      <td>6.191231</td>\n      <td>1.974123</td>\n      <td>5.390947</td>\n      <td>0.134482</td>\n      <td>0.100617</td>\n      <td>0.100279</td>\n      <td>0.083546</td>\n      <td>0.134665</td>\n      <td>12.975633</td>\n    </tr>\n  </tbody>\n</table>\n<p>400 rows × 400 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNM_tif_400_400"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:25:55.156110Z",
     "start_time": "2024-08-16T22:25:55.146084Z"
    }
   },
   "id": "8e1aa0f3aa75bbb8",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "           0          1          2          3         4         5    \\\n0    11.703094  11.513471  11.286567   1.253409  0.207760  4.426733   \n1    12.682668  12.404897  12.837981  11.249045  3.714829  3.667571   \n2    12.913445  12.871802  12.960849  11.909585  3.723816  3.683537   \n3    12.982196  12.585004  12.559067  11.697251  3.773731  3.792436   \n4    13.056303  12.170793  11.922002  11.618642  3.904216  3.929128   \n..         ...        ...        ...        ...       ...       ...   \n395   3.291593   3.495945   2.087739   3.240322  2.807239  2.641202   \n396   0.532536   2.604383   2.687629   3.227966  0.757667  0.225518   \n397   2.656645   2.679482   0.739573   0.229136  0.246738  0.264264   \n398   2.061498   0.741691   0.226480   0.226332  0.218504  3.336829   \n399   0.253471   0.210405   0.215314   3.292883  3.548788  3.472415   \n\n           6          7          8          9    ...       390       391  \\\n0     0.190369   0.192572   1.287046   1.170727  ...  4.611815  5.524700   \n1     3.682163  15.231741   4.195957   2.029002  ...  4.638979  5.538800   \n2     3.633667  15.218008  15.292415  15.322996  ...  4.648035  5.735416   \n3    15.327128  16.822643  18.197123  18.102167  ...  4.726882  5.631545   \n4    15.533101  17.560122  18.735416  18.033915  ...  4.847797  5.617404   \n..         ...        ...        ...        ...  ...       ...       ...   \n395   0.771736   0.600138   1.652475   3.183188  ...  0.209617  0.207521   \n396   0.642340   2.925718   3.708146   4.468881  ...  0.202330  0.233022   \n397   2.835313   3.804538   4.250561   4.674016  ...  0.197272  0.234314   \n398   3.796409   4.084798   4.301322   4.459764  ...  0.210043  2.156065   \n399   3.670783   3.837908   3.371783   0.834010  ...  0.219696  2.626493   \n\n          392       393       394       395       396       397       398  \\\n0    5.279030  6.125511  5.893874  4.668797  6.045528  6.236449  5.773465   \n1    5.822803  6.437773  6.343462  5.308187  5.709566  5.705228  5.202675   \n2    5.888018  6.982550  6.961005  5.585521  3.292903  4.344562  4.131776   \n3    6.165065  6.846701  7.092823  5.734808  5.292293  5.514290  5.015574   \n4    5.850429  6.862560  6.956920  4.905268  5.118083  5.494463  4.884488   \n..        ...       ...       ...       ...       ...       ...       ...   \n395  0.192840  0.193973  3.432755  3.459712  6.140699  7.591858  6.926023   \n396  0.210823  2.940562  1.331458  3.347820  4.849782  7.830654  7.673233   \n397  2.590450  1.691919  3.583515  0.223354  3.238238  7.284504  7.632605   \n398  1.227868  2.236545  0.212460  0.217318  0.229317  5.348557  6.133271   \n399  0.939649  2.306379  0.203793  0.190247  0.190111  0.183418  0.203866   \n\n          399  \n0    0.184346  \n1    0.179655  \n2    0.172498  \n3    0.171587  \n4    2.455063  \n..        ...  \n395  7.070030  \n396  6.886859  \n397  7.033358  \n398  7.492608  \n399  5.340253  \n\n[400 rows x 400 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>390</th>\n      <th>391</th>\n      <th>392</th>\n      <th>393</th>\n      <th>394</th>\n      <th>395</th>\n      <th>396</th>\n      <th>397</th>\n      <th>398</th>\n      <th>399</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11.703094</td>\n      <td>11.513471</td>\n      <td>11.286567</td>\n      <td>1.253409</td>\n      <td>0.207760</td>\n      <td>4.426733</td>\n      <td>0.190369</td>\n      <td>0.192572</td>\n      <td>1.287046</td>\n      <td>1.170727</td>\n      <td>...</td>\n      <td>4.611815</td>\n      <td>5.524700</td>\n      <td>5.279030</td>\n      <td>6.125511</td>\n      <td>5.893874</td>\n      <td>4.668797</td>\n      <td>6.045528</td>\n      <td>6.236449</td>\n      <td>5.773465</td>\n      <td>0.184346</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12.682668</td>\n      <td>12.404897</td>\n      <td>12.837981</td>\n      <td>11.249045</td>\n      <td>3.714829</td>\n      <td>3.667571</td>\n      <td>3.682163</td>\n      <td>15.231741</td>\n      <td>4.195957</td>\n      <td>2.029002</td>\n      <td>...</td>\n      <td>4.638979</td>\n      <td>5.538800</td>\n      <td>5.822803</td>\n      <td>6.437773</td>\n      <td>6.343462</td>\n      <td>5.308187</td>\n      <td>5.709566</td>\n      <td>5.705228</td>\n      <td>5.202675</td>\n      <td>0.179655</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>12.913445</td>\n      <td>12.871802</td>\n      <td>12.960849</td>\n      <td>11.909585</td>\n      <td>3.723816</td>\n      <td>3.683537</td>\n      <td>3.633667</td>\n      <td>15.218008</td>\n      <td>15.292415</td>\n      <td>15.322996</td>\n      <td>...</td>\n      <td>4.648035</td>\n      <td>5.735416</td>\n      <td>5.888018</td>\n      <td>6.982550</td>\n      <td>6.961005</td>\n      <td>5.585521</td>\n      <td>3.292903</td>\n      <td>4.344562</td>\n      <td>4.131776</td>\n      <td>0.172498</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>12.982196</td>\n      <td>12.585004</td>\n      <td>12.559067</td>\n      <td>11.697251</td>\n      <td>3.773731</td>\n      <td>3.792436</td>\n      <td>15.327128</td>\n      <td>16.822643</td>\n      <td>18.197123</td>\n      <td>18.102167</td>\n      <td>...</td>\n      <td>4.726882</td>\n      <td>5.631545</td>\n      <td>6.165065</td>\n      <td>6.846701</td>\n      <td>7.092823</td>\n      <td>5.734808</td>\n      <td>5.292293</td>\n      <td>5.514290</td>\n      <td>5.015574</td>\n      <td>0.171587</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13.056303</td>\n      <td>12.170793</td>\n      <td>11.922002</td>\n      <td>11.618642</td>\n      <td>3.904216</td>\n      <td>3.929128</td>\n      <td>15.533101</td>\n      <td>17.560122</td>\n      <td>18.735416</td>\n      <td>18.033915</td>\n      <td>...</td>\n      <td>4.847797</td>\n      <td>5.617404</td>\n      <td>5.850429</td>\n      <td>6.862560</td>\n      <td>6.956920</td>\n      <td>4.905268</td>\n      <td>5.118083</td>\n      <td>5.494463</td>\n      <td>4.884488</td>\n      <td>2.455063</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>3.291593</td>\n      <td>3.495945</td>\n      <td>2.087739</td>\n      <td>3.240322</td>\n      <td>2.807239</td>\n      <td>2.641202</td>\n      <td>0.771736</td>\n      <td>0.600138</td>\n      <td>1.652475</td>\n      <td>3.183188</td>\n      <td>...</td>\n      <td>0.209617</td>\n      <td>0.207521</td>\n      <td>0.192840</td>\n      <td>0.193973</td>\n      <td>3.432755</td>\n      <td>3.459712</td>\n      <td>6.140699</td>\n      <td>7.591858</td>\n      <td>6.926023</td>\n      <td>7.070030</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>0.532536</td>\n      <td>2.604383</td>\n      <td>2.687629</td>\n      <td>3.227966</td>\n      <td>0.757667</td>\n      <td>0.225518</td>\n      <td>0.642340</td>\n      <td>2.925718</td>\n      <td>3.708146</td>\n      <td>4.468881</td>\n      <td>...</td>\n      <td>0.202330</td>\n      <td>0.233022</td>\n      <td>0.210823</td>\n      <td>2.940562</td>\n      <td>1.331458</td>\n      <td>3.347820</td>\n      <td>4.849782</td>\n      <td>7.830654</td>\n      <td>7.673233</td>\n      <td>6.886859</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>2.656645</td>\n      <td>2.679482</td>\n      <td>0.739573</td>\n      <td>0.229136</td>\n      <td>0.246738</td>\n      <td>0.264264</td>\n      <td>2.835313</td>\n      <td>3.804538</td>\n      <td>4.250561</td>\n      <td>4.674016</td>\n      <td>...</td>\n      <td>0.197272</td>\n      <td>0.234314</td>\n      <td>2.590450</td>\n      <td>1.691919</td>\n      <td>3.583515</td>\n      <td>0.223354</td>\n      <td>3.238238</td>\n      <td>7.284504</td>\n      <td>7.632605</td>\n      <td>7.033358</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>2.061498</td>\n      <td>0.741691</td>\n      <td>0.226480</td>\n      <td>0.226332</td>\n      <td>0.218504</td>\n      <td>3.336829</td>\n      <td>3.796409</td>\n      <td>4.084798</td>\n      <td>4.301322</td>\n      <td>4.459764</td>\n      <td>...</td>\n      <td>0.210043</td>\n      <td>2.156065</td>\n      <td>1.227868</td>\n      <td>2.236545</td>\n      <td>0.212460</td>\n      <td>0.217318</td>\n      <td>0.229317</td>\n      <td>5.348557</td>\n      <td>6.133271</td>\n      <td>7.492608</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>0.253471</td>\n      <td>0.210405</td>\n      <td>0.215314</td>\n      <td>3.292883</td>\n      <td>3.548788</td>\n      <td>3.472415</td>\n      <td>3.670783</td>\n      <td>3.837908</td>\n      <td>3.371783</td>\n      <td>0.834010</td>\n      <td>...</td>\n      <td>0.219696</td>\n      <td>2.626493</td>\n      <td>0.939649</td>\n      <td>2.306379</td>\n      <td>0.203793</td>\n      <td>0.190247</td>\n      <td>0.190111</td>\n      <td>0.183418</td>\n      <td>0.203866</td>\n      <td>5.340253</td>\n    </tr>\n  </tbody>\n</table>\n<p>400 rows × 400 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAI_tif_400_400"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:25:55.167139Z",
     "start_time": "2024-08-16T22:25:55.157115Z"
    }
   },
   "id": "37a040c3d0fb30aa",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "           0          1          2          3          4          5    \\\n0    55.240002  54.737000  54.116001  28.997999  26.305000  36.851002   \n1    57.542999  56.951000  58.069000  53.936001  35.202000  35.173000   \n2    57.914001  57.915001  58.180000  55.446999  35.180000  35.118000   \n3    57.902000  56.983002  57.056999  54.966999  35.151001  35.153999   \n4    57.896000  55.709999  55.188999  54.411999  35.167000  35.139000   \n..         ...        ...        ...        ...        ...        ...   \n395  20.770000  21.219000  17.767000  20.597000  19.320000  19.219000   \n396  13.817000  18.978001  19.076000  20.559999  15.870000  14.533000   \n397  19.027000  19.032000  15.773000  14.468000  14.498000  14.697000   \n398  19.003000  15.733000  14.537000  14.563000  14.663000  22.521999   \n399  14.480000  14.572000  14.617000  22.417000  23.125000  22.844000   \n\n           6          7          8          9    ...        390        391  \\\n0    26.337000  26.381001  29.226000  28.996000  ...  54.425999  56.618000   \n1    35.178001  64.069000  36.485001  31.046000  ...  54.401001  56.634998   \n2    35.161999  64.047997  64.017998  64.115997  ...  54.452999  57.066002   \n3    64.021004  67.403000  70.310997  69.667999  ...  54.480999  56.620998   \n4    64.057999  68.619003  70.752998  68.504997  ...  54.573002  56.277000   \n..         ...        ...        ...        ...  ...        ...        ...   \n395  15.900000  15.523000  18.162001  22.080000  ...   1.939000   1.929000   \n396  15.565000  21.305000  23.405001  25.334999  ...   1.928000   1.969000   \n397  21.311001  23.761000  24.823000  25.846001  ...   1.900000   1.935000   \n398  23.721001  24.323000  24.785000  25.090000  ...   1.918000   6.753000   \n399  23.247000  23.603001  22.426001  16.091000  ...   1.951000   7.976000   \n\n           392        393        394        395        396        397  \\\n0    55.994999  58.116001  57.266998  54.292999  58.080002  58.844002   \n1    57.306999  58.853001  58.251999  55.695000  56.910000  57.408001   \n2    57.313999  59.846001  59.633999  56.175999  50.634998  53.714001   \n3    57.842999  59.458000  59.789001  56.402000  55.410000  56.500999   \n4    56.844002  59.401001  59.251999  54.109001  54.824001  56.171001   \n..         ...        ...        ...        ...        ...        ...   \n395   1.901000   1.919000  10.614000  11.532000  17.280001  22.101000   \n396   1.945000   8.757000   9.386000   9.779000  13.541000  21.530001   \n397   7.889000  10.258000  10.389000   1.979000   9.516000  19.636000   \n398   7.966000   7.975000   1.925000   1.981000   1.988000  14.830000   \n399   8.014000   7.206000   1.922000   1.918000   1.892000   1.903000   \n\n           398        399  \n0    57.251999  43.179001  \n1    55.799999  43.125000  \n2    53.067001  43.102001  \n3    55.379002  43.084000  \n4    54.978001  48.729000  \n..         ...        ...  \n395  21.753000  22.700001  \n396  22.023001  21.482000  \n397  20.577999  20.239000  \n398  16.715000  20.174000  \n399   1.944000  14.771000  \n\n[400 rows x 400 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>390</th>\n      <th>391</th>\n      <th>392</th>\n      <th>393</th>\n      <th>394</th>\n      <th>395</th>\n      <th>396</th>\n      <th>397</th>\n      <th>398</th>\n      <th>399</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>55.240002</td>\n      <td>54.737000</td>\n      <td>54.116001</td>\n      <td>28.997999</td>\n      <td>26.305000</td>\n      <td>36.851002</td>\n      <td>26.337000</td>\n      <td>26.381001</td>\n      <td>29.226000</td>\n      <td>28.996000</td>\n      <td>...</td>\n      <td>54.425999</td>\n      <td>56.618000</td>\n      <td>55.994999</td>\n      <td>58.116001</td>\n      <td>57.266998</td>\n      <td>54.292999</td>\n      <td>58.080002</td>\n      <td>58.844002</td>\n      <td>57.251999</td>\n      <td>43.179001</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>57.542999</td>\n      <td>56.951000</td>\n      <td>58.069000</td>\n      <td>53.936001</td>\n      <td>35.202000</td>\n      <td>35.173000</td>\n      <td>35.178001</td>\n      <td>64.069000</td>\n      <td>36.485001</td>\n      <td>31.046000</td>\n      <td>...</td>\n      <td>54.401001</td>\n      <td>56.634998</td>\n      <td>57.306999</td>\n      <td>58.853001</td>\n      <td>58.251999</td>\n      <td>55.695000</td>\n      <td>56.910000</td>\n      <td>57.408001</td>\n      <td>55.799999</td>\n      <td>43.125000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>57.914001</td>\n      <td>57.915001</td>\n      <td>58.180000</td>\n      <td>55.446999</td>\n      <td>35.180000</td>\n      <td>35.118000</td>\n      <td>35.161999</td>\n      <td>64.047997</td>\n      <td>64.017998</td>\n      <td>64.115997</td>\n      <td>...</td>\n      <td>54.452999</td>\n      <td>57.066002</td>\n      <td>57.313999</td>\n      <td>59.846001</td>\n      <td>59.633999</td>\n      <td>56.175999</td>\n      <td>50.634998</td>\n      <td>53.714001</td>\n      <td>53.067001</td>\n      <td>43.102001</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>57.902000</td>\n      <td>56.983002</td>\n      <td>57.056999</td>\n      <td>54.966999</td>\n      <td>35.151001</td>\n      <td>35.153999</td>\n      <td>64.021004</td>\n      <td>67.403000</td>\n      <td>70.310997</td>\n      <td>69.667999</td>\n      <td>...</td>\n      <td>54.480999</td>\n      <td>56.620998</td>\n      <td>57.842999</td>\n      <td>59.458000</td>\n      <td>59.789001</td>\n      <td>56.402000</td>\n      <td>55.410000</td>\n      <td>56.500999</td>\n      <td>55.379002</td>\n      <td>43.084000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>57.896000</td>\n      <td>55.709999</td>\n      <td>55.188999</td>\n      <td>54.411999</td>\n      <td>35.167000</td>\n      <td>35.139000</td>\n      <td>64.057999</td>\n      <td>68.619003</td>\n      <td>70.752998</td>\n      <td>68.504997</td>\n      <td>...</td>\n      <td>54.573002</td>\n      <td>56.277000</td>\n      <td>56.844002</td>\n      <td>59.401001</td>\n      <td>59.251999</td>\n      <td>54.109001</td>\n      <td>54.824001</td>\n      <td>56.171001</td>\n      <td>54.978001</td>\n      <td>48.729000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>20.770000</td>\n      <td>21.219000</td>\n      <td>17.767000</td>\n      <td>20.597000</td>\n      <td>19.320000</td>\n      <td>19.219000</td>\n      <td>15.900000</td>\n      <td>15.523000</td>\n      <td>18.162001</td>\n      <td>22.080000</td>\n      <td>...</td>\n      <td>1.939000</td>\n      <td>1.929000</td>\n      <td>1.901000</td>\n      <td>1.919000</td>\n      <td>10.614000</td>\n      <td>11.532000</td>\n      <td>17.280001</td>\n      <td>22.101000</td>\n      <td>21.753000</td>\n      <td>22.700001</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>13.817000</td>\n      <td>18.978001</td>\n      <td>19.076000</td>\n      <td>20.559999</td>\n      <td>15.870000</td>\n      <td>14.533000</td>\n      <td>15.565000</td>\n      <td>21.305000</td>\n      <td>23.405001</td>\n      <td>25.334999</td>\n      <td>...</td>\n      <td>1.928000</td>\n      <td>1.969000</td>\n      <td>1.945000</td>\n      <td>8.757000</td>\n      <td>9.386000</td>\n      <td>9.779000</td>\n      <td>13.541000</td>\n      <td>21.530001</td>\n      <td>22.023001</td>\n      <td>21.482000</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>19.027000</td>\n      <td>19.032000</td>\n      <td>15.773000</td>\n      <td>14.468000</td>\n      <td>14.498000</td>\n      <td>14.697000</td>\n      <td>21.311001</td>\n      <td>23.761000</td>\n      <td>24.823000</td>\n      <td>25.846001</td>\n      <td>...</td>\n      <td>1.900000</td>\n      <td>1.935000</td>\n      <td>7.889000</td>\n      <td>10.258000</td>\n      <td>10.389000</td>\n      <td>1.979000</td>\n      <td>9.516000</td>\n      <td>19.636000</td>\n      <td>20.577999</td>\n      <td>20.239000</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>19.003000</td>\n      <td>15.733000</td>\n      <td>14.537000</td>\n      <td>14.563000</td>\n      <td>14.663000</td>\n      <td>22.521999</td>\n      <td>23.721001</td>\n      <td>24.323000</td>\n      <td>24.785000</td>\n      <td>25.090000</td>\n      <td>...</td>\n      <td>1.918000</td>\n      <td>6.753000</td>\n      <td>7.966000</td>\n      <td>7.975000</td>\n      <td>1.925000</td>\n      <td>1.981000</td>\n      <td>1.988000</td>\n      <td>14.830000</td>\n      <td>16.715000</td>\n      <td>20.174000</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>14.480000</td>\n      <td>14.572000</td>\n      <td>14.617000</td>\n      <td>22.417000</td>\n      <td>23.125000</td>\n      <td>22.844000</td>\n      <td>23.247000</td>\n      <td>23.603001</td>\n      <td>22.426001</td>\n      <td>16.091000</td>\n      <td>...</td>\n      <td>1.951000</td>\n      <td>7.976000</td>\n      <td>8.014000</td>\n      <td>7.206000</td>\n      <td>1.922000</td>\n      <td>1.918000</td>\n      <td>1.892000</td>\n      <td>1.903000</td>\n      <td>1.944000</td>\n      <td>14.771000</td>\n    </tr>\n  </tbody>\n</table>\n<p>400 rows × 400 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DSM_tif_400_400"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:25:55.178766Z",
     "start_time": "2024-08-16T22:25:55.168143Z"
    }
   },
   "id": "2f4dddc69c828205",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "           0          1          2          3          4          5    \\\n0    14.600371  14.600463  14.600558  14.600656  14.600757  14.600861   \n1    14.600907  14.600999  14.601094  14.601191  14.601292  14.601395   \n2    14.601441  14.601532  14.601626  14.601723  14.601823  14.601926   \n3    14.601973  14.602063  14.602157  14.602253  14.602352  14.602454   \n4    14.602502  14.602592  14.602684  14.602780  14.602879  14.602980   \n..         ...        ...        ...        ...        ...        ...   \n395  14.641298  14.641528  14.641759  14.641991  14.642225  14.642461   \n396  14.641077  14.641307  14.641539  14.641773  14.642008  14.642245   \n397  14.640854  14.641086  14.641319  14.641554  14.641790  14.642028   \n398  14.640631  14.640863  14.641098  14.641334  14.641571  14.641810   \n399  14.640406  14.640640  14.640876  14.641113  14.641351  14.641591   \n\n           6          7          8          9    ...        390        391  \\\n0    14.600968  14.601077  14.601190  14.601305  ...  14.809819  14.810692   \n1    14.601501  14.601609  14.601721  14.601835  ...  14.809988  14.810860   \n2    14.602031  14.602139  14.602250  14.602364  ...  14.810156  14.811027   \n3    14.602559  14.602666  14.602777  14.602890  ...  14.810324  14.811193   \n4    14.603084  14.603191  14.603301  14.603413  ...  14.810490  14.811359   \n..         ...        ...        ...        ...  ...        ...        ...   \n395  14.642698  14.642936  14.643176  14.643418  ...  14.822261  14.822910   \n396  14.642483  14.642722  14.642963  14.643206  ...  14.822184  14.822833   \n397  14.642267  14.642507  14.642749  14.642993  ...  14.822107  14.822756   \n398  14.642050  14.642291  14.642535  14.642779  ...  14.822030  14.822679   \n399  14.641832  14.642075  14.642319  14.642564  ...  14.821952  14.822601   \n\n           392        393        394        395        396        397  \\\n0    14.811567  14.812442  14.813318  14.814196  14.815075  14.815955   \n1    14.811733  14.812607  14.813483  14.814359  14.815237  14.816115   \n2    14.811899  14.812772  14.813646  14.814521  14.815398  14.816275   \n3    14.812064  14.812936  14.813809  14.814683  14.815558  14.816435   \n4    14.812228  14.813099  14.813971  14.814844  14.815718  14.816593   \n..         ...        ...        ...        ...        ...        ...   \n395  14.823560  14.824210  14.824862  14.825513  14.826166  14.826819   \n396  14.823483  14.824133  14.824784  14.825436  14.826088  14.826742   \n397  14.823406  14.824056  14.824707  14.825358  14.826011  14.826664   \n398  14.823328  14.823978  14.824629  14.825280  14.825932  14.826585   \n399  14.823250  14.823900  14.824551  14.825202  14.825854  14.826507   \n\n           398        399  \n0    14.816835  14.817718  \n1    14.816995  14.817876  \n2    14.817154  14.818034  \n3    14.817312  14.818191  \n4    14.817470  14.818347  \n..         ...        ...  \n395  14.827473  14.828128  \n396  14.827395  14.828050  \n397  14.827317  14.827971  \n398  14.827239  14.827893  \n399  14.827160  14.827814  \n\n[400 rows x 400 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>390</th>\n      <th>391</th>\n      <th>392</th>\n      <th>393</th>\n      <th>394</th>\n      <th>395</th>\n      <th>396</th>\n      <th>397</th>\n      <th>398</th>\n      <th>399</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14.600371</td>\n      <td>14.600463</td>\n      <td>14.600558</td>\n      <td>14.600656</td>\n      <td>14.600757</td>\n      <td>14.600861</td>\n      <td>14.600968</td>\n      <td>14.601077</td>\n      <td>14.601190</td>\n      <td>14.601305</td>\n      <td>...</td>\n      <td>14.809819</td>\n      <td>14.810692</td>\n      <td>14.811567</td>\n      <td>14.812442</td>\n      <td>14.813318</td>\n      <td>14.814196</td>\n      <td>14.815075</td>\n      <td>14.815955</td>\n      <td>14.816835</td>\n      <td>14.817718</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14.600907</td>\n      <td>14.600999</td>\n      <td>14.601094</td>\n      <td>14.601191</td>\n      <td>14.601292</td>\n      <td>14.601395</td>\n      <td>14.601501</td>\n      <td>14.601609</td>\n      <td>14.601721</td>\n      <td>14.601835</td>\n      <td>...</td>\n      <td>14.809988</td>\n      <td>14.810860</td>\n      <td>14.811733</td>\n      <td>14.812607</td>\n      <td>14.813483</td>\n      <td>14.814359</td>\n      <td>14.815237</td>\n      <td>14.816115</td>\n      <td>14.816995</td>\n      <td>14.817876</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>14.601441</td>\n      <td>14.601532</td>\n      <td>14.601626</td>\n      <td>14.601723</td>\n      <td>14.601823</td>\n      <td>14.601926</td>\n      <td>14.602031</td>\n      <td>14.602139</td>\n      <td>14.602250</td>\n      <td>14.602364</td>\n      <td>...</td>\n      <td>14.810156</td>\n      <td>14.811027</td>\n      <td>14.811899</td>\n      <td>14.812772</td>\n      <td>14.813646</td>\n      <td>14.814521</td>\n      <td>14.815398</td>\n      <td>14.816275</td>\n      <td>14.817154</td>\n      <td>14.818034</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.601973</td>\n      <td>14.602063</td>\n      <td>14.602157</td>\n      <td>14.602253</td>\n      <td>14.602352</td>\n      <td>14.602454</td>\n      <td>14.602559</td>\n      <td>14.602666</td>\n      <td>14.602777</td>\n      <td>14.602890</td>\n      <td>...</td>\n      <td>14.810324</td>\n      <td>14.811193</td>\n      <td>14.812064</td>\n      <td>14.812936</td>\n      <td>14.813809</td>\n      <td>14.814683</td>\n      <td>14.815558</td>\n      <td>14.816435</td>\n      <td>14.817312</td>\n      <td>14.818191</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>14.602502</td>\n      <td>14.602592</td>\n      <td>14.602684</td>\n      <td>14.602780</td>\n      <td>14.602879</td>\n      <td>14.602980</td>\n      <td>14.603084</td>\n      <td>14.603191</td>\n      <td>14.603301</td>\n      <td>14.603413</td>\n      <td>...</td>\n      <td>14.810490</td>\n      <td>14.811359</td>\n      <td>14.812228</td>\n      <td>14.813099</td>\n      <td>14.813971</td>\n      <td>14.814844</td>\n      <td>14.815718</td>\n      <td>14.816593</td>\n      <td>14.817470</td>\n      <td>14.818347</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>14.641298</td>\n      <td>14.641528</td>\n      <td>14.641759</td>\n      <td>14.641991</td>\n      <td>14.642225</td>\n      <td>14.642461</td>\n      <td>14.642698</td>\n      <td>14.642936</td>\n      <td>14.643176</td>\n      <td>14.643418</td>\n      <td>...</td>\n      <td>14.822261</td>\n      <td>14.822910</td>\n      <td>14.823560</td>\n      <td>14.824210</td>\n      <td>14.824862</td>\n      <td>14.825513</td>\n      <td>14.826166</td>\n      <td>14.826819</td>\n      <td>14.827473</td>\n      <td>14.828128</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>14.641077</td>\n      <td>14.641307</td>\n      <td>14.641539</td>\n      <td>14.641773</td>\n      <td>14.642008</td>\n      <td>14.642245</td>\n      <td>14.642483</td>\n      <td>14.642722</td>\n      <td>14.642963</td>\n      <td>14.643206</td>\n      <td>...</td>\n      <td>14.822184</td>\n      <td>14.822833</td>\n      <td>14.823483</td>\n      <td>14.824133</td>\n      <td>14.824784</td>\n      <td>14.825436</td>\n      <td>14.826088</td>\n      <td>14.826742</td>\n      <td>14.827395</td>\n      <td>14.828050</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>14.640854</td>\n      <td>14.641086</td>\n      <td>14.641319</td>\n      <td>14.641554</td>\n      <td>14.641790</td>\n      <td>14.642028</td>\n      <td>14.642267</td>\n      <td>14.642507</td>\n      <td>14.642749</td>\n      <td>14.642993</td>\n      <td>...</td>\n      <td>14.822107</td>\n      <td>14.822756</td>\n      <td>14.823406</td>\n      <td>14.824056</td>\n      <td>14.824707</td>\n      <td>14.825358</td>\n      <td>14.826011</td>\n      <td>14.826664</td>\n      <td>14.827317</td>\n      <td>14.827971</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>14.640631</td>\n      <td>14.640863</td>\n      <td>14.641098</td>\n      <td>14.641334</td>\n      <td>14.641571</td>\n      <td>14.641810</td>\n      <td>14.642050</td>\n      <td>14.642291</td>\n      <td>14.642535</td>\n      <td>14.642779</td>\n      <td>...</td>\n      <td>14.822030</td>\n      <td>14.822679</td>\n      <td>14.823328</td>\n      <td>14.823978</td>\n      <td>14.824629</td>\n      <td>14.825280</td>\n      <td>14.825932</td>\n      <td>14.826585</td>\n      <td>14.827239</td>\n      <td>14.827893</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>14.640406</td>\n      <td>14.640640</td>\n      <td>14.640876</td>\n      <td>14.641113</td>\n      <td>14.641351</td>\n      <td>14.641591</td>\n      <td>14.641832</td>\n      <td>14.642075</td>\n      <td>14.642319</td>\n      <td>14.642564</td>\n      <td>...</td>\n      <td>14.821952</td>\n      <td>14.822601</td>\n      <td>14.823250</td>\n      <td>14.823900</td>\n      <td>14.824551</td>\n      <td>14.825202</td>\n      <td>14.825854</td>\n      <td>14.826507</td>\n      <td>14.827160</td>\n      <td>14.827814</td>\n    </tr>\n  </tbody>\n</table>\n<p>400 rows × 400 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Air_Temperature_C_400_400"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:25:55.189401Z",
     "start_time": "2024-08-16T22:25:55.178766Z"
    }
   },
   "id": "d13fd0afc265d2d3",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "          0         1         2         3         4         5         6    \\\n0    8.999907  8.999255  8.998604  8.997953  8.997304  8.996655  8.996008   \n1    9.000151  8.999499  8.998848  8.998198  8.997548  8.996900  8.996252   \n2    9.000394  8.999742  8.999091  8.998441  8.997792  8.997143  8.996496   \n3    9.000636  8.999984  8.999333  8.998683  8.998034  8.997386  8.996738   \n4    9.000877  9.000225  8.999574  8.998924  8.998275  8.997627  8.996980   \n..        ...       ...       ...       ...       ...       ...       ...   \n395  9.022687  9.022213  9.021740  9.021267  9.020795  9.020324  9.019852   \n396  9.022600  9.022128  9.021655  9.021183  9.020712  9.020241  9.019771   \n397  9.022514  9.022042  9.021570  9.021099  9.020628  9.020158  9.019688   \n398  9.022427  9.021955  9.021485  9.021014  9.020544  9.020075  9.019606   \n399  9.022339  9.021869  9.021399  9.020929  9.020460  9.019991  9.019523   \n\n          7         8         9    ...       390       391       392  \\\n0    8.995361  8.994715  8.994070  ...  8.805135  8.804766  8.804398   \n1    8.995606  8.994960  8.994315  ...  8.805388  8.805019  8.804650   \n2    8.995849  8.995203  8.994559  ...  8.805640  8.805271  8.804902   \n3    8.996092  8.995446  8.994802  ...  8.805891  8.805522  8.805153   \n4    8.996333  8.995688  8.995043  ...  8.806142  8.805773  8.805404   \n..        ...       ...       ...  ...       ...       ...       ...   \n395  9.019382  9.018911  9.018442  ...  8.869372  8.869050  8.868729   \n396  9.019301  9.018831  9.018362  ...  8.869461  8.869139  8.868818   \n397  9.019219  9.018750  9.018282  ...  8.869549  8.869228  8.868907   \n398  9.019137  9.018669  9.018201  ...  8.869638  8.869317  8.868996   \n399  9.019055  9.018587  9.018121  ...  8.869726  8.869405  8.869084   \n\n          393       394       395       396       397       398       399  \n0    8.804029  8.803662  8.803295  8.802928  8.802562  8.802197  8.801832  \n1    8.804282  8.803914  8.803547  8.803180  8.802814  8.802449  8.802084  \n2    8.804534  8.804166  8.803799  8.803432  8.803066  8.802701  8.802336  \n3    8.804785  8.804417  8.804050  8.803683  8.803317  8.802952  8.802587  \n4    8.805036  8.804668  8.804301  8.803934  8.803568  8.803202  8.802837  \n..        ...       ...       ...       ...       ...       ...       ...  \n395  8.868408  8.868087  8.867766  8.867446  8.867126  8.866807  8.866488  \n396  8.868497  8.868177  8.867856  8.867536  8.867217  8.866897  8.866579  \n397  8.868586  8.868266  8.867946  8.867626  8.867307  8.866988  8.866669  \n398  8.868675  8.868355  8.868036  8.867716  8.867397  8.867078  8.866760  \n399  8.868764  8.868444  8.868125  8.867806  8.867487  8.867168  8.866850  \n\n[400 rows x 400 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>390</th>\n      <th>391</th>\n      <th>392</th>\n      <th>393</th>\n      <th>394</th>\n      <th>395</th>\n      <th>396</th>\n      <th>397</th>\n      <th>398</th>\n      <th>399</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8.999907</td>\n      <td>8.999255</td>\n      <td>8.998604</td>\n      <td>8.997953</td>\n      <td>8.997304</td>\n      <td>8.996655</td>\n      <td>8.996008</td>\n      <td>8.995361</td>\n      <td>8.994715</td>\n      <td>8.994070</td>\n      <td>...</td>\n      <td>8.805135</td>\n      <td>8.804766</td>\n      <td>8.804398</td>\n      <td>8.804029</td>\n      <td>8.803662</td>\n      <td>8.803295</td>\n      <td>8.802928</td>\n      <td>8.802562</td>\n      <td>8.802197</td>\n      <td>8.801832</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9.000151</td>\n      <td>8.999499</td>\n      <td>8.998848</td>\n      <td>8.998198</td>\n      <td>8.997548</td>\n      <td>8.996900</td>\n      <td>8.996252</td>\n      <td>8.995606</td>\n      <td>8.994960</td>\n      <td>8.994315</td>\n      <td>...</td>\n      <td>8.805388</td>\n      <td>8.805019</td>\n      <td>8.804650</td>\n      <td>8.804282</td>\n      <td>8.803914</td>\n      <td>8.803547</td>\n      <td>8.803180</td>\n      <td>8.802814</td>\n      <td>8.802449</td>\n      <td>8.802084</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9.000394</td>\n      <td>8.999742</td>\n      <td>8.999091</td>\n      <td>8.998441</td>\n      <td>8.997792</td>\n      <td>8.997143</td>\n      <td>8.996496</td>\n      <td>8.995849</td>\n      <td>8.995203</td>\n      <td>8.994559</td>\n      <td>...</td>\n      <td>8.805640</td>\n      <td>8.805271</td>\n      <td>8.804902</td>\n      <td>8.804534</td>\n      <td>8.804166</td>\n      <td>8.803799</td>\n      <td>8.803432</td>\n      <td>8.803066</td>\n      <td>8.802701</td>\n      <td>8.802336</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9.000636</td>\n      <td>8.999984</td>\n      <td>8.999333</td>\n      <td>8.998683</td>\n      <td>8.998034</td>\n      <td>8.997386</td>\n      <td>8.996738</td>\n      <td>8.996092</td>\n      <td>8.995446</td>\n      <td>8.994802</td>\n      <td>...</td>\n      <td>8.805891</td>\n      <td>8.805522</td>\n      <td>8.805153</td>\n      <td>8.804785</td>\n      <td>8.804417</td>\n      <td>8.804050</td>\n      <td>8.803683</td>\n      <td>8.803317</td>\n      <td>8.802952</td>\n      <td>8.802587</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9.000877</td>\n      <td>9.000225</td>\n      <td>8.999574</td>\n      <td>8.998924</td>\n      <td>8.998275</td>\n      <td>8.997627</td>\n      <td>8.996980</td>\n      <td>8.996333</td>\n      <td>8.995688</td>\n      <td>8.995043</td>\n      <td>...</td>\n      <td>8.806142</td>\n      <td>8.805773</td>\n      <td>8.805404</td>\n      <td>8.805036</td>\n      <td>8.804668</td>\n      <td>8.804301</td>\n      <td>8.803934</td>\n      <td>8.803568</td>\n      <td>8.803202</td>\n      <td>8.802837</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>9.022687</td>\n      <td>9.022213</td>\n      <td>9.021740</td>\n      <td>9.021267</td>\n      <td>9.020795</td>\n      <td>9.020324</td>\n      <td>9.019852</td>\n      <td>9.019382</td>\n      <td>9.018911</td>\n      <td>9.018442</td>\n      <td>...</td>\n      <td>8.869372</td>\n      <td>8.869050</td>\n      <td>8.868729</td>\n      <td>8.868408</td>\n      <td>8.868087</td>\n      <td>8.867766</td>\n      <td>8.867446</td>\n      <td>8.867126</td>\n      <td>8.866807</td>\n      <td>8.866488</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>9.022600</td>\n      <td>9.022128</td>\n      <td>9.021655</td>\n      <td>9.021183</td>\n      <td>9.020712</td>\n      <td>9.020241</td>\n      <td>9.019771</td>\n      <td>9.019301</td>\n      <td>9.018831</td>\n      <td>9.018362</td>\n      <td>...</td>\n      <td>8.869461</td>\n      <td>8.869139</td>\n      <td>8.868818</td>\n      <td>8.868497</td>\n      <td>8.868177</td>\n      <td>8.867856</td>\n      <td>8.867536</td>\n      <td>8.867217</td>\n      <td>8.866897</td>\n      <td>8.866579</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>9.022514</td>\n      <td>9.022042</td>\n      <td>9.021570</td>\n      <td>9.021099</td>\n      <td>9.020628</td>\n      <td>9.020158</td>\n      <td>9.019688</td>\n      <td>9.019219</td>\n      <td>9.018750</td>\n      <td>9.018282</td>\n      <td>...</td>\n      <td>8.869549</td>\n      <td>8.869228</td>\n      <td>8.868907</td>\n      <td>8.868586</td>\n      <td>8.868266</td>\n      <td>8.867946</td>\n      <td>8.867626</td>\n      <td>8.867307</td>\n      <td>8.866988</td>\n      <td>8.866669</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>9.022427</td>\n      <td>9.021955</td>\n      <td>9.021485</td>\n      <td>9.021014</td>\n      <td>9.020544</td>\n      <td>9.020075</td>\n      <td>9.019606</td>\n      <td>9.019137</td>\n      <td>9.018669</td>\n      <td>9.018201</td>\n      <td>...</td>\n      <td>8.869638</td>\n      <td>8.869317</td>\n      <td>8.868996</td>\n      <td>8.868675</td>\n      <td>8.868355</td>\n      <td>8.868036</td>\n      <td>8.867716</td>\n      <td>8.867397</td>\n      <td>8.867078</td>\n      <td>8.866760</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>9.022339</td>\n      <td>9.021869</td>\n      <td>9.021399</td>\n      <td>9.020929</td>\n      <td>9.020460</td>\n      <td>9.019991</td>\n      <td>9.019523</td>\n      <td>9.019055</td>\n      <td>9.018587</td>\n      <td>9.018121</td>\n      <td>...</td>\n      <td>8.869726</td>\n      <td>8.869405</td>\n      <td>8.869084</td>\n      <td>8.868764</td>\n      <td>8.868444</td>\n      <td>8.868125</td>\n      <td>8.867806</td>\n      <td>8.867487</td>\n      <td>8.867168</td>\n      <td>8.866850</td>\n    </tr>\n  </tbody>\n</table>\n<p>400 rows × 400 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dew_Point_C_400_400"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:25:55.201242Z",
     "start_time": "2024-08-16T22:25:55.190440Z"
    }
   },
   "id": "c5e1752eccdd0cc8",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "           0          1          2          3          4          5    \\\n0    69.439597  69.436530  69.433455  69.430372  69.427280  69.424181   \n1    69.437913  69.434850  69.431778  69.428698  69.425610  69.422513   \n2    69.436236  69.433176  69.430108  69.427031  69.423946  69.420852   \n3    69.434566  69.431509  69.428444  69.425370  69.422288  69.419198   \n4    69.432902  69.429848  69.426786  69.423715  69.420637  69.417550   \n..         ...        ...        ...        ...        ...        ...   \n395  69.207684  69.204856  69.202024  69.199188  69.196347  69.193501   \n396  69.207924  69.205095  69.202262  69.199424  69.196581  69.193735   \n397  69.208167  69.205336  69.202501  69.199662  69.196818  69.193970   \n398  69.208412  69.205580  69.202743  69.199903  69.197058  69.194208   \n399  69.208659  69.205825  69.202988  69.200146  69.197299  69.194448   \n\n           6          7          8          9    ...        390        391  \\\n0    69.421073  69.417956  69.414832  69.411699  ...  67.747347  67.742103   \n1    69.419408  69.416295  69.413174  69.410044  ...  67.747344  67.742104   \n2    69.417751  69.414641  69.411523  69.408396  ...  67.747342  67.742107   \n3    69.416099  69.412993  69.409878  69.406755  ...  67.747341  67.742111   \n4    69.414455  69.411351  69.408239  69.405119  ...  67.747341  67.742116   \n..         ...        ...        ...        ...  ...        ...        ...   \n395  69.190652  69.187798  69.184939  69.182076  ...  67.838805  67.834784   \n396  69.190884  69.188028  69.185168  69.182304  ...  67.839221  67.835201   \n397  69.191118  69.188261  69.185400  69.182534  ...  67.839638  67.835620   \n398  69.191354  69.188496  69.185634  69.182767  ...  67.840055  67.836039   \n399  69.191593  69.188734  69.185870  69.183002  ...  67.840472  67.836458   \n\n           392        393        394        395        396        397  \\\n0    67.736856  67.731606  67.726354  67.721099  67.715841  67.710581   \n1    67.736862  67.731618  67.726370  67.721120  67.715868  67.710613   \n2    67.736870  67.731630  67.726388  67.721143  67.715895  67.710645   \n3    67.736879  67.731644  67.726407  67.721166  67.715924  67.710678   \n4    67.736889  67.731659  67.726427  67.721191  67.715954  67.710713   \n..         ...        ...        ...        ...        ...        ...   \n395  67.830761  67.826736  67.822709  67.818681  67.814651  67.810619   \n396  67.831180  67.827157  67.823132  67.819105  67.815077  67.811047   \n397  67.831600  67.827579  67.823555  67.819531  67.815504  67.811476   \n398  67.832020  67.828001  67.823979  67.819956  67.815931  67.811905   \n399  67.832441  67.828423  67.824404  67.820382  67.816359  67.812334   \n\n           398        399  \n0    67.705319  67.700053  \n1    67.705355  67.700094  \n2    67.705392  67.700137  \n3    67.705430  67.700180  \n4    67.705470  67.700225  \n..         ...        ...  \n395  67.806586  67.802551  \n396  67.807015  67.802982  \n397  67.807446  67.803414  \n398  67.807876  67.803847  \n399  67.808308  67.804279  \n\n[400 rows x 400 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>390</th>\n      <th>391</th>\n      <th>392</th>\n      <th>393</th>\n      <th>394</th>\n      <th>395</th>\n      <th>396</th>\n      <th>397</th>\n      <th>398</th>\n      <th>399</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>69.439597</td>\n      <td>69.436530</td>\n      <td>69.433455</td>\n      <td>69.430372</td>\n      <td>69.427280</td>\n      <td>69.424181</td>\n      <td>69.421073</td>\n      <td>69.417956</td>\n      <td>69.414832</td>\n      <td>69.411699</td>\n      <td>...</td>\n      <td>67.747347</td>\n      <td>67.742103</td>\n      <td>67.736856</td>\n      <td>67.731606</td>\n      <td>67.726354</td>\n      <td>67.721099</td>\n      <td>67.715841</td>\n      <td>67.710581</td>\n      <td>67.705319</td>\n      <td>67.700053</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>69.437913</td>\n      <td>69.434850</td>\n      <td>69.431778</td>\n      <td>69.428698</td>\n      <td>69.425610</td>\n      <td>69.422513</td>\n      <td>69.419408</td>\n      <td>69.416295</td>\n      <td>69.413174</td>\n      <td>69.410044</td>\n      <td>...</td>\n      <td>67.747344</td>\n      <td>67.742104</td>\n      <td>67.736862</td>\n      <td>67.731618</td>\n      <td>67.726370</td>\n      <td>67.721120</td>\n      <td>67.715868</td>\n      <td>67.710613</td>\n      <td>67.705355</td>\n      <td>67.700094</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>69.436236</td>\n      <td>69.433176</td>\n      <td>69.430108</td>\n      <td>69.427031</td>\n      <td>69.423946</td>\n      <td>69.420852</td>\n      <td>69.417751</td>\n      <td>69.414641</td>\n      <td>69.411523</td>\n      <td>69.408396</td>\n      <td>...</td>\n      <td>67.747342</td>\n      <td>67.742107</td>\n      <td>67.736870</td>\n      <td>67.731630</td>\n      <td>67.726388</td>\n      <td>67.721143</td>\n      <td>67.715895</td>\n      <td>67.710645</td>\n      <td>67.705392</td>\n      <td>67.700137</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>69.434566</td>\n      <td>69.431509</td>\n      <td>69.428444</td>\n      <td>69.425370</td>\n      <td>69.422288</td>\n      <td>69.419198</td>\n      <td>69.416099</td>\n      <td>69.412993</td>\n      <td>69.409878</td>\n      <td>69.406755</td>\n      <td>...</td>\n      <td>67.747341</td>\n      <td>67.742111</td>\n      <td>67.736879</td>\n      <td>67.731644</td>\n      <td>67.726407</td>\n      <td>67.721166</td>\n      <td>67.715924</td>\n      <td>67.710678</td>\n      <td>67.705430</td>\n      <td>67.700180</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>69.432902</td>\n      <td>69.429848</td>\n      <td>69.426786</td>\n      <td>69.423715</td>\n      <td>69.420637</td>\n      <td>69.417550</td>\n      <td>69.414455</td>\n      <td>69.411351</td>\n      <td>69.408239</td>\n      <td>69.405119</td>\n      <td>...</td>\n      <td>67.747341</td>\n      <td>67.742116</td>\n      <td>67.736889</td>\n      <td>67.731659</td>\n      <td>67.726427</td>\n      <td>67.721191</td>\n      <td>67.715954</td>\n      <td>67.710713</td>\n      <td>67.705470</td>\n      <td>67.700225</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>69.207684</td>\n      <td>69.204856</td>\n      <td>69.202024</td>\n      <td>69.199188</td>\n      <td>69.196347</td>\n      <td>69.193501</td>\n      <td>69.190652</td>\n      <td>69.187798</td>\n      <td>69.184939</td>\n      <td>69.182076</td>\n      <td>...</td>\n      <td>67.838805</td>\n      <td>67.834784</td>\n      <td>67.830761</td>\n      <td>67.826736</td>\n      <td>67.822709</td>\n      <td>67.818681</td>\n      <td>67.814651</td>\n      <td>67.810619</td>\n      <td>67.806586</td>\n      <td>67.802551</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>69.207924</td>\n      <td>69.205095</td>\n      <td>69.202262</td>\n      <td>69.199424</td>\n      <td>69.196581</td>\n      <td>69.193735</td>\n      <td>69.190884</td>\n      <td>69.188028</td>\n      <td>69.185168</td>\n      <td>69.182304</td>\n      <td>...</td>\n      <td>67.839221</td>\n      <td>67.835201</td>\n      <td>67.831180</td>\n      <td>67.827157</td>\n      <td>67.823132</td>\n      <td>67.819105</td>\n      <td>67.815077</td>\n      <td>67.811047</td>\n      <td>67.807015</td>\n      <td>67.802982</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>69.208167</td>\n      <td>69.205336</td>\n      <td>69.202501</td>\n      <td>69.199662</td>\n      <td>69.196818</td>\n      <td>69.193970</td>\n      <td>69.191118</td>\n      <td>69.188261</td>\n      <td>69.185400</td>\n      <td>69.182534</td>\n      <td>...</td>\n      <td>67.839638</td>\n      <td>67.835620</td>\n      <td>67.831600</td>\n      <td>67.827579</td>\n      <td>67.823555</td>\n      <td>67.819531</td>\n      <td>67.815504</td>\n      <td>67.811476</td>\n      <td>67.807446</td>\n      <td>67.803414</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>69.208412</td>\n      <td>69.205580</td>\n      <td>69.202743</td>\n      <td>69.199903</td>\n      <td>69.197058</td>\n      <td>69.194208</td>\n      <td>69.191354</td>\n      <td>69.188496</td>\n      <td>69.185634</td>\n      <td>69.182767</td>\n      <td>...</td>\n      <td>67.840055</td>\n      <td>67.836039</td>\n      <td>67.832020</td>\n      <td>67.828001</td>\n      <td>67.823979</td>\n      <td>67.819956</td>\n      <td>67.815931</td>\n      <td>67.811905</td>\n      <td>67.807876</td>\n      <td>67.803847</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>69.208659</td>\n      <td>69.205825</td>\n      <td>69.202988</td>\n      <td>69.200146</td>\n      <td>69.197299</td>\n      <td>69.194448</td>\n      <td>69.191593</td>\n      <td>69.188734</td>\n      <td>69.185870</td>\n      <td>69.183002</td>\n      <td>...</td>\n      <td>67.840472</td>\n      <td>67.836458</td>\n      <td>67.832441</td>\n      <td>67.828423</td>\n      <td>67.824404</td>\n      <td>67.820382</td>\n      <td>67.816359</td>\n      <td>67.812334</td>\n      <td>67.808308</td>\n      <td>67.804279</td>\n    </tr>\n  </tbody>\n</table>\n<p>400 rows × 400 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Relative_Humidity_400_400"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:25:55.212724Z",
     "start_time": "2024-08-16T22:25:55.202402Z"
    }
   },
   "id": "8949ec831750a2fd",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "          0         1         2         3         4         5         6    \\\n0    0.404868  0.405059  0.405249  0.405438  0.405627  0.405815  0.406003   \n1    0.404750  0.404940  0.405130  0.405320  0.405509  0.405697  0.405885   \n2    0.404632  0.404823  0.405013  0.405202  0.405391  0.405579  0.405767   \n3    0.404515  0.404705  0.404895  0.405085  0.405274  0.405462  0.405650   \n4    0.404398  0.404589  0.404779  0.404968  0.405157  0.405345  0.405533   \n..        ...       ...       ...       ...       ...       ...       ...   \n395  0.393879  0.394005  0.394131  0.394256  0.394381  0.394506  0.394631   \n396  0.393920  0.394046  0.394171  0.394297  0.394422  0.394546  0.394671   \n397  0.393962  0.394087  0.394212  0.394337  0.394462  0.394586  0.394711   \n398  0.394004  0.394129  0.394254  0.394378  0.394503  0.394627  0.394751   \n399  0.394046  0.394171  0.394295  0.394419  0.394543  0.394667  0.394791   \n\n          7         8         9    ...       390       391       392  \\\n0    0.406190  0.406377  0.406564  ...  0.447458  0.447502  0.447545   \n1    0.406072  0.406259  0.406446  ...  0.447367  0.447410  0.447454   \n2    0.405955  0.406142  0.406328  ...  0.447276  0.447319  0.447362   \n3    0.405838  0.406025  0.406211  ...  0.447184  0.447228  0.447272   \n4    0.405721  0.405908  0.406094  ...  0.447093  0.447137  0.447181   \n..        ...       ...       ...  ...       ...       ...       ...   \n395  0.394756  0.394880  0.395004  ...  0.426328  0.426375  0.426421   \n396  0.394795  0.394919  0.395042  ...  0.426305  0.426352  0.426399   \n397  0.394834  0.394958  0.395081  ...  0.426283  0.426330  0.426376   \n398  0.394874  0.394997  0.395120  ...  0.426260  0.426307  0.426354   \n399  0.394914  0.395037  0.395160  ...  0.426238  0.426285  0.426332   \n\n          393       394       395       396       397       398       399  \n0    0.447588  0.447630  0.447673  0.447715  0.447757  0.447798  0.447840  \n1    0.447497  0.447539  0.447582  0.447624  0.447666  0.447708  0.447749  \n2    0.447406  0.447448  0.447491  0.447533  0.447575  0.447617  0.447659  \n3    0.447315  0.447358  0.447400  0.447443  0.447485  0.447527  0.447569  \n4    0.447224  0.447267  0.447310  0.447352  0.447395  0.447437  0.447479  \n..        ...       ...       ...       ...       ...       ...       ...  \n395  0.426468  0.426515  0.426561  0.426607  0.426653  0.426699  0.426745  \n396  0.426445  0.426492  0.426538  0.426584  0.426630  0.426676  0.426722  \n397  0.426423  0.426469  0.426516  0.426562  0.426608  0.426653  0.426699  \n398  0.426400  0.426447  0.426493  0.426539  0.426585  0.426631  0.426676  \n399  0.426378  0.426424  0.426470  0.426516  0.426562  0.426608  0.426654  \n\n[400 rows x 400 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>390</th>\n      <th>391</th>\n      <th>392</th>\n      <th>393</th>\n      <th>394</th>\n      <th>395</th>\n      <th>396</th>\n      <th>397</th>\n      <th>398</th>\n      <th>399</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.404868</td>\n      <td>0.405059</td>\n      <td>0.405249</td>\n      <td>0.405438</td>\n      <td>0.405627</td>\n      <td>0.405815</td>\n      <td>0.406003</td>\n      <td>0.406190</td>\n      <td>0.406377</td>\n      <td>0.406564</td>\n      <td>...</td>\n      <td>0.447458</td>\n      <td>0.447502</td>\n      <td>0.447545</td>\n      <td>0.447588</td>\n      <td>0.447630</td>\n      <td>0.447673</td>\n      <td>0.447715</td>\n      <td>0.447757</td>\n      <td>0.447798</td>\n      <td>0.447840</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.404750</td>\n      <td>0.404940</td>\n      <td>0.405130</td>\n      <td>0.405320</td>\n      <td>0.405509</td>\n      <td>0.405697</td>\n      <td>0.405885</td>\n      <td>0.406072</td>\n      <td>0.406259</td>\n      <td>0.406446</td>\n      <td>...</td>\n      <td>0.447367</td>\n      <td>0.447410</td>\n      <td>0.447454</td>\n      <td>0.447497</td>\n      <td>0.447539</td>\n      <td>0.447582</td>\n      <td>0.447624</td>\n      <td>0.447666</td>\n      <td>0.447708</td>\n      <td>0.447749</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.404632</td>\n      <td>0.404823</td>\n      <td>0.405013</td>\n      <td>0.405202</td>\n      <td>0.405391</td>\n      <td>0.405579</td>\n      <td>0.405767</td>\n      <td>0.405955</td>\n      <td>0.406142</td>\n      <td>0.406328</td>\n      <td>...</td>\n      <td>0.447276</td>\n      <td>0.447319</td>\n      <td>0.447362</td>\n      <td>0.447406</td>\n      <td>0.447448</td>\n      <td>0.447491</td>\n      <td>0.447533</td>\n      <td>0.447575</td>\n      <td>0.447617</td>\n      <td>0.447659</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.404515</td>\n      <td>0.404705</td>\n      <td>0.404895</td>\n      <td>0.405085</td>\n      <td>0.405274</td>\n      <td>0.405462</td>\n      <td>0.405650</td>\n      <td>0.405838</td>\n      <td>0.406025</td>\n      <td>0.406211</td>\n      <td>...</td>\n      <td>0.447184</td>\n      <td>0.447228</td>\n      <td>0.447272</td>\n      <td>0.447315</td>\n      <td>0.447358</td>\n      <td>0.447400</td>\n      <td>0.447443</td>\n      <td>0.447485</td>\n      <td>0.447527</td>\n      <td>0.447569</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.404398</td>\n      <td>0.404589</td>\n      <td>0.404779</td>\n      <td>0.404968</td>\n      <td>0.405157</td>\n      <td>0.405345</td>\n      <td>0.405533</td>\n      <td>0.405721</td>\n      <td>0.405908</td>\n      <td>0.406094</td>\n      <td>...</td>\n      <td>0.447093</td>\n      <td>0.447137</td>\n      <td>0.447181</td>\n      <td>0.447224</td>\n      <td>0.447267</td>\n      <td>0.447310</td>\n      <td>0.447352</td>\n      <td>0.447395</td>\n      <td>0.447437</td>\n      <td>0.447479</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>0.393879</td>\n      <td>0.394005</td>\n      <td>0.394131</td>\n      <td>0.394256</td>\n      <td>0.394381</td>\n      <td>0.394506</td>\n      <td>0.394631</td>\n      <td>0.394756</td>\n      <td>0.394880</td>\n      <td>0.395004</td>\n      <td>...</td>\n      <td>0.426328</td>\n      <td>0.426375</td>\n      <td>0.426421</td>\n      <td>0.426468</td>\n      <td>0.426515</td>\n      <td>0.426561</td>\n      <td>0.426607</td>\n      <td>0.426653</td>\n      <td>0.426699</td>\n      <td>0.426745</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>0.393920</td>\n      <td>0.394046</td>\n      <td>0.394171</td>\n      <td>0.394297</td>\n      <td>0.394422</td>\n      <td>0.394546</td>\n      <td>0.394671</td>\n      <td>0.394795</td>\n      <td>0.394919</td>\n      <td>0.395042</td>\n      <td>...</td>\n      <td>0.426305</td>\n      <td>0.426352</td>\n      <td>0.426399</td>\n      <td>0.426445</td>\n      <td>0.426492</td>\n      <td>0.426538</td>\n      <td>0.426584</td>\n      <td>0.426630</td>\n      <td>0.426676</td>\n      <td>0.426722</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>0.393962</td>\n      <td>0.394087</td>\n      <td>0.394212</td>\n      <td>0.394337</td>\n      <td>0.394462</td>\n      <td>0.394586</td>\n      <td>0.394711</td>\n      <td>0.394834</td>\n      <td>0.394958</td>\n      <td>0.395081</td>\n      <td>...</td>\n      <td>0.426283</td>\n      <td>0.426330</td>\n      <td>0.426376</td>\n      <td>0.426423</td>\n      <td>0.426469</td>\n      <td>0.426516</td>\n      <td>0.426562</td>\n      <td>0.426608</td>\n      <td>0.426653</td>\n      <td>0.426699</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>0.394004</td>\n      <td>0.394129</td>\n      <td>0.394254</td>\n      <td>0.394378</td>\n      <td>0.394503</td>\n      <td>0.394627</td>\n      <td>0.394751</td>\n      <td>0.394874</td>\n      <td>0.394997</td>\n      <td>0.395120</td>\n      <td>...</td>\n      <td>0.426260</td>\n      <td>0.426307</td>\n      <td>0.426354</td>\n      <td>0.426400</td>\n      <td>0.426447</td>\n      <td>0.426493</td>\n      <td>0.426539</td>\n      <td>0.426585</td>\n      <td>0.426631</td>\n      <td>0.426676</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>0.394046</td>\n      <td>0.394171</td>\n      <td>0.394295</td>\n      <td>0.394419</td>\n      <td>0.394543</td>\n      <td>0.394667</td>\n      <td>0.394791</td>\n      <td>0.394914</td>\n      <td>0.395037</td>\n      <td>0.395160</td>\n      <td>...</td>\n      <td>0.426238</td>\n      <td>0.426285</td>\n      <td>0.426332</td>\n      <td>0.426378</td>\n      <td>0.426424</td>\n      <td>0.426470</td>\n      <td>0.426516</td>\n      <td>0.426562</td>\n      <td>0.426608</td>\n      <td>0.426654</td>\n    </tr>\n  </tbody>\n</table>\n<p>400 rows × 400 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wind_Speed_m_s_400_400"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:25:55.223730Z",
     "start_time": "2024-08-16T22:25:55.213729Z"
    }
   },
   "id": "48edd5f4471fbdf",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      tempMin  tempMax  cloudCover  humidity  windSpeed  visibility\n0        7.53    12.23        0.80      0.89      14.69        4.43\n1        3.58     7.15        0.62      0.79      15.04        5.64\n2       -0.61     6.54        0.31      0.84       4.48        6.20\n3       -0.63     7.59        0.78      0.85       4.35        6.22\n4        6.51    10.43        0.85      0.91       6.20        5.91\n...       ...      ...         ...       ...        ...         ...\n1790     8.55    10.57        0.96      0.85       6.80       10.00\n1791     5.79     9.40        0.87      0.85       6.82        9.03\n1792     2.30     6.67        0.40      0.79       5.15       10.00\n1793     1.08     6.14        0.18      0.76       4.85        9.11\n1794     0.42     5.69        0.55      0.72       4.79       10.00\n\n[1795 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tempMin</th>\n      <th>tempMax</th>\n      <th>cloudCover</th>\n      <th>humidity</th>\n      <th>windSpeed</th>\n      <th>visibility</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.53</td>\n      <td>12.23</td>\n      <td>0.80</td>\n      <td>0.89</td>\n      <td>14.69</td>\n      <td>4.43</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3.58</td>\n      <td>7.15</td>\n      <td>0.62</td>\n      <td>0.79</td>\n      <td>15.04</td>\n      <td>5.64</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.61</td>\n      <td>6.54</td>\n      <td>0.31</td>\n      <td>0.84</td>\n      <td>4.48</td>\n      <td>6.20</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.63</td>\n      <td>7.59</td>\n      <td>0.78</td>\n      <td>0.85</td>\n      <td>4.35</td>\n      <td>6.22</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6.51</td>\n      <td>10.43</td>\n      <td>0.85</td>\n      <td>0.91</td>\n      <td>6.20</td>\n      <td>5.91</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1790</th>\n      <td>8.55</td>\n      <td>10.57</td>\n      <td>0.96</td>\n      <td>0.85</td>\n      <td>6.80</td>\n      <td>10.00</td>\n    </tr>\n    <tr>\n      <th>1791</th>\n      <td>5.79</td>\n      <td>9.40</td>\n      <td>0.87</td>\n      <td>0.85</td>\n      <td>6.82</td>\n      <td>9.03</td>\n    </tr>\n    <tr>\n      <th>1792</th>\n      <td>2.30</td>\n      <td>6.67</td>\n      <td>0.40</td>\n      <td>0.79</td>\n      <td>5.15</td>\n      <td>10.00</td>\n    </tr>\n    <tr>\n      <th>1793</th>\n      <td>1.08</td>\n      <td>6.14</td>\n      <td>0.18</td>\n      <td>0.76</td>\n      <td>4.85</td>\n      <td>9.11</td>\n    </tr>\n    <tr>\n      <th>1794</th>\n      <td>0.42</td>\n      <td>5.69</td>\n      <td>0.55</td>\n      <td>0.72</td>\n      <td>4.79</td>\n      <td>10.00</td>\n    </tr>\n  </tbody>\n</table>\n<p>1795 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_1795_6"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:25:55.387291Z",
     "start_time": "2024-08-16T22:25:55.378849Z"
    }
   },
   "id": "36c329d55467a41f",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 数据处理"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b92466b03bef8597"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:25:59.353987Z",
     "start_time": "2024-08-16T22:25:59.351285Z"
    }
   },
   "id": "fd2b51a1463d7e0b",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X = np.stack([BH_tif_400_400.to_numpy(), BV_tif_400_400.to_numpy(), CNM_tif_400_400.to_numpy(), LAI_tif_400_400.to_numpy(), DSM_tif_400_400.to_numpy()], axis=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T00:32:16.470876Z",
     "start_time": "2024-08-17T00:32:16.464891Z"
    }
   },
   "id": "8c674fac3ae26795",
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[1.81000000e+002, 1.79000000e+308, 2.88827362e+001,\n         1.17030945e+001, 5.52400017e+001],\n        [6.55350000e+004, 1.79000000e+308, 2.84086761e+001,\n         1.15134706e+001, 5.47369995e+001],\n        [6.55350000e+004, 1.79000000e+308, 2.78414173e+001,\n         1.12865667e+001, 5.41160011e+001],\n        ...,\n        [6.55350000e+004, 1.79000000e+308, 1.52161217e+001,\n         6.23644876e+000, 5.88440018e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.40586624e+001,\n         5.77346516e+000, 5.72519989e+001],\n        [6.55350000e+004, 1.79000000e+308, 8.58650208e-002,\n         1.84346020e-001, 4.31790009e+001]],\n\n       [[1.81000000e+002, 1.79000000e+308, 3.13316689e+001,\n         1.26826677e+001, 5.75429993e+001],\n        [1.81000000e+002, 1.79000000e+308, 3.06372433e+001,\n         1.24048967e+001, 5.69510002e+001],\n        [1.81000000e+002, 1.79000000e+308, 3.17199535e+001,\n         1.28379812e+001, 5.80690002e+001],\n        ...,\n        [6.55350000e+004, 1.79000000e+308, 1.38880692e+001,\n         5.70522785e+000, 5.74080009e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.26316872e+001,\n         5.20267487e+000, 5.57999992e+001],\n        [6.55350000e+004, 1.79000000e+308, 7.41386414e-002,\n         1.79655463e-001, 4.31250000e+001]],\n\n       [[1.81000000e+002, 1.79000000e+308, 3.19086132e+001,\n         1.29134455e+001, 5.79140015e+001],\n        [1.81000000e+002, 1.79000000e+308, 3.18045063e+001,\n         1.28718023e+001, 5.79150009e+001],\n        [1.81000000e+002, 1.79000000e+308, 3.20271225e+001,\n         1.29608488e+001, 5.81800003e+001],\n        ...,\n        [6.55350000e+004, 1.79000000e+308, 1.04864044e+001,\n         4.34456205e+000, 5.37140007e+001],\n        [6.55350000e+004, 1.79000000e+308, 9.95444107e+000,\n         4.13177633e+000, 5.30670013e+001],\n        [6.55350000e+004, 1.79000000e+308, 5.62438965e-002,\n         1.72497571e-001, 4.31020012e+001]],\n\n       ...,\n\n       [[6.55350000e+004, 1.79000000e+308, 6.26661301e+000,\n         2.65664530e+000, 1.90270004e+001],\n        [6.55350000e+004, 1.79000000e+308, 6.32370472e+000,\n         2.67948198e+000, 1.90319996e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.47393322e+000,\n         7.39573359e-001, 1.57729998e+001],\n        ...,\n        [6.55350000e+004, 1.79000000e+308, 1.78362598e+001,\n         7.28450394e+000, 1.96359997e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.87065105e+001,\n         7.63260460e+000, 2.05779991e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.72083931e+001,\n         7.03335762e+000, 2.02390003e+001]],\n\n       [[6.55350000e+004, 1.79000000e+308, 4.77874565e+000,\n         2.06149840e+000, 1.90030003e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.47922707e+000,\n         7.41690874e-001, 1.57329998e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.91199303e-001,\n         2.26479739e-001, 1.45369997e+001],\n        ...,\n        [6.55350000e+004, 1.79000000e+308, 1.29963923e+001,\n         5.34855700e+000, 1.48299999e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.49581776e+001,\n         6.13327122e+000, 1.67150002e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.83565197e+001,\n         7.49260807e+000, 2.01739998e+001]],\n\n       [[6.55350000e+004, 1.79000000e+308, 2.58676529e-001,\n         2.53470629e-001, 1.44799995e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.51011467e-001,\n         2.10404590e-001, 1.45719995e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.63286209e-001,\n         2.15314493e-001, 1.46169996e+001],\n        ...,\n        [6.55350000e+004, 1.79000000e+308, 8.35458040e-002,\n         1.83418334e-001, 1.90300000e+000],\n        [6.55350000e+004, 1.79000000e+308, 1.34665489e-001,\n         2.03866199e-001, 1.94400001e+000],\n        [6.55350000e+004, 1.79000000e+308, 1.29756327e+001,\n         5.34025335e+000, 1.47709999e+001]]])"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T00:32:17.331406Z",
     "start_time": "2024-08-17T00:32:17.326034Z"
    }
   },
   "id": "c0abc3be430e3e07",
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(400, 400, 5)"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T00:36:07.018113Z",
     "start_time": "2024-08-17T00:36:07.011564Z"
    }
   },
   "id": "d633c0680612969",
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Y = weather_1795_6['tempMax']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T00:36:07.710127Z",
     "start_time": "2024-08-17T00:36:07.707143Z"
    }
   },
   "id": "cba4ce3bfed06c58",
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0       12.23\n1        7.15\n2        6.54\n3        7.59\n4       10.43\n        ...  \n1790    10.57\n1791     9.40\n1792     6.67\n1793     6.14\n1794     5.69\nName: tempMax, Length: 1795, dtype: float64"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T00:36:08.194705Z",
     "start_time": "2024-08-17T00:36:08.189928Z"
    }
   },
   "id": "ff62b6c1457a9df8",
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 保存 X 为 .npy 文件\n",
    "np.save('X_array.npy', X)\n",
    "\n",
    "# 保存 Y 为 .pkl 文件\n",
    "Y.to_pickle('Y_series.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T00:36:36.845261Z",
     "start_time": "2024-08-17T00:36:36.814564Z"
    }
   },
   "id": "7c47e8215173c7b4",
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:26:06.391229Z",
     "start_time": "2024-08-16T22:26:05.332294Z"
    }
   },
   "id": "bdb653ace6e6b9fd",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 将Y数据分为训练集和验证集（80%训练，20%验证）\n",
    "Y_train, Y_val = train_test_split(Y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:26:06.395793Z",
     "start_time": "2024-08-16T22:26:06.392269Z"
    }
   },
   "id": "c5367a6a0bb22e97",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 模型构建"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "546fae4e986b1c00"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "# import numpy as np\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T05:14:39.758214Z",
     "start_time": "2024-08-15T05:14:39.751182Z"
    }
   },
   "id": "53d349c077bc42aa",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# class CNNFeatureExtractor(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNNFeatureExtractor, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(4, 32, kernel_size=3, padding=1)  # 输入400*400*4，输出400*400*32\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2)  # 输出200*200*32\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # 输出200*200*64\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2)  # 输出100*100*64\n",
    "#         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)  # 输出100*100*128\n",
    "#         self.pool3 = nn.MaxPool2d(2, 2)  # 输出50*50*128\n",
    "#         self.fc = nn.Linear(128 * 50 * 50, 128)  # 输出128\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         x = self.pool1(torch.relu(self.conv1(x)))\n",
    "#         x = self.pool2(torch.relu(self.conv2(x)))\n",
    "#         x = self.pool3(torch.relu(self.conv3(x)))\n",
    "#         x = x.view(-1, 128 * 50 * 50)  # -1表示自适应, 一般是batch_size\n",
    "#         x = torch.relu(self.fc(x))\n",
    "#         return x\n",
    "# \n",
    "# class CNNLSTMModel(nn.Module):\n",
    "#     def __init__(self, seq_length):\n",
    "#         super(CNNLSTMModel, self).__init__()\n",
    "#         self.cnn = CNNFeatureExtractor()  # CNN特征提取器（不改变形状）\n",
    "#         self.lstm = nn.LSTM(128, 128, batch_first=True)  # batch_first=True 表示输入和输出的第一个维度是批次大小。 128 -> 128\n",
    "#         self.fc = nn.Linear(128, 1)  # 128 -> 1\n",
    "#         self.seq_length = seq_length\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         batch_size = x.size(0)  # 获取批次大小，x.size() => [batch_size, 4, 400, 400]\n",
    "#         \n",
    "#         # CNN特征提取\n",
    "#         cnn_out = self.cnn(x)\n",
    "#         # cnn_out.size() => [batch_size, 128]\n",
    "#         # 这里CNN的输出是一个128维的特征向量，不再有空间维度\n",
    "# \n",
    "#         # 调整形状以适应LSTM输入\n",
    "#         cnn_out = cnn_out.unsqueeze(1).repeat(1, self.seq_length, 1)\n",
    "#         # cnn_out.size() => [batch_size, seq_length, 128]\n",
    "#         # 通过unsqueeze在第二维添加一个维度，然后repeat扩展成seq_length长的时间序列数据\n",
    "#         # 其中每个seq_length的数据都是一样的CNN特征\n",
    "# \n",
    "#         # LSTM\n",
    "#         lstm_out, _ = self.lstm(cnn_out)\n",
    "#         # lstm_out.size() => [batch_size, seq_length, 128]\n",
    "#         # LSTM输出的形状与输入相同，输出也是每个时间步长一个128维的向量\n",
    "# \n",
    "#         # 全连接层处理\n",
    "#         output = self.fc(lstm_out)\n",
    "#         # output.size() => [batch_size, seq_length, 1]\n",
    "#         # 全连接层将LSTM的每个时间步长的128维输出转换为1维的预测值\n",
    "# \n",
    "#         return output\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T05:14:39.949913Z",
     "start_time": "2024-08-15T05:14:39.946158Z"
    }
   },
   "id": "819c4199003f6af6",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # 假设你已经有了X和Y数据\n",
    "# # X = np.random.rand(400, 400, 4)  # 示例数据，应替换为真实数据\n",
    "# # Y = np.random.rand(1000, 1)  # 示例数据，应替换为真实数据\n",
    "# \n",
    "# X = torch.tensor(X, dtype=torch.float32).unsqueeze(0).permute(0, 3, 1, 2)  # 转换为[N, C, H, W]\n",
    "# Y = torch.tensor(Y, dtype=torch.float32)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T05:14:40.007985Z",
     "start_time": "2024-08-15T05:14:40.005137Z"
    }
   },
   "id": "25bf3c27755e0af7",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T05:14:40.073716Z",
     "start_time": "2024-08-15T05:14:40.070302Z"
    }
   },
   "id": "84cc39ed5e1868c",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T05:14:40.105803Z",
     "start_time": "2024-08-15T05:14:40.103262Z"
    }
   },
   "id": "c1d1253119c9ed3e",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 创建数据集和数据加载器\n",
    "# dataset = TensorDataset(X, Y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T05:14:40.159765Z",
     "start_time": "2024-08-15T05:14:40.157114Z"
    }
   },
   "id": "6f6fa18272a8b482",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "# \n",
    "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T05:14:40.188828Z",
     "start_time": "2024-08-15T05:14:40.185830Z"
    }
   },
   "id": "4c12c01d1fa59d3a",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # 定义模型、损失函数和优化器\n",
    "# model = CNNLSTMModel(seq_length=1000)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# \n",
    "# # 训练模型\n",
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for inputs, targets in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs.squeeze(), targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_loader)}')\n",
    "# \n",
    "#     # 验证模型\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets in val_loader:\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs.squeeze(), targets)\n",
    "#             val_loss += loss.item()\n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss/len(val_loader)}')\n",
    "# \n",
    "# # 输出验证集结果\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for inputs, targets in val_loader:\n",
    "#         outputs = model(inputs)\n",
    "#         print(\"Validation Predictions: \", outputs.squeeze().numpy())\n",
    "#         print(\"Validation Targets: \", targets.numpy())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T05:14:40.246196Z",
     "start_time": "2024-08-15T05:14:40.242890Z"
    }
   },
   "id": "332957a482581d15",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[1.81000000e+002, 1.79000000e+308, 2.88827362e+001,\n         1.17030945e+001, 5.52400017e+001],\n        [6.55350000e+004, 1.79000000e+308, 2.84086761e+001,\n         1.15134706e+001, 5.47369995e+001],\n        [6.55350000e+004, 1.79000000e+308, 2.78414173e+001,\n         1.12865667e+001, 5.41160011e+001],\n        ...,\n        [6.55350000e+004, 1.79000000e+308, 1.52161217e+001,\n         6.23644876e+000, 5.88440018e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.40586624e+001,\n         5.77346516e+000, 5.72519989e+001],\n        [6.55350000e+004, 1.79000000e+308, 8.58650208e-002,\n         1.84346020e-001, 4.31790009e+001]],\n\n       [[1.81000000e+002, 1.79000000e+308, 3.13316689e+001,\n         1.26826677e+001, 5.75429993e+001],\n        [1.81000000e+002, 1.79000000e+308, 3.06372433e+001,\n         1.24048967e+001, 5.69510002e+001],\n        [1.81000000e+002, 1.79000000e+308, 3.17199535e+001,\n         1.28379812e+001, 5.80690002e+001],\n        ...,\n        [6.55350000e+004, 1.79000000e+308, 1.38880692e+001,\n         5.70522785e+000, 5.74080009e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.26316872e+001,\n         5.20267487e+000, 5.57999992e+001],\n        [6.55350000e+004, 1.79000000e+308, 7.41386414e-002,\n         1.79655463e-001, 4.31250000e+001]],\n\n       [[1.81000000e+002, 1.79000000e+308, 3.19086132e+001,\n         1.29134455e+001, 5.79140015e+001],\n        [1.81000000e+002, 1.79000000e+308, 3.18045063e+001,\n         1.28718023e+001, 5.79150009e+001],\n        [1.81000000e+002, 1.79000000e+308, 3.20271225e+001,\n         1.29608488e+001, 5.81800003e+001],\n        ...,\n        [6.55350000e+004, 1.79000000e+308, 1.04864044e+001,\n         4.34456205e+000, 5.37140007e+001],\n        [6.55350000e+004, 1.79000000e+308, 9.95444107e+000,\n         4.13177633e+000, 5.30670013e+001],\n        [6.55350000e+004, 1.79000000e+308, 5.62438965e-002,\n         1.72497571e-001, 4.31020012e+001]],\n\n       ...,\n\n       [[6.55350000e+004, 1.79000000e+308, 6.26661301e+000,\n         2.65664530e+000, 1.90270004e+001],\n        [6.55350000e+004, 1.79000000e+308, 6.32370472e+000,\n         2.67948198e+000, 1.90319996e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.47393322e+000,\n         7.39573359e-001, 1.57729998e+001],\n        ...,\n        [6.55350000e+004, 1.79000000e+308, 1.78362598e+001,\n         7.28450394e+000, 1.96359997e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.87065105e+001,\n         7.63260460e+000, 2.05779991e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.72083931e+001,\n         7.03335762e+000, 2.02390003e+001]],\n\n       [[6.55350000e+004, 1.79000000e+308, 4.77874565e+000,\n         2.06149840e+000, 1.90030003e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.47922707e+000,\n         7.41690874e-001, 1.57329998e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.91199303e-001,\n         2.26479739e-001, 1.45369997e+001],\n        ...,\n        [6.55350000e+004, 1.79000000e+308, 1.29963923e+001,\n         5.34855700e+000, 1.48299999e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.49581776e+001,\n         6.13327122e+000, 1.67150002e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.83565197e+001,\n         7.49260807e+000, 2.01739998e+001]],\n\n       [[6.55350000e+004, 1.79000000e+308, 2.58676529e-001,\n         2.53470629e-001, 1.44799995e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.51011467e-001,\n         2.10404590e-001, 1.45719995e+001],\n        [6.55350000e+004, 1.79000000e+308, 1.63286209e-001,\n         2.15314493e-001, 1.46169996e+001],\n        ...,\n        [6.55350000e+004, 1.79000000e+308, 8.35458040e-002,\n         1.83418334e-001, 1.90300000e+000],\n        [6.55350000e+004, 1.79000000e+308, 1.34665489e-001,\n         2.03866199e-001, 1.94400001e+000],\n        [6.55350000e+004, 1.79000000e+308, 1.29756327e+001,\n         5.34025335e+000, 1.47709999e+001]]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T02:03:58.828772Z",
     "start_time": "2024-08-16T02:03:58.823997Z"
    }
   },
   "id": "5ae4e3ae0faf35c",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0       12.23\n1        7.15\n2        6.54\n3        7.59\n4       10.43\n        ...  \n1790    10.57\n1791     9.40\n1792     6.67\n1793     6.14\n1794     5.69\nName: tempMax, Length: 1795, dtype: float64"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T02:03:59.768497Z",
     "start_time": "2024-08-16T02:03:59.763692Z"
    }
   },
   "id": "d5a50ba95ac7cb82",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_original = X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:26:14.328762Z",
     "start_time": "2024-08-16T22:26:14.324777Z"
    }
   },
   "id": "170c3792d12067aa",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Y_original = Y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:26:14.824544Z",
     "start_time": "2024-08-16T22:26:14.820862Z"
    }
   },
   "id": "5272234a2a48666e",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# 假设 X_original 是形状为 [400, 400, n] 的空间数据\n",
    "# 假设 Y_original 是形状为 [1000,] 的温度时间序列数据\n",
    "# 初始设定 n = 4\n",
    "n = 5\n",
    "seq_length = 20\n",
    "\n",
    "# 将时间序列拆分为输入序列和预测值\n",
    "def create_sequences(data, seq_length):\n",
    "    X_seq = []\n",
    "    Y_seq = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X_seq.append(data[i:i+seq_length])\n",
    "        Y_seq.append(data[i+seq_length])\n",
    "    return np.array(X_seq), np.array(Y_seq)\n",
    "\n",
    "# 拆分时间序列数据\n",
    "Y_seq, Y_target = create_sequences(Y_original, seq_length)\n",
    "Y_seq = np.expand_dims(Y_seq, axis=-1)  # [980, 20, 1]\n",
    "Y_target = np.expand_dims(Y_target, axis=-1)  # [980, 1]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:26:18.825877Z",
     "start_time": "2024-08-16T22:26:15.550650Z"
    }
   },
   "id": "493924d4d803d574",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:26:18.828884Z",
     "start_time": "2024-08-16T22:26:18.826909Z"
    }
   },
   "id": "70152a0952ad1492",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 转换为 PyTorch 张量\n",
    "X_land = torch.tensor(X_original, dtype=torch.float32).unsqueeze(0).repeat(Y_seq.shape[0], 1, 1, 1)\n",
    "Y_seq = torch.tensor(Y_seq, dtype=torch.float32)\n",
    "Y_target = torch.tensor(Y_target, dtype=torch.float32)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:26:22.257837Z",
     "start_time": "2024-08-16T22:26:18.828884Z"
    }
   },
   "id": "95d2d43ff0bbccba",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # 数据集拆分为训练集和验证集\n",
    "# train_size = int(0.8 * Y_seq.shape[0])\n",
    "# val_size = Y_seq.shape[0] - train_size\n",
    "# \n",
    "# train_dataset, val_dataset = random_split(TensorDataset(X_land, Y_seq, Y_target), [train_size, val_size])\n",
    "# \n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:26:22.261791Z",
     "start_time": "2024-08-16T22:26:22.258841Z"
    }
   },
   "id": "89d5231fb3a7bd35",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1775, 400, 400, 5])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_land.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:26:22.269712Z",
     "start_time": "2024-08-16T22:26:22.262799Z"
    }
   },
   "id": "4d2edcb5e16fd1a4",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1775, 20, 1])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_seq.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:26:22.276785Z",
     "start_time": "2024-08-16T22:26:22.270716Z"
    }
   },
   "id": "3f4b121f623deebd",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1775, 1])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_target.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:26:22.284204Z",
     "start_time": "2024-08-16T22:26:22.277869Z"
    }
   },
   "id": "bb945151b38c245d",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 定义CNN特征提取器和LSTM模型\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(128 * 50 * 50, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.reshape(-1, 128 * 50 * 50)\n",
    "        x = torch.relu(self.fc(x))\n",
    "        return x\n",
    "\n",
    "class LSTMWithSpatialFeatures(nn.Module):\n",
    "    def __init__(self, seq_length, in_channels=1):\n",
    "        super(LSTMWithSpatialFeatures, self).__init__()  # 确保最开始调用 super().__init__()\n",
    "        # 判断是否使用CNN\n",
    "        if in_channels > 0:\n",
    "            self.use_cnn = True\n",
    "            self.cnn = CNNFeatureExtractor(in_channels=in_channels)\n",
    "            lstm_input_size = 128 + 1  # CNN输出的128维特征 + 1维时间序列数据\n",
    "        else:\n",
    "            self.use_cnn = False\n",
    "            lstm_input_size = 1  # 只有时间序列数据，没有CNN输出\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=lstm_input_size, hidden_size=128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, land_data, temp_seq):\n",
    "        if self.use_cnn:\n",
    "            \n",
    "            # 调整输入形状为 [batch_size, in_channels, height, width]\n",
    "            land_data = land_data.permute(0, 3, 1, 2)  # 之前的形状 [batch_size, 400, 400, 4] -> 新形状 [batch_size, 4, 400, 400]\n",
    "            \n",
    "            cnn_out = self.cnn(land_data)\n",
    "            # 经过 CNN 处理后\n",
    "            # land_data -> [batch_size, 4, 400, 400]\n",
    "            # CNN 的输出 cnn_out -> [batch_size, 128]  # 经过卷积和全连接层后，输出为 128 维的特征向量\n",
    "            \n",
    "            cnn_out = cnn_out.unsqueeze(1).repeat(1, temp_seq.size(1), 1)\n",
    "            # 在第二个维度（时间步长维度）增加一个维度，然后沿着这个维度重复\n",
    "            # cnn_out -> [batch_size, 1, 128] -> [batch_size, seq_length, 128]  # 这里 seq_length = temp_seq.size(1)\n",
    "            \n",
    "            combined_input = torch.cat((cnn_out, temp_seq), dim=2)\n",
    "            # 将 CNN 输出的特征向量和温度序列数据结合\n",
    "            # temp_seq -> [batch_size, seq_length, 1]\n",
    "            # combined_input -> [batch_size, seq_length, 128 + 1] -> [batch_size, seq_length, 129]\n",
    "        else:\n",
    "            combined_input = temp_seq\n",
    "            # combined_input -> [batch_size, seq_length, 1]  只有时间序列数据\n",
    "\n",
    "        lstm_out, _ = self.lstm(combined_input)\n",
    "        # 经过 LSTM 层处理\n",
    "        # combined_input -> [batch_size, seq_length, 129]\n",
    "        # lstm_out -> [batch_size, seq_length, 128]  # LSTM 的输出是 128 维的特征向量        \n",
    "        \n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        # 取 LSTM 最后一层输出，并通过全连接层进行预测\n",
    "        # lstm_out[:, -1, :] -> [batch_size, 128]\n",
    "        # output -> [batch_size, 1]  # 最终输出一个标量，表示下一时间步长的预测值\n",
    "        \n",
    "        return output\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T22:26:23.130529Z",
     "start_time": "2024-08-16T22:26:23.123042Z"
    }
   },
   "id": "6575521a8a4d5b9f",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 训练和验证函数\n",
    "def train_and_evaluate(model, train_loader, val_loader, num_epochs=1):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'----------Epoch {epoch+1}/{num_epochs}----------')\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        print('----------Training----------')\n",
    "        for i, (land_data, temp_seq, target) in enumerate(train_loader):\n",
    "            print(f'Training Epoch {epoch+1}/{num_epochs} Batch {i+1}/{len(train_loader)}')\n",
    "            optimizer.zero_grad()\n",
    "            output = model(land_data, temp_seq)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            print(f'Batch Loss: {loss.item()}')\n",
    "        print(f'Training Loss: {train_loss / len(train_loader)}')\n",
    "    \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        print('----------Validation----------')\n",
    "        with torch.no_grad():\n",
    "            for i, (land_data, temp_seq, target) in enumerate(val_loader):\n",
    "                print(f'Batch {i+1}/{len(val_loader)}')\n",
    "                output = model(land_data, temp_seq)\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item()\n",
    "                print(f'Batch Loss: {loss.item()}')\n",
    "        print(f'Validation Loss: {val_loss / len(val_loader)}')\n",
    "    \n",
    "    return val_loss / len(val_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T02:04:21.049791Z",
     "start_time": "2024-08-16T02:04:21.044251Z"
    }
   },
   "id": "3714f32f1bff50b",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 比较不同模型的函数\n",
    "specific_indices=None\n",
    "results = {}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T02:04:22.032822Z",
     "start_time": "2024-08-16T02:04:22.029606Z"
    }
   },
   "id": "428b8afb27ebeab9",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader_full length: 45\n",
      "val_loader_full length: 12\n",
      "train_loader_channel_0 length: 45\n",
      "val_loader_channel_0 length: 12\n",
      "train_loader_channel_1 length: 45\n",
      "val_loader_channel_1 length: 12\n",
      "train_loader_channel_2 length: 45\n",
      "val_loader_channel_2 length: 12\n",
      "train_loader_channel_3 length: 45\n",
      "val_loader_channel_3 length: 12\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "# 数据集大小\n",
    "total_size = X_land.shape[0]\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "# 创建使用完整 X_land 的 DataLoader\n",
    "train_dataset_full, val_dataset_full = random_split(TensorDataset(X_land, Y_seq, Y_target), [train_size, val_size])\n",
    "\n",
    "train_loader_full = DataLoader(train_dataset_full, batch_size=32, shuffle=True)\n",
    "val_loader_full = DataLoader(val_dataset_full, batch_size=32)\n",
    "\n",
    "# 创建使用单独通道的 DataLoader\n",
    "channels = [0, 1, 2, 3]\n",
    "train_loaders = {}\n",
    "val_loaders = {}\n",
    "\n",
    "for channel in channels:\n",
    "    X_land_channel = X_land[:, :, :, channel:channel+1]  # 选择单个通道\n",
    "    train_dataset_channel, val_dataset_channel = random_split(TensorDataset(X_land_channel, Y_seq, Y_target), [train_size, val_size])\n",
    "\n",
    "    train_loaders[channel] = DataLoader(train_dataset_channel, batch_size=32, shuffle=True)\n",
    "    val_loaders[channel] = DataLoader(val_dataset_channel, batch_size=32)\n",
    "\n",
    "# 结果展示\n",
    "print(\"train_loader_full length:\", len(train_loader_full))\n",
    "print(\"val_loader_full length:\", len(val_loader_full))\n",
    "\n",
    "for channel in channels:\n",
    "    print(f\"train_loader_channel_{channel} length:\", len(train_loaders[channel]))\n",
    "    print(f\"val_loader_channel_{channel} length:\", len(val_loaders[channel]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T02:04:25.367719Z",
     "start_time": "2024-08-16T02:04:25.361601Z"
    }
   },
   "id": "a2b3dce393d067cc",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<torch.utils.data.dataloader.DataLoader at 0x2406d23a590>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loaders[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T02:04:30.504110Z",
     "start_time": "2024-08-16T02:04:30.500555Z"
    }
   },
   "id": "76ac4ec63aadebbd",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "land_data, temp_seq, target = next(iter(train_loaders[0]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T02:04:33.481971Z",
     "start_time": "2024-08-16T02:04:33.468113Z"
    }
   },
   "id": "e8ef4fb0109205b7",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 400, 400, 1])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "land_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T05:14:55.809983Z",
     "start_time": "2024-08-15T05:14:55.805885Z"
    }
   },
   "id": "abb1dc04ec1b28f9",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 20, 1])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_seq.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T05:14:55.893751Z",
     "start_time": "2024-08-15T05:14:55.889566Z"
    }
   },
   "id": "83e3f8571b1113f",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 1])"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T05:14:56.024170Z",
     "start_time": "2024-08-15T05:14:56.019609Z"
    }
   },
   "id": "14e22aeceda9e6ae",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "land_data, temp_seq, target = next(iter(val_loader_full))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T05:14:57.101100Z",
     "start_time": "2024-08-15T05:14:57.075951Z"
    }
   },
   "id": "80ab266a8772e75",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 400, 400, 4])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "land_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T05:14:58.101610Z",
     "start_time": "2024-08-15T05:14:58.097078Z"
    }
   },
   "id": "8ed7370c59772bef",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 20, 1])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_seq.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T05:14:58.143322Z",
     "start_time": "2024-08-15T05:14:58.139978Z"
    }
   },
   "id": "c6d6d9640775ec6e",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 1])"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T05:14:59.242639Z",
     "start_time": "2024-08-15T05:14:59.237873Z"
    }
   },
   "id": "188d5e9ad6325382",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline model...\n",
      "----------Epoch 1/1----------\n",
      "----------Training----------\n",
      "Training Epoch 1/1 Batch 1/45\n",
      "Batch Loss: 200.28225708007812\n",
      "Training Epoch 1/1 Batch 2/45\n",
      "Batch Loss: 209.90423583984375\n",
      "Training Epoch 1/1 Batch 3/45\n",
      "Batch Loss: 197.92901611328125\n",
      "Training Epoch 1/1 Batch 4/45\n",
      "Batch Loss: 187.40908813476562\n",
      "Training Epoch 1/1 Batch 5/45\n",
      "Batch Loss: 170.16781616210938\n",
      "Training Epoch 1/1 Batch 6/45\n",
      "Batch Loss: 187.29562377929688\n",
      "Training Epoch 1/1 Batch 7/45\n",
      "Batch Loss: 191.69630432128906\n",
      "Training Epoch 1/1 Batch 8/45\n",
      "Batch Loss: 143.52725219726562\n",
      "Training Epoch 1/1 Batch 9/45\n",
      "Batch Loss: 180.85992431640625\n",
      "Training Epoch 1/1 Batch 10/45\n",
      "Batch Loss: 136.6522979736328\n",
      "Training Epoch 1/1 Batch 11/45\n",
      "Batch Loss: 152.4853515625\n",
      "Training Epoch 1/1 Batch 12/45\n",
      "Batch Loss: 179.05001831054688\n",
      "Training Epoch 1/1 Batch 13/45\n",
      "Batch Loss: 119.70748138427734\n",
      "Training Epoch 1/1 Batch 14/45\n",
      "Batch Loss: 167.6324005126953\n",
      "Training Epoch 1/1 Batch 15/45\n",
      "Batch Loss: 107.87080383300781\n",
      "Training Epoch 1/1 Batch 16/45\n",
      "Batch Loss: 106.96417236328125\n",
      "Training Epoch 1/1 Batch 17/45\n",
      "Batch Loss: 112.64762115478516\n",
      "Training Epoch 1/1 Batch 18/45\n",
      "Batch Loss: 134.48995971679688\n",
      "Training Epoch 1/1 Batch 19/45\n",
      "Batch Loss: 96.71903228759766\n",
      "Training Epoch 1/1 Batch 20/45\n",
      "Batch Loss: 80.77449798583984\n",
      "Training Epoch 1/1 Batch 21/45\n",
      "Batch Loss: 134.4520263671875\n",
      "Training Epoch 1/1 Batch 22/45\n",
      "Batch Loss: 75.94435119628906\n",
      "Training Epoch 1/1 Batch 23/45\n",
      "Batch Loss: 107.98538208007812\n",
      "Training Epoch 1/1 Batch 24/45\n",
      "Batch Loss: 108.38485717773438\n",
      "Training Epoch 1/1 Batch 25/45\n",
      "Batch Loss: 93.75115203857422\n",
      "Training Epoch 1/1 Batch 26/45\n",
      "Batch Loss: 98.85900115966797\n",
      "Training Epoch 1/1 Batch 27/45\n",
      "Batch Loss: 62.10164260864258\n",
      "Training Epoch 1/1 Batch 28/45\n",
      "Batch Loss: 94.54383087158203\n",
      "Training Epoch 1/1 Batch 29/45\n",
      "Batch Loss: 96.23233032226562\n",
      "Training Epoch 1/1 Batch 30/45\n",
      "Batch Loss: 83.53892517089844\n",
      "Training Epoch 1/1 Batch 31/45\n",
      "Batch Loss: 91.4023208618164\n",
      "Training Epoch 1/1 Batch 32/45\n",
      "Batch Loss: 97.51873016357422\n",
      "Training Epoch 1/1 Batch 33/45\n",
      "Batch Loss: 69.83251190185547\n",
      "Training Epoch 1/1 Batch 34/45\n",
      "Batch Loss: 64.54768371582031\n",
      "Training Epoch 1/1 Batch 35/45\n",
      "Batch Loss: 60.061832427978516\n",
      "Training Epoch 1/1 Batch 36/45\n",
      "Batch Loss: 69.70318603515625\n",
      "Training Epoch 1/1 Batch 37/45\n",
      "Batch Loss: 52.412940979003906\n",
      "Training Epoch 1/1 Batch 38/45\n",
      "Batch Loss: 51.05398178100586\n",
      "Training Epoch 1/1 Batch 39/45\n",
      "Batch Loss: 73.49111938476562\n",
      "Training Epoch 1/1 Batch 40/45\n",
      "Batch Loss: 56.45133972167969\n",
      "Training Epoch 1/1 Batch 41/45\n",
      "Batch Loss: 56.41866683959961\n",
      "Training Epoch 1/1 Batch 42/45\n",
      "Batch Loss: 41.96757888793945\n",
      "Training Epoch 1/1 Batch 43/45\n",
      "Batch Loss: 57.97153854370117\n",
      "Training Epoch 1/1 Batch 44/45\n",
      "Batch Loss: 35.69123840332031\n",
      "Training Epoch 1/1 Batch 45/45\n",
      "Batch Loss: 45.94228744506836\n",
      "Training Loss: 109.87390246921116\n",
      "----------Validation----------\n",
      "Batch 1/12\n",
      "Batch Loss: 31.480607986450195\n",
      "Batch 2/12\n",
      "Batch Loss: 34.37541961669922\n",
      "Batch 3/12\n",
      "Batch Loss: 40.843299865722656\n",
      "Batch 4/12\n",
      "Batch Loss: 64.22547149658203\n",
      "Batch 5/12\n",
      "Batch Loss: 50.15366744995117\n",
      "Batch 6/12\n",
      "Batch Loss: 67.01234436035156\n",
      "Batch 7/12\n",
      "Batch Loss: 48.00244903564453\n",
      "Batch 8/12\n",
      "Batch Loss: 43.716217041015625\n",
      "Batch 9/12\n",
      "Batch Loss: 39.96393966674805\n",
      "Batch 10/12\n",
      "Batch Loss: 41.85881805419922\n",
      "Batch 11/12\n",
      "Batch Loss: 49.3597412109375\n",
      "Batch 12/12\n",
      "Batch Loss: 27.50516700744629\n",
      "Validation Loss: 44.87476189931234\n",
      "Baseline MSE: 44.87476189931234\n",
      "CPU times: total: 516 ms\n",
      "Wall time: 1.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Baseline\n",
    "print(\"Training baseline model...\")\n",
    "model_baseline = LSTMWithSpatialFeatures(seq_length=seq_length, in_channels=0)  # 无空间数据\n",
    "results[\"Baseline\"] = train_and_evaluate(model_baseline, train_loader_full, val_loader_full)\n",
    "print(f\"Baseline MSE: {results['Baseline']}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T02:04:38.551352Z",
     "start_time": "2024-08-16T02:04:37.042600Z"
    }
   },
   "id": "c146335f0e32528",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training baseline + all 5 spatial features model...\n",
      "----------Epoch 1/1----------\n",
      "----------Training----------\n",
      "Training Epoch 1/1 Batch 1/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 2/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 3/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 4/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 5/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 6/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 7/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 8/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 9/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 10/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 11/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 12/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 13/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 14/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 15/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 16/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 17/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 18/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 19/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 20/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 21/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 22/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 23/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 24/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 25/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 26/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 27/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 28/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 29/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 30/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 31/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 32/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 33/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 34/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 35/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 36/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 37/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 38/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 39/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 40/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 41/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 42/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 43/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 44/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 45/45\n",
      "Batch Loss: nan\n",
      "Training Loss: nan\n",
      "----------Validation----------\n",
      "Batch 1/12\n",
      "Batch Loss: nan\n",
      "Batch 2/12\n",
      "Batch Loss: nan\n",
      "Batch 3/12\n",
      "Batch Loss: nan\n",
      "Batch 4/12\n",
      "Batch Loss: nan\n",
      "Batch 5/12\n",
      "Batch Loss: nan\n",
      "Batch 6/12\n",
      "Batch Loss: nan\n",
      "Batch 7/12\n",
      "Batch Loss: nan\n",
      "Batch 8/12\n",
      "Batch Loss: nan\n",
      "Batch 9/12\n",
      "Batch Loss: nan\n",
      "Batch 10/12\n",
      "Batch Loss: nan\n",
      "Batch 11/12\n",
      "Batch Loss: nan\n",
      "Batch 12/12\n",
      "Batch Loss: nan\n",
      "Validation Loss: nan\n",
      "Baseline + all 5 features MSE: nan\n",
      "CPU times: total: 46.5 s\n",
      "Wall time: 4min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Baseline + n种所有空间数据\n",
    "print(f\"\\nTraining baseline + all {n} spatial features model...\")\n",
    "model_baseline_n = LSTMWithSpatialFeatures(seq_length=seq_length, in_channels=n)\n",
    "results[f\"Baseline + all {n} features\"] = train_and_evaluate(model_baseline_n, train_loader_full, val_loader_full)\n",
    "print(f\"Baseline + all {n} features MSE: {results[f'Baseline + all {n} features']}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T02:09:29.074710Z",
     "start_time": "2024-08-16T02:04:41.321006Z"
    }
   },
   "id": "5f06631e90dd78d8",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Baseline + 单个空间数据\n",
    "model_baseline_BH = LSTMWithSpatialFeatures(seq_length=seq_length, in_channels=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T02:09:29.193303Z",
     "start_time": "2024-08-16T02:09:29.075736Z"
    }
   },
   "id": "665726e4670fb1cf",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Epoch 1/1----------\n",
      "----------Training----------\n",
      "Training Epoch 1/1 Batch 1/45\n",
      "Batch Loss: 208.8105926513672\n",
      "Training Epoch 1/1 Batch 2/45\n",
      "Batch Loss: 226.22238159179688\n",
      "Training Epoch 1/1 Batch 3/45\n",
      "Batch Loss: 260.3247375488281\n",
      "Training Epoch 1/1 Batch 4/45\n",
      "Batch Loss: 243.3546600341797\n",
      "Training Epoch 1/1 Batch 5/45\n",
      "Batch Loss: 173.8017578125\n",
      "Training Epoch 1/1 Batch 6/45\n",
      "Batch Loss: 221.7898406982422\n",
      "Training Epoch 1/1 Batch 7/45\n",
      "Batch Loss: 209.67340087890625\n",
      "Training Epoch 1/1 Batch 8/45\n",
      "Batch Loss: 177.2684326171875\n",
      "Training Epoch 1/1 Batch 9/45\n",
      "Batch Loss: 154.1030731201172\n",
      "Training Epoch 1/1 Batch 10/45\n",
      "Batch Loss: 178.9468994140625\n",
      "Training Epoch 1/1 Batch 11/45\n",
      "Batch Loss: 201.61358642578125\n",
      "Training Epoch 1/1 Batch 12/45\n",
      "Batch Loss: 182.45912170410156\n",
      "Training Epoch 1/1 Batch 13/45\n",
      "Batch Loss: 207.8770751953125\n",
      "Training Epoch 1/1 Batch 14/45\n",
      "Batch Loss: 168.11007690429688\n",
      "Training Epoch 1/1 Batch 15/45\n",
      "Batch Loss: 217.3198699951172\n",
      "Training Epoch 1/1 Batch 16/45\n",
      "Batch Loss: 228.38653564453125\n",
      "Training Epoch 1/1 Batch 17/45\n",
      "Batch Loss: 180.8294677734375\n",
      "Training Epoch 1/1 Batch 18/45\n",
      "Batch Loss: 217.52896118164062\n",
      "Training Epoch 1/1 Batch 19/45\n",
      "Batch Loss: 164.118896484375\n",
      "Training Epoch 1/1 Batch 20/45\n",
      "Batch Loss: 192.44972229003906\n",
      "Training Epoch 1/1 Batch 21/45\n",
      "Batch Loss: 203.05355834960938\n",
      "Training Epoch 1/1 Batch 22/45\n",
      "Batch Loss: 203.1389617919922\n",
      "Training Epoch 1/1 Batch 23/45\n",
      "Batch Loss: 186.8905792236328\n",
      "Training Epoch 1/1 Batch 24/45\n",
      "Batch Loss: 244.3063507080078\n",
      "Training Epoch 1/1 Batch 25/45\n",
      "Batch Loss: 222.8689422607422\n",
      "Training Epoch 1/1 Batch 26/45\n",
      "Batch Loss: 230.5391845703125\n",
      "Training Epoch 1/1 Batch 27/45\n",
      "Batch Loss: 158.2346954345703\n",
      "Training Epoch 1/1 Batch 28/45\n",
      "Batch Loss: 193.64308166503906\n",
      "Training Epoch 1/1 Batch 29/45\n",
      "Batch Loss: 185.601806640625\n",
      "Training Epoch 1/1 Batch 30/45\n",
      "Batch Loss: 174.5165557861328\n",
      "Training Epoch 1/1 Batch 31/45\n",
      "Batch Loss: 206.2461395263672\n",
      "Training Epoch 1/1 Batch 32/45\n",
      "Batch Loss: 140.60975646972656\n",
      "Training Epoch 1/1 Batch 33/45\n",
      "Batch Loss: 167.5691680908203\n",
      "Training Epoch 1/1 Batch 34/45\n",
      "Batch Loss: 188.47628784179688\n",
      "Training Epoch 1/1 Batch 35/45\n",
      "Batch Loss: 183.4043731689453\n",
      "Training Epoch 1/1 Batch 36/45\n",
      "Batch Loss: 198.5200958251953\n",
      "Training Epoch 1/1 Batch 37/45\n",
      "Batch Loss: 213.7034912109375\n",
      "Training Epoch 1/1 Batch 38/45\n",
      "Batch Loss: 226.2696533203125\n",
      "Training Epoch 1/1 Batch 39/45\n",
      "Batch Loss: 160.9865264892578\n",
      "Training Epoch 1/1 Batch 40/45\n",
      "Batch Loss: 162.9607696533203\n",
      "Training Epoch 1/1 Batch 41/45\n",
      "Batch Loss: 178.55152893066406\n",
      "Training Epoch 1/1 Batch 42/45\n",
      "Batch Loss: 212.2119140625\n",
      "Training Epoch 1/1 Batch 43/45\n",
      "Batch Loss: 150.6669921875\n",
      "Training Epoch 1/1 Batch 44/45\n",
      "Batch Loss: 206.12388610839844\n",
      "Training Epoch 1/1 Batch 45/45\n",
      "Batch Loss: 158.77899169921875\n",
      "Training Loss: 194.95249735514324\n",
      "----------Validation----------\n",
      "Batch 1/12\n",
      "Batch Loss: 206.322021484375\n",
      "Batch 2/12\n",
      "Batch Loss: 171.50575256347656\n",
      "Batch 3/12\n",
      "Batch Loss: 161.5903778076172\n",
      "Batch 4/12\n",
      "Batch Loss: 153.40603637695312\n",
      "Batch 5/12\n",
      "Batch Loss: 183.4972686767578\n",
      "Batch 6/12\n",
      "Batch Loss: 170.83462524414062\n",
      "Batch 7/12\n",
      "Batch Loss: 143.7149200439453\n",
      "Batch 8/12\n",
      "Batch Loss: 165.2462615966797\n",
      "Batch 9/12\n",
      "Batch Loss: 183.27606201171875\n",
      "Batch 10/12\n",
      "Batch Loss: 141.2197265625\n",
      "Batch 11/12\n",
      "Batch Loss: 203.45899963378906\n",
      "Batch 12/12\n",
      "Batch Loss: 136.2345428466797\n",
      "Validation Loss: 168.35888290405273\n",
      "CPU times: total: 9.72 s\n",
      "Wall time: 5min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results[f\"Baseline + feature BH\"] = train_and_evaluate(model_baseline_BH, train_loaders[0], val_loaders[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T02:14:33.010502Z",
     "start_time": "2024-08-16T02:09:29.194475Z"
    }
   },
   "id": "7b5059628750504f",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_baseline_BV = LSTMWithSpatialFeatures(seq_length=seq_length, in_channels=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T02:14:33.120888Z",
     "start_time": "2024-08-16T02:14:33.011516Z"
    }
   },
   "id": "1d5b47ad8b1943c1",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Epoch 1/1----------\n",
      "----------Training----------\n",
      "Training Epoch 1/1 Batch 1/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 2/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 3/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 4/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 5/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 6/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 7/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 8/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 9/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 10/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 11/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 12/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 13/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 14/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 15/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 16/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 17/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 18/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 19/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 20/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 21/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 22/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 23/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 24/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 25/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 26/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 27/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 28/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 29/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 30/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 31/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 32/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 33/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 34/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 35/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 36/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 37/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 38/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 39/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 40/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 41/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 42/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 43/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 44/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 45/45\n",
      "Batch Loss: nan\n",
      "Training Loss: nan\n",
      "----------Validation----------\n",
      "Batch 1/12\n",
      "Batch Loss: nan\n",
      "Batch 2/12\n",
      "Batch Loss: nan\n",
      "Batch 3/12\n",
      "Batch Loss: nan\n",
      "Batch 4/12\n",
      "Batch Loss: nan\n",
      "Batch 5/12\n",
      "Batch Loss: nan\n",
      "Batch 6/12\n",
      "Batch Loss: nan\n",
      "Batch 7/12\n",
      "Batch Loss: nan\n",
      "Batch 8/12\n",
      "Batch Loss: nan\n",
      "Batch 9/12\n",
      "Batch Loss: nan\n",
      "Batch 10/12\n",
      "Batch Loss: nan\n",
      "Batch 11/12\n",
      "Batch Loss: nan\n",
      "Batch 12/12\n",
      "Batch Loss: nan\n",
      "Validation Loss: nan\n",
      "CPU times: total: 32.1 s\n",
      "Wall time: 4min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results[f\"Baseline + feature BV\"] = train_and_evaluate(model_baseline_BV, train_loaders[1], val_loaders[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T02:19:20.969454Z",
     "start_time": "2024-08-16T02:14:33.121891Z"
    }
   },
   "id": "e0fe67e6a6cc06d6",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_baseline_CNM = LSTMWithSpatialFeatures(seq_length=seq_length, in_channels=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T02:19:21.088052Z",
     "start_time": "2024-08-16T02:19:20.970533Z"
    }
   },
   "id": "31112ab2a49c0f6b",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Epoch 1/1----------\n",
      "----------Training----------\n",
      "Training Epoch 1/1 Batch 1/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 2/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 3/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 4/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 5/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 6/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 7/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 8/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 9/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 10/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 11/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 12/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 13/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 14/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 15/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 16/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 17/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 18/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 19/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 20/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 21/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 22/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 23/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 24/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 25/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 26/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 27/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 28/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 29/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 30/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 31/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 32/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 33/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 34/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 35/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 36/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 37/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 38/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 39/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 40/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 41/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 42/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 43/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 44/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 45/45\n",
      "Batch Loss: nan\n",
      "Training Loss: nan\n",
      "----------Validation----------\n",
      "Batch 1/12\n",
      "Batch Loss: nan\n",
      "Batch 2/12\n",
      "Batch Loss: nan\n",
      "Batch 3/12\n",
      "Batch Loss: nan\n",
      "Batch 4/12\n",
      "Batch Loss: nan\n",
      "Batch 5/12\n",
      "Batch Loss: nan\n",
      "Batch 6/12\n",
      "Batch Loss: nan\n",
      "Batch 7/12\n",
      "Batch Loss: nan\n",
      "Batch 8/12\n",
      "Batch Loss: nan\n",
      "Batch 9/12\n",
      "Batch Loss: nan\n",
      "Batch 10/12\n",
      "Batch Loss: nan\n",
      "Batch 11/12\n",
      "Batch Loss: nan\n",
      "Batch 12/12\n",
      "Batch Loss: nan\n",
      "Validation Loss: nan\n",
      "CPU times: total: 23.2 s\n",
      "Wall time: 4min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results[f\"Baseline + feature CNM\"] = train_and_evaluate(model_baseline_CNM, train_loaders[2], val_loaders[2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T02:24:18.615635Z",
     "start_time": "2024-08-16T02:19:21.090670Z"
    }
   },
   "id": "cbc63965fd1f6572",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_baseline_LAI = LSTMWithSpatialFeatures(seq_length=seq_length, in_channels=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T02:24:18.730992Z",
     "start_time": "2024-08-16T02:24:18.616235Z"
    }
   },
   "id": "5578347f55890b12",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Epoch 1/1----------\n",
      "----------Training----------\n",
      "Training Epoch 1/1 Batch 1/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 2/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 3/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 4/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 5/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 6/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 7/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 8/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 9/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 10/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 11/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 12/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 13/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 14/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 15/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 16/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 17/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 18/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 19/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 20/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 21/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 22/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 23/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 24/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 25/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 26/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 27/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 28/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 29/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 30/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 31/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 32/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 33/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 34/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 35/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 36/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 37/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 38/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 39/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 40/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 41/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 42/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 43/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 44/45\n",
      "Batch Loss: nan\n",
      "Training Epoch 1/1 Batch 45/45\n",
      "Batch Loss: nan\n",
      "Training Loss: nan\n",
      "----------Validation----------\n",
      "Batch 1/12\n",
      "Batch Loss: nan\n",
      "Batch 2/12\n",
      "Batch Loss: nan\n",
      "Batch 3/12\n",
      "Batch Loss: nan\n",
      "Batch 4/12\n",
      "Batch Loss: nan\n",
      "Batch 5/12\n",
      "Batch Loss: nan\n",
      "Batch 6/12\n",
      "Batch Loss: nan\n",
      "Batch 7/12\n",
      "Batch Loss: nan\n",
      "Batch 8/12\n",
      "Batch Loss: nan\n",
      "Batch 9/12\n",
      "Batch Loss: nan\n",
      "Batch 10/12\n",
      "Batch Loss: nan\n",
      "Batch 11/12\n",
      "Batch Loss: nan\n",
      "Batch 12/12\n",
      "Batch Loss: nan\n",
      "Validation Loss: nan\n",
      "CPU times: total: 18.8 s\n",
      "Wall time: 5min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results[f\"Baseline + feature LAI\"] = train_and_evaluate(model_baseline_LAI, train_loaders[3], val_loaders[3])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T02:29:40.009105Z",
     "start_time": "2024-08-16T02:24:18.733091Z"
    }
   },
   "id": "1a983f5a05efc898",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_baseline_DSM = LSTMWithSpatialFeatures(seq_length=seq_length, in_channels=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T02:29:40.142017Z",
     "start_time": "2024-08-16T02:29:40.011113Z"
    }
   },
   "id": "d61b1ad4fac4a1b7",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "4",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m<timed exec>:1\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 4"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results[f\"Baseline + feature DSM\"] = train_and_evaluate(model_baseline_DSM, train_loaders[4], val_loaders[4])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T02:29:40.405080Z",
     "start_time": "2024-08-16T02:29:40.143029Z"
    }
   },
   "id": "90303a53b285f231",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_x = {}\n",
    "\n",
    "def train_model_x(i):\n",
    "    print(f\"\\nTraining baseline + spatial feature {i+1}/{n} model...\")\n",
    "    model_x[f'model_baseline_{i}'] = LSTMWithSpatialFeatures(seq_length=seq_length, in_channels=1)\n",
    "    X_land_single_channel = X_land[:, :, :, i:i+1]\n",
    "    \n",
    "    train_dataset_single = TensorDataset(X_land_single_channel, Y_seq, Y_target)\n",
    "    val_dataset_single = TensorDataset(X_land_single_channel, Y_seq, Y_target)\n",
    "    \n",
    "    train_loader_single = DataLoader(train_dataset_single, batch_size=32, shuffle=True)\n",
    "    val_loader_single = DataLoader(val_dataset_single, batch_size=32)\n",
    "    \n",
    "    results[f\"Baseline + feature {i+1}\"] = train_and_evaluate(model_x[f'model_baseline_{i}'], train_loader_single, val_loader_single)\n",
    "    \n",
    "    print(f\"Baseline + feature {i+1} MSE: {results[f'Baseline + feature {i+1}']}\")\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T01:25:22.513206Z",
     "start_time": "2024-08-15T01:25:22.508758Z"
    }
   },
   "id": "8ad0cde6182d2b8c",
   "execution_count": 163
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training baseline + spatial feature 1/4 model...\n",
      "----------Epoch 1/10----------\n",
      "----------Training----------\n",
      "Batch 1/56\n",
      "Batch Loss: 229.70556640625\n",
      "Batch 2/56\n",
      "Batch Loss: 228.86865234375\n",
      "Batch 3/56\n",
      "Batch Loss: 168.5008544921875\n",
      "Batch 4/56\n",
      "Batch Loss: 182.23793029785156\n",
      "Batch 5/56\n",
      "Batch Loss: 223.1148223876953\n",
      "Batch 6/56\n",
      "Batch Loss: 172.73814392089844\n",
      "Batch 7/56\n",
      "Batch Loss: 189.8527374267578\n",
      "Batch 8/56\n",
      "Batch Loss: 317.1990966796875\n",
      "Batch 9/56\n",
      "Batch Loss: 156.36253356933594\n",
      "Batch 10/56\n",
      "Batch Loss: 200.8006591796875\n",
      "Batch 11/56\n",
      "Batch Loss: 184.1835479736328\n",
      "Batch 12/56\n",
      "Batch Loss: 175.05078125\n",
      "Batch 13/56\n",
      "Batch Loss: 202.91400146484375\n",
      "Batch 14/56\n",
      "Batch Loss: 218.0608367919922\n",
      "Batch 15/56\n",
      "Batch Loss: 183.1203155517578\n",
      "Batch 16/56\n",
      "Batch Loss: 174.67164611816406\n",
      "Batch 17/56\n",
      "Batch Loss: 168.21669006347656\n",
      "Batch 18/56\n",
      "Batch Loss: 202.68951416015625\n",
      "Batch 19/56\n",
      "Batch Loss: 166.89865112304688\n",
      "Batch 20/56\n",
      "Batch Loss: 196.18125915527344\n",
      "Batch 21/56\n",
      "Batch Loss: 236.97222900390625\n",
      "Batch 22/56\n",
      "Batch Loss: 160.9925537109375\n",
      "Batch 23/56\n",
      "Batch Loss: 174.38002014160156\n",
      "Batch 24/56\n",
      "Batch Loss: 129.0749053955078\n",
      "Batch 25/56\n",
      "Batch Loss: 197.56951904296875\n",
      "Batch 26/56\n",
      "Batch Loss: 197.0603485107422\n",
      "Batch 27/56\n",
      "Batch Loss: 178.20156860351562\n",
      "Batch 28/56\n",
      "Batch Loss: 198.20375061035156\n",
      "Batch 29/56\n",
      "Batch Loss: 180.59352111816406\n",
      "Batch 30/56\n",
      "Batch Loss: 215.83497619628906\n",
      "Batch 31/56\n",
      "Batch Loss: 180.1488037109375\n",
      "Batch 32/56\n",
      "Batch Loss: 157.8447723388672\n",
      "Batch 33/56\n",
      "Batch Loss: 186.22689819335938\n",
      "Batch 34/56\n",
      "Batch Loss: 166.30850219726562\n",
      "Batch 35/56\n",
      "Batch Loss: 210.13424682617188\n",
      "Batch 36/56\n",
      "Batch Loss: 202.72451782226562\n",
      "Batch 37/56\n",
      "Batch Loss: 185.2648162841797\n",
      "Batch 38/56\n",
      "Batch Loss: 201.23797607421875\n",
      "Batch 39/56\n",
      "Batch Loss: 206.59503173828125\n",
      "Batch 40/56\n",
      "Batch Loss: 218.14759826660156\n",
      "Batch 41/56\n",
      "Batch Loss: 154.20120239257812\n",
      "Batch 42/56\n",
      "Batch Loss: 157.34262084960938\n",
      "Batch 43/56\n",
      "Batch Loss: 183.4133758544922\n",
      "Batch 44/56\n",
      "Batch Loss: 179.1504364013672\n",
      "Batch 45/56\n",
      "Batch Loss: 161.88230895996094\n",
      "Batch 46/56\n",
      "Batch Loss: 178.62281799316406\n",
      "Batch 47/56\n",
      "Batch Loss: 223.4420623779297\n",
      "Batch 48/56\n",
      "Batch Loss: 169.09194946289062\n",
      "Batch 49/56\n",
      "Batch Loss: 180.55047607421875\n",
      "Batch 50/56\n",
      "Batch Loss: 184.444580078125\n",
      "Batch 51/56\n",
      "Batch Loss: 138.1510467529297\n",
      "Batch 52/56\n",
      "Batch Loss: 150.12135314941406\n",
      "Batch 53/56\n",
      "Batch Loss: 161.24200439453125\n",
      "Batch 54/56\n",
      "Batch Loss: 128.79714965820312\n",
      "Batch 55/56\n",
      "Batch Loss: 145.1486053466797\n",
      "Batch 56/56\n",
      "Batch Loss: 137.2900390625\n",
      "Training Loss: 184.96030044555664\n",
      "----------Validation----------\n",
      "Batch 1/56\n",
      "Batch Loss: 35.57997512817383\n",
      "Batch 2/56\n",
      "Batch Loss: 64.1496810913086\n",
      "Batch 3/56\n",
      "Batch Loss: 142.21771240234375\n",
      "Batch 4/56\n",
      "Batch Loss: 130.09336853027344\n",
      "Batch 5/56\n",
      "Batch Loss: 225.674072265625\n",
      "Batch 6/56\n",
      "Batch Loss: 277.78173828125\n",
      "Batch 7/56\n",
      "Batch Loss: 293.025634765625\n",
      "Batch 8/56\n",
      "Batch Loss: 236.0008544921875\n",
      "Batch 9/56\n",
      "Batch Loss: 155.3914794921875\n",
      "Batch 10/56\n",
      "Batch Loss: 93.57505798339844\n",
      "Batch 11/56\n",
      "Batch Loss: 76.47796630859375\n",
      "Batch 12/56\n",
      "Batch Loss: 43.84666061401367\n",
      "Batch 13/56\n",
      "Batch Loss: 34.63677215576172\n",
      "Batch 14/56\n",
      "Batch Loss: 85.87905883789062\n",
      "Batch 15/56\n",
      "Batch Loss: 163.00596618652344\n",
      "Batch 16/56\n",
      "Batch Loss: 297.98321533203125\n",
      "Batch 17/56\n",
      "Batch Loss: 248.01170349121094\n",
      "Batch 18/56\n",
      "Batch Loss: 332.6583557128906\n",
      "Batch 19/56\n",
      "Batch Loss: 299.3142395019531\n",
      "Batch 20/56\n",
      "Batch Loss: 176.71649169921875\n",
      "Batch 21/56\n",
      "Batch Loss: 80.2481689453125\n",
      "Batch 22/56\n",
      "Batch Loss: 61.37974548339844\n",
      "Batch 23/56\n",
      "Batch Loss: 55.82861328125\n",
      "Batch 24/56\n",
      "Batch Loss: 47.88634490966797\n",
      "Batch 25/56\n",
      "Batch Loss: 92.10835266113281\n",
      "Batch 26/56\n",
      "Batch Loss: 120.99536895751953\n",
      "Batch 27/56\n",
      "Batch Loss: 267.56158447265625\n",
      "Batch 28/56\n",
      "Batch Loss: 255.41322326660156\n",
      "Batch 29/56\n",
      "Batch Loss: 302.4596252441406\n",
      "Batch 30/56\n",
      "Batch Loss: 269.02410888671875\n",
      "Batch 31/56\n",
      "Batch Loss: 194.244140625\n",
      "Batch 32/56\n",
      "Batch Loss: 147.11492919921875\n",
      "Batch 33/56\n",
      "Batch Loss: 51.1490364074707\n",
      "Batch 34/56\n",
      "Batch Loss: 35.78812789916992\n",
      "Batch 35/56\n",
      "Batch Loss: 33.244171142578125\n",
      "Batch 36/56\n",
      "Batch Loss: 31.977157592773438\n",
      "Batch 37/56\n",
      "Batch Loss: 77.00304412841797\n",
      "Batch 38/56\n",
      "Batch Loss: 196.445068359375\n",
      "Batch 39/56\n",
      "Batch Loss: 350.1482238769531\n",
      "Batch 40/56\n",
      "Batch Loss: 442.8675537109375\n",
      "Batch 41/56\n",
      "Batch Loss: 310.4653015136719\n",
      "Batch 42/56\n",
      "Batch Loss: 221.96322631835938\n",
      "Batch 43/56\n",
      "Batch Loss: 159.69882202148438\n",
      "Batch 44/56\n",
      "Batch Loss: 88.0509033203125\n",
      "Batch 45/56\n",
      "Batch Loss: 56.39766311645508\n",
      "Batch 46/56\n",
      "Batch Loss: 45.20948028564453\n",
      "Batch 47/56\n",
      "Batch Loss: 92.3796157836914\n",
      "Batch 48/56\n",
      "Batch Loss: 80.89730834960938\n",
      "Batch 49/56\n",
      "Batch Loss: 177.77182006835938\n",
      "Batch 50/56\n",
      "Batch Loss: 230.6241455078125\n",
      "Batch 51/56\n",
      "Batch Loss: 301.0822448730469\n",
      "Batch 52/56\n",
      "Batch Loss: 422.3070068359375\n",
      "Batch 53/56\n",
      "Batch Loss: 296.6396179199219\n",
      "Batch 54/56\n",
      "Batch Loss: 215.18655395507812\n",
      "Batch 55/56\n",
      "Batch Loss: 89.26704406738281\n",
      "Batch 56/56\n",
      "Batch Loss: 43.90779495239258\n",
      "Validation Loss: 167.08491325378418\n",
      "----------Epoch 2/10----------\n",
      "----------Training----------\n",
      "Batch 1/56\n",
      "Batch Loss: 132.299072265625\n",
      "Batch 2/56\n",
      "Batch Loss: 164.29046630859375\n",
      "Batch 3/56\n",
      "Batch Loss: 212.7269287109375\n",
      "Batch 4/56\n",
      "Batch Loss: 169.94602966308594\n",
      "Batch 5/56\n",
      "Batch Loss: 170.3847198486328\n",
      "Batch 6/56\n",
      "Batch Loss: 149.62722778320312\n",
      "Batch 7/56\n",
      "Batch Loss: 153.9029541015625\n",
      "Batch 8/56\n",
      "Batch Loss: 198.51193237304688\n",
      "Batch 9/56\n",
      "Batch Loss: 171.03704833984375\n",
      "Batch 10/56\n",
      "Batch Loss: 188.98590087890625\n",
      "Batch 11/56\n",
      "Batch Loss: 178.36204528808594\n",
      "Batch 12/56\n",
      "Batch Loss: 219.3467559814453\n",
      "Batch 13/56\n",
      "Batch Loss: 136.2312469482422\n",
      "Batch 14/56\n",
      "Batch Loss: 147.39410400390625\n",
      "Batch 15/56\n",
      "Batch Loss: 151.17779541015625\n",
      "Batch 16/56\n",
      "Batch Loss: 174.3199920654297\n",
      "Batch 17/56\n",
      "Batch Loss: 169.80117797851562\n",
      "Batch 18/56\n",
      "Batch Loss: 130.9316864013672\n",
      "Batch 19/56\n",
      "Batch Loss: 138.13197326660156\n",
      "Batch 20/56\n",
      "Batch Loss: 130.16798400878906\n",
      "Batch 21/56\n",
      "Batch Loss: 170.35105895996094\n",
      "Batch 22/56\n",
      "Batch Loss: 168.98814392089844\n",
      "Batch 23/56\n",
      "Batch Loss: 173.497314453125\n",
      "Batch 24/56\n",
      "Batch Loss: 170.94960021972656\n",
      "Batch 25/56\n",
      "Batch Loss: 135.40005493164062\n",
      "Batch 26/56\n",
      "Batch Loss: 151.35443115234375\n",
      "Batch 27/56\n",
      "Batch Loss: 148.3568572998047\n",
      "Batch 28/56\n",
      "Batch Loss: 186.8416748046875\n",
      "Batch 29/56\n",
      "Batch Loss: 152.15701293945312\n",
      "Batch 30/56\n",
      "Batch Loss: 116.56861114501953\n",
      "Batch 31/56\n",
      "Batch Loss: 149.64260864257812\n",
      "Batch 32/56\n",
      "Batch Loss: 132.77587890625\n",
      "Batch 33/56\n",
      "Batch Loss: 181.4368896484375\n",
      "Batch 34/56\n",
      "Batch Loss: 147.8431396484375\n",
      "Batch 35/56\n",
      "Batch Loss: 123.62791442871094\n",
      "Batch 36/56\n",
      "Batch Loss: 124.31198120117188\n",
      "Batch 37/56\n",
      "Batch Loss: 168.7516326904297\n",
      "Batch 38/56\n",
      "Batch Loss: 113.12004089355469\n",
      "Batch 39/56\n",
      "Batch Loss: 182.38087463378906\n",
      "Batch 40/56\n",
      "Batch Loss: 160.80902099609375\n",
      "Batch 41/56\n",
      "Batch Loss: 151.89846801757812\n",
      "Batch 42/56\n",
      "Batch Loss: 132.83851623535156\n",
      "Batch 43/56\n",
      "Batch Loss: 192.41929626464844\n",
      "Batch 44/56\n",
      "Batch Loss: 127.10197448730469\n",
      "Batch 45/56\n",
      "Batch Loss: 177.59634399414062\n",
      "Batch 46/56\n",
      "Batch Loss: 168.64727783203125\n",
      "Batch 47/56\n",
      "Batch Loss: 113.52861022949219\n",
      "Batch 48/56\n",
      "Batch Loss: 121.21961975097656\n",
      "Batch 49/56\n",
      "Batch Loss: 155.43934631347656\n",
      "Batch 50/56\n",
      "Batch Loss: 152.8711700439453\n",
      "Batch 51/56\n",
      "Batch Loss: 138.88021850585938\n",
      "Batch 52/56\n",
      "Batch Loss: 104.52588653564453\n",
      "Batch 53/56\n",
      "Batch Loss: 155.873046875\n",
      "Batch 54/56\n",
      "Batch Loss: 130.62664794921875\n",
      "Batch 55/56\n",
      "Batch Loss: 129.5587615966797\n",
      "Batch 56/56\n",
      "Batch Loss: 81.3326187133789\n",
      "Training Loss: 153.23392118726457\n",
      "----------Validation----------\n",
      "Batch 1/56\n",
      "Batch Loss: 23.139028549194336\n",
      "Batch 2/56\n",
      "Batch Loss: 45.96796798706055\n",
      "Batch 3/56\n",
      "Batch Loss: 114.41455841064453\n",
      "Batch 4/56\n",
      "Batch Loss: 103.0068130493164\n",
      "Batch 5/56\n",
      "Batch Loss: 189.4868621826172\n",
      "Batch 6/56\n",
      "Batch Loss: 237.26449584960938\n",
      "Batch 7/56\n",
      "Batch Loss: 251.02626037597656\n",
      "Batch 8/56\n",
      "Batch Loss: 198.4895782470703\n",
      "Batch 9/56\n",
      "Batch Loss: 125.25800323486328\n",
      "Batch 10/56\n",
      "Batch Loss: 71.64667510986328\n",
      "Batch 11/56\n",
      "Batch Loss: 57.09817886352539\n",
      "Batch 12/56\n",
      "Batch Loss: 30.682703018188477\n",
      "Batch 13/56\n",
      "Batch Loss: 21.671823501586914\n",
      "Batch 14/56\n",
      "Batch Loss: 64.05980682373047\n",
      "Batch 15/56\n",
      "Batch Loss: 133.61636352539062\n",
      "Batch 16/56\n",
      "Batch Loss: 256.328125\n",
      "Batch 17/56\n",
      "Batch Loss: 209.45884704589844\n",
      "Batch 18/56\n",
      "Batch Loss: 288.09991455078125\n",
      "Batch 19/56\n",
      "Batch Loss: 256.7427062988281\n",
      "Batch 20/56\n",
      "Batch Loss: 144.49661254882812\n",
      "Batch 21/56\n",
      "Batch Loss: 60.6517219543457\n",
      "Batch 22/56\n",
      "Batch Loss: 44.46912384033203\n",
      "Batch 23/56\n",
      "Batch Loss: 39.531864166259766\n",
      "Batch 24/56\n",
      "Batch Loss: 33.07575225830078\n",
      "Batch 25/56\n",
      "Batch Loss: 70.21446990966797\n",
      "Batch 26/56\n",
      "Batch Loss: 94.77144622802734\n",
      "Batch 27/56\n",
      "Batch Loss: 227.89976501464844\n",
      "Batch 28/56\n",
      "Batch Loss: 216.60037231445312\n",
      "Batch 29/56\n",
      "Batch Loss: 259.84393310546875\n",
      "Batch 30/56\n",
      "Batch Loss: 228.72914123535156\n",
      "Batch 31/56\n",
      "Batch Loss: 160.32305908203125\n",
      "Batch 32/56\n",
      "Batch Loss: 118.01017761230469\n",
      "Batch 33/56\n",
      "Batch Loss: 36.624114990234375\n",
      "Batch 34/56\n",
      "Batch Loss: 24.38636016845703\n",
      "Batch 35/56\n",
      "Batch Loss: 21.671480178833008\n",
      "Batch 36/56\n",
      "Batch Loss: 21.360998153686523\n",
      "Batch 37/56\n",
      "Batch Loss: 57.86079406738281\n",
      "Batch 38/56\n",
      "Batch Loss: 162.9677734375\n",
      "Batch 39/56\n",
      "Batch Loss: 304.40057373046875\n",
      "Batch 40/56\n",
      "Batch Loss: 391.05499267578125\n",
      "Batch 41/56\n",
      "Batch Loss: 267.2886962890625\n",
      "Batch 42/56\n",
      "Batch Loss: 185.6644744873047\n",
      "Batch 43/56\n",
      "Batch Loss: 129.460205078125\n",
      "Batch 44/56\n",
      "Batch Loss: 66.45714569091797\n",
      "Batch 45/56\n",
      "Batch Loss: 40.29911804199219\n",
      "Batch 46/56\n",
      "Batch Loss: 32.06879425048828\n",
      "Batch 47/56\n",
      "Batch Loss: 70.58123779296875\n",
      "Batch 48/56\n",
      "Batch Loss: 60.136512756347656\n",
      "Batch 49/56\n",
      "Batch Loss: 146.6505126953125\n",
      "Batch 50/56\n",
      "Batch Loss: 194.08306884765625\n",
      "Batch 51/56\n",
      "Batch Loss: 258.7450256347656\n",
      "Batch 52/56\n",
      "Batch Loss: 371.6186828613281\n",
      "Batch 53/56\n",
      "Batch Loss: 254.64744567871094\n",
      "Batch 54/56\n",
      "Batch Loss: 179.9406280517578\n",
      "Batch 55/56\n",
      "Batch Loss: 67.56256103515625\n",
      "Batch 56/56\n",
      "Batch Loss: 29.486499786376953\n",
      "Validation Loss: 138.41239012990678\n",
      "----------Epoch 3/10----------\n",
      "----------Training----------\n",
      "Batch 1/56\n",
      "Batch Loss: 153.7025146484375\n",
      "Batch 2/56\n",
      "Batch Loss: 151.4853057861328\n",
      "Batch 3/56\n",
      "Batch Loss: 168.70848083496094\n",
      "Batch 4/56\n",
      "Batch Loss: 177.19898986816406\n",
      "Batch 5/56\n",
      "Batch Loss: 120.1921615600586\n",
      "Batch 6/56\n",
      "Batch Loss: 140.5258026123047\n",
      "Batch 7/56\n",
      "Batch Loss: 111.85079193115234\n",
      "Batch 8/56\n",
      "Batch Loss: 123.06214141845703\n",
      "Batch 9/56\n",
      "Batch Loss: 119.0423583984375\n",
      "Batch 10/56\n",
      "Batch Loss: 129.3497314453125\n",
      "Batch 11/56\n",
      "Batch Loss: 137.67987060546875\n",
      "Batch 12/56\n",
      "Batch Loss: 136.92648315429688\n",
      "Batch 13/56\n",
      "Batch Loss: 111.9334487915039\n",
      "Batch 14/56\n",
      "Batch Loss: 100.68025207519531\n",
      "Batch 15/56\n",
      "Batch Loss: 137.128173828125\n",
      "Batch 16/56\n",
      "Batch Loss: 113.50177001953125\n",
      "Batch 17/56\n",
      "Batch Loss: 126.7059326171875\n",
      "Batch 18/56\n",
      "Batch Loss: 145.14556884765625\n",
      "Batch 19/56\n",
      "Batch Loss: 122.6381607055664\n",
      "Batch 20/56\n",
      "Batch Loss: 116.91387939453125\n",
      "Batch 21/56\n",
      "Batch Loss: 150.5374298095703\n",
      "Batch 22/56\n",
      "Batch Loss: 109.80586242675781\n",
      "Batch 23/56\n",
      "Batch Loss: 152.3253173828125\n",
      "Batch 24/56\n",
      "Batch Loss: 151.47885131835938\n",
      "Batch 25/56\n",
      "Batch Loss: 145.53390502929688\n",
      "Batch 26/56\n",
      "Batch Loss: 128.45816040039062\n",
      "Batch 27/56\n",
      "Batch Loss: 138.02183532714844\n",
      "Batch 28/56\n",
      "Batch Loss: 103.79911041259766\n",
      "Batch 29/56\n",
      "Batch Loss: 125.68470001220703\n",
      "Batch 30/56\n",
      "Batch Loss: 141.27999877929688\n",
      "Batch 31/56\n",
      "Batch Loss: 103.6801528930664\n",
      "Batch 32/56\n",
      "Batch Loss: 93.76246643066406\n",
      "Batch 33/56\n",
      "Batch Loss: 154.3735809326172\n",
      "Batch 34/56\n",
      "Batch Loss: 139.94772338867188\n",
      "Batch 35/56\n",
      "Batch Loss: 129.19744873046875\n",
      "Batch 36/56\n",
      "Batch Loss: 109.87870788574219\n",
      "Batch 37/56\n",
      "Batch Loss: 118.97593688964844\n",
      "Batch 38/56\n",
      "Batch Loss: 109.61779022216797\n",
      "Batch 39/56\n",
      "Batch Loss: 107.50674438476562\n",
      "Batch 40/56\n",
      "Batch Loss: 88.00633239746094\n",
      "Batch 41/56\n",
      "Batch Loss: 142.34432983398438\n",
      "Batch 42/56\n",
      "Batch Loss: 105.84599304199219\n",
      "Batch 43/56\n",
      "Batch Loss: 120.36075592041016\n",
      "Batch 44/56\n",
      "Batch Loss: 126.6297836303711\n",
      "Batch 45/56\n",
      "Batch Loss: 132.04150390625\n",
      "Batch 46/56\n",
      "Batch Loss: 145.25726318359375\n",
      "Batch 47/56\n",
      "Batch Loss: 114.30807495117188\n",
      "Batch 48/56\n",
      "Batch Loss: 126.40728759765625\n",
      "Batch 49/56\n",
      "Batch Loss: 117.61235046386719\n",
      "Batch 50/56\n",
      "Batch Loss: 130.5118408203125\n",
      "Batch 51/56\n",
      "Batch Loss: 126.0625\n",
      "Batch 52/56\n",
      "Batch Loss: 118.82577514648438\n",
      "Batch 53/56\n",
      "Batch Loss: 135.50767517089844\n",
      "Batch 54/56\n",
      "Batch Loss: 91.22296142578125\n",
      "Batch 55/56\n",
      "Batch Loss: 136.13575744628906\n",
      "Batch 56/56\n",
      "Batch Loss: 111.0208740234375\n",
      "Training Loss: 127.25604643140521\n",
      "----------Validation----------\n",
      "Batch 1/56\n",
      "Batch Loss: 14.469084739685059\n",
      "Batch 2/56\n",
      "Batch Loss: 31.915420532226562\n",
      "Batch 3/56\n",
      "Batch Loss: 91.34083557128906\n",
      "Batch 4/56\n",
      "Batch Loss: 80.60498046875\n",
      "Batch 5/56\n",
      "Batch Loss: 158.5521697998047\n",
      "Batch 6/56\n",
      "Batch Loss: 202.26986694335938\n",
      "Batch 7/56\n",
      "Batch Loss: 214.64198303222656\n",
      "Batch 8/56\n",
      "Batch Loss: 166.3134307861328\n",
      "Batch 9/56\n",
      "Batch Loss: 99.99933624267578\n",
      "Batch 10/56\n",
      "Batch Loss: 54.08121871948242\n",
      "Batch 11/56\n",
      "Batch Loss: 41.92230987548828\n",
      "Batch 12/56\n",
      "Batch Loss: 21.33485221862793\n",
      "Batch 13/56\n",
      "Batch Loss: 12.510566711425781\n",
      "Batch 14/56\n",
      "Batch Loss: 46.59667205810547\n",
      "Batch 15/56\n",
      "Batch Loss: 109.05514526367188\n",
      "Batch 16/56\n",
      "Batch Loss: 220.2666015625\n",
      "Batch 17/56\n",
      "Batch Loss: 176.3061065673828\n",
      "Batch 18/56\n",
      "Batch Loss: 249.31625366210938\n",
      "Batch 19/56\n",
      "Batch Loss: 219.82196044921875\n",
      "Batch 20/56\n",
      "Batch Loss: 117.28173065185547\n",
      "Batch 21/56\n",
      "Batch Loss: 45.272708892822266\n",
      "Batch 22/56\n",
      "Batch Loss: 31.608362197875977\n",
      "Batch 23/56\n",
      "Batch Loss: 27.246681213378906\n",
      "Batch 24/56\n",
      "Batch Loss: 22.18399429321289\n",
      "Batch 25/56\n",
      "Batch Loss: 52.681358337402344\n",
      "Batch 26/56\n",
      "Batch Loss: 73.17843627929688\n",
      "Batch 27/56\n",
      "Batch Loss: 193.7071990966797\n",
      "Batch 28/56\n",
      "Batch Loss: 183.203857421875\n",
      "Batch 29/56\n",
      "Batch Loss: 222.88180541992188\n",
      "Batch 30/56\n",
      "Batch Loss: 193.94293212890625\n",
      "Batch 31/56\n",
      "Batch Loss: 131.5131072998047\n",
      "Batch 32/56\n",
      "Batch Loss: 93.7160873413086\n",
      "Batch 33/56\n",
      "Batch Loss: 26.000207901000977\n",
      "Batch 34/56\n",
      "Batch Loss: 16.69076156616211\n",
      "Batch 35/56\n",
      "Batch Loss: 13.81562328338623\n",
      "Batch 36/56\n",
      "Batch Loss: 14.40199089050293\n",
      "Batch 37/56\n",
      "Batch Loss: 42.907630920410156\n",
      "Batch 38/56\n",
      "Batch Loss: 134.57388305664062\n",
      "Batch 39/56\n",
      "Batch Loss: 264.5018310546875\n",
      "Batch 40/56\n",
      "Batch Loss: 345.46966552734375\n",
      "Batch 41/56\n",
      "Batch Loss: 229.8006591796875\n",
      "Batch 42/56\n",
      "Batch Loss: 154.62518310546875\n",
      "Batch 43/56\n",
      "Batch Loss: 104.10297393798828\n",
      "Batch 44/56\n",
      "Batch Loss: 49.20542907714844\n",
      "Batch 45/56\n",
      "Batch Loss: 28.199779510498047\n",
      "Batch 46/56\n",
      "Batch Loss: 22.74276351928711\n",
      "Batch 47/56\n",
      "Batch Loss: 53.13765335083008\n",
      "Batch 48/56\n",
      "Batch Loss: 43.665771484375\n",
      "Batch 49/56\n",
      "Batch Loss: 120.46563720703125\n",
      "Batch 50/56\n",
      "Batch Loss: 162.81655883789062\n",
      "Batch 51/56\n",
      "Batch Loss: 222.04397583007812\n",
      "Batch 52/56\n",
      "Batch Loss: 327.08746337890625\n",
      "Batch 53/56\n",
      "Batch Loss: 218.26991271972656\n",
      "Batch 54/56\n",
      "Batch Loss: 149.8884735107422\n",
      "Batch 55/56\n",
      "Batch Loss: 50.207008361816406\n",
      "Batch 56/56\n",
      "Batch Loss: 18.959754943847656\n",
      "Validation Loss: 114.52352942739215\n",
      "----------Epoch 4/10----------\n",
      "----------Training----------\n",
      "Batch 1/56\n",
      "Batch Loss: 148.2779083251953\n",
      "Batch 2/56\n",
      "Batch Loss: 110.77157592773438\n",
      "Batch 3/56\n",
      "Batch Loss: 113.66049194335938\n",
      "Batch 4/56\n",
      "Batch Loss: 123.62741088867188\n",
      "Batch 5/56\n",
      "Batch Loss: 93.70470428466797\n",
      "Batch 6/56\n",
      "Batch Loss: 72.12196350097656\n",
      "Batch 7/56\n",
      "Batch Loss: 109.79576873779297\n",
      "Batch 8/56\n",
      "Batch Loss: 106.86466979980469\n",
      "Batch 9/56\n",
      "Batch Loss: 83.24158477783203\n",
      "Batch 10/56\n",
      "Batch Loss: 127.7066650390625\n",
      "Batch 11/56\n",
      "Batch Loss: 132.18603515625\n",
      "Batch 12/56\n",
      "Batch Loss: 107.31556701660156\n",
      "Batch 13/56\n",
      "Batch Loss: 104.08179473876953\n",
      "Batch 14/56\n",
      "Batch Loss: 120.67530822753906\n",
      "Batch 15/56\n",
      "Batch Loss: 89.13990783691406\n",
      "Batch 16/56\n",
      "Batch Loss: 112.52005004882812\n",
      "Batch 17/56\n",
      "Batch Loss: 106.20591735839844\n",
      "Batch 18/56\n",
      "Batch Loss: 137.2519989013672\n",
      "Batch 19/56\n",
      "Batch Loss: 52.071346282958984\n",
      "Batch 20/56\n",
      "Batch Loss: 87.86432647705078\n",
      "Batch 21/56\n",
      "Batch Loss: 133.91131591796875\n",
      "Batch 22/56\n",
      "Batch Loss: 86.13121032714844\n",
      "Batch 23/56\n",
      "Batch Loss: 101.06368255615234\n",
      "Batch 24/56\n",
      "Batch Loss: 96.17882537841797\n",
      "Batch 25/56\n",
      "Batch Loss: 106.42720031738281\n",
      "Batch 26/56\n",
      "Batch Loss: 132.82960510253906\n",
      "Batch 27/56\n",
      "Batch Loss: 91.78014373779297\n",
      "Batch 28/56\n",
      "Batch Loss: 145.56146240234375\n",
      "Batch 29/56\n",
      "Batch Loss: 101.19007873535156\n",
      "Batch 30/56\n",
      "Batch Loss: 98.74730682373047\n",
      "Batch 31/56\n",
      "Batch Loss: 101.6863021850586\n",
      "Batch 32/56\n",
      "Batch Loss: 81.13005065917969\n",
      "Batch 33/56\n",
      "Batch Loss: 103.75759887695312\n",
      "Batch 34/56\n",
      "Batch Loss: 106.07935333251953\n",
      "Batch 35/56\n",
      "Batch Loss: 88.34847259521484\n",
      "Batch 36/56\n",
      "Batch Loss: 101.22166442871094\n",
      "Batch 37/56\n",
      "Batch Loss: 112.38977813720703\n",
      "Batch 38/56\n",
      "Batch Loss: 126.41267395019531\n",
      "Batch 39/56\n",
      "Batch Loss: 99.04167938232422\n",
      "Batch 40/56\n",
      "Batch Loss: 138.02784729003906\n",
      "Batch 41/56\n",
      "Batch Loss: 134.13995361328125\n",
      "Batch 42/56\n",
      "Batch Loss: 112.89070129394531\n",
      "Batch 43/56\n",
      "Batch Loss: 121.44810485839844\n",
      "Batch 44/56\n",
      "Batch Loss: 83.23594665527344\n",
      "Batch 45/56\n",
      "Batch Loss: 93.5718765258789\n",
      "Batch 46/56\n",
      "Batch Loss: 100.3046875\n",
      "Batch 47/56\n",
      "Batch Loss: 133.42991638183594\n",
      "Batch 48/56\n",
      "Batch Loss: 57.654720306396484\n",
      "Batch 49/56\n",
      "Batch Loss: 80.9796142578125\n",
      "Batch 50/56\n",
      "Batch Loss: 80.10890197753906\n",
      "Batch 51/56\n",
      "Batch Loss: 135.8603973388672\n",
      "Batch 52/56\n",
      "Batch Loss: 108.96353149414062\n",
      "Batch 53/56\n",
      "Batch Loss: 99.00448608398438\n",
      "Batch 54/56\n",
      "Batch Loss: 97.17988586425781\n",
      "Batch 55/56\n",
      "Batch Loss: 81.70247650146484\n",
      "Batch 56/56\n",
      "Batch Loss: 70.28644561767578\n",
      "Training Loss: 105.03148024422782\n",
      "----------Validation----------\n",
      "Batch 1/56\n",
      "Batch Loss: 8.948959350585938\n",
      "Batch 2/56\n",
      "Batch Loss: 21.319908142089844\n",
      "Batch 3/56\n",
      "Batch Loss: 72.23905944824219\n",
      "Batch 4/56\n",
      "Batch Loss: 62.13673782348633\n",
      "Batch 5/56\n",
      "Batch Loss: 132.0380859375\n",
      "Batch 6/56\n",
      "Batch Loss: 171.92762756347656\n",
      "Batch 7/56\n",
      "Batch Loss: 182.98939514160156\n",
      "Batch 8/56\n",
      "Batch Loss: 138.6287841796875\n",
      "Batch 9/56\n",
      "Batch Loss: 78.83732604980469\n",
      "Batch 10/56\n",
      "Batch Loss: 40.17329406738281\n",
      "Batch 11/56\n",
      "Batch Loss: 30.267595291137695\n",
      "Batch 12/56\n",
      "Batch Loss: 15.175516128540039\n",
      "Batch 13/56\n",
      "Batch Loss: 6.527171611785889\n",
      "Batch 14/56\n",
      "Batch Loss: 32.78523635864258\n",
      "Batch 15/56\n",
      "Batch Loss: 88.5508041381836\n",
      "Batch 16/56\n",
      "Batch Loss: 188.91836547851562\n",
      "Batch 17/56\n",
      "Batch Loss: 147.70057678222656\n",
      "Batch 18/56\n",
      "Batch Loss: 215.4012451171875\n",
      "Batch 19/56\n",
      "Batch Loss: 187.66355895996094\n",
      "Batch 20/56\n",
      "Batch Loss: 94.27515411376953\n",
      "Batch 21/56\n",
      "Batch Loss: 33.42644500732422\n",
      "Batch 22/56\n",
      "Batch Loss: 22.13661766052246\n",
      "Batch 23/56\n",
      "Batch Loss: 18.317663192749023\n",
      "Batch 24/56\n",
      "Batch Loss: 14.568872451782227\n",
      "Batch 25/56\n",
      "Batch Loss: 38.80394744873047\n",
      "Batch 26/56\n",
      "Batch Loss: 55.47285461425781\n",
      "Batch 27/56\n",
      "Batch Loss: 164.12123107910156\n",
      "Batch 28/56\n",
      "Batch Loss: 154.36846923828125\n",
      "Batch 29/56\n",
      "Batch Loss: 190.68435668945312\n",
      "Batch 30/56\n",
      "Batch Loss: 163.7971954345703\n",
      "Batch 31/56\n",
      "Batch Loss: 107.00249481201172\n",
      "Batch 32/56\n",
      "Batch Loss: 73.46358489990234\n",
      "Batch 33/56\n",
      "Batch Loss: 18.63764762878418\n",
      "Batch 34/56\n",
      "Batch Loss: 12.08936882019043\n",
      "Batch 35/56\n",
      "Batch Loss: 9.063119888305664\n",
      "Batch 36/56\n",
      "Batch Loss: 10.495146751403809\n",
      "Batch 37/56\n",
      "Batch Loss: 31.462913513183594\n",
      "Batch 38/56\n",
      "Batch Loss: 110.45560455322266\n",
      "Batch 39/56\n",
      "Batch Loss: 229.5354461669922\n",
      "Batch 40/56\n",
      "Batch Loss: 305.1412658691406\n",
      "Batch 41/56\n",
      "Batch Loss: 197.1072998046875\n",
      "Batch 42/56\n",
      "Batch Loss: 128.0124969482422\n",
      "Batch 43/56\n",
      "Batch Loss: 82.84803009033203\n",
      "Batch 44/56\n",
      "Batch Loss: 35.593353271484375\n",
      "Batch 45/56\n",
      "Batch Loss: 19.445995330810547\n",
      "Batch 46/56\n",
      "Batch Loss: 16.604000091552734\n",
      "Batch 47/56\n",
      "Batch Loss: 39.34466552734375\n",
      "Batch 48/56\n",
      "Batch Loss: 30.790103912353516\n",
      "Batch 49/56\n",
      "Batch Loss: 98.43031311035156\n",
      "Batch 50/56\n",
      "Batch Loss: 135.98960876464844\n",
      "Batch 51/56\n",
      "Batch Loss: 190.09271240234375\n",
      "Batch 52/56\n",
      "Batch Loss: 287.75299072265625\n",
      "Batch 53/56\n",
      "Batch Loss: 186.62371826171875\n",
      "Batch 54/56\n",
      "Batch Loss: 124.20657348632812\n",
      "Batch 55/56\n",
      "Batch Loss: 36.49702453613281\n",
      "Batch 56/56\n",
      "Batch Loss: 11.688807487487793\n",
      "Validation Loss: 94.65314894914627\n",
      "----------Epoch 5/10----------\n",
      "----------Training----------\n",
      "Batch 1/56\n",
      "Batch Loss: 97.69331359863281\n",
      "Batch 2/56\n",
      "Batch Loss: 99.72246551513672\n",
      "Batch 3/56\n",
      "Batch Loss: 76.87775421142578\n",
      "Batch 4/56\n",
      "Batch Loss: 95.23933410644531\n",
      "Batch 5/56\n",
      "Batch Loss: 92.50265502929688\n",
      "Batch 6/56\n",
      "Batch Loss: 92.38912963867188\n",
      "Batch 7/56\n",
      "Batch Loss: 112.28480529785156\n",
      "Batch 8/56\n",
      "Batch Loss: 97.11810302734375\n",
      "Batch 9/56\n",
      "Batch Loss: 84.52034759521484\n",
      "Batch 10/56\n",
      "Batch Loss: 103.81260681152344\n",
      "Batch 11/56\n",
      "Batch Loss: 103.72429656982422\n",
      "Batch 12/56\n",
      "Batch Loss: 89.82970428466797\n",
      "Batch 13/56\n",
      "Batch Loss: 97.6965103149414\n",
      "Batch 14/56\n",
      "Batch Loss: 103.91803741455078\n",
      "Batch 15/56\n",
      "Batch Loss: 103.42507934570312\n",
      "Batch 16/56\n",
      "Batch Loss: 70.70399475097656\n",
      "Batch 17/56\n",
      "Batch Loss: 108.1821517944336\n",
      "Batch 18/56\n",
      "Batch Loss: 87.35665130615234\n",
      "Batch 19/56\n",
      "Batch Loss: 68.51181030273438\n",
      "Batch 20/56\n",
      "Batch Loss: 66.58419036865234\n",
      "Batch 21/56\n",
      "Batch Loss: 107.36756134033203\n",
      "Batch 22/56\n",
      "Batch Loss: 72.04667663574219\n",
      "Batch 23/56\n",
      "Batch Loss: 76.10443878173828\n",
      "Batch 24/56\n",
      "Batch Loss: 98.24048614501953\n",
      "Batch 25/56\n",
      "Batch Loss: 62.876976013183594\n",
      "Batch 26/56\n",
      "Batch Loss: 98.73265838623047\n",
      "Batch 27/56\n",
      "Batch Loss: 72.10411834716797\n",
      "Batch 28/56\n",
      "Batch Loss: 104.61480712890625\n",
      "Batch 29/56\n",
      "Batch Loss: 74.6302719116211\n",
      "Batch 30/56\n",
      "Batch Loss: 53.511863708496094\n",
      "Batch 31/56\n",
      "Batch Loss: 84.43289947509766\n",
      "Batch 32/56\n",
      "Batch Loss: 64.58091735839844\n",
      "Batch 33/56\n",
      "Batch Loss: 93.20442962646484\n",
      "Batch 34/56\n",
      "Batch Loss: 99.33451843261719\n",
      "Batch 35/56\n",
      "Batch Loss: 103.21256256103516\n",
      "Batch 36/56\n",
      "Batch Loss: 94.46287536621094\n",
      "Batch 37/56\n",
      "Batch Loss: 89.56971740722656\n",
      "Batch 38/56\n",
      "Batch Loss: 71.96066284179688\n",
      "Batch 39/56\n",
      "Batch Loss: 106.19807434082031\n",
      "Batch 40/56\n",
      "Batch Loss: 90.21295928955078\n",
      "Batch 41/56\n",
      "Batch Loss: 73.21041107177734\n",
      "Batch 42/56\n",
      "Batch Loss: 56.243473052978516\n",
      "Batch 43/56\n",
      "Batch Loss: 66.77642822265625\n",
      "Batch 44/56\n",
      "Batch Loss: 106.96775817871094\n",
      "Batch 45/56\n",
      "Batch Loss: 84.41696166992188\n",
      "Batch 46/56\n",
      "Batch Loss: 120.46461486816406\n",
      "Batch 47/56\n",
      "Batch Loss: 103.32868194580078\n",
      "Batch 48/56\n",
      "Batch Loss: 72.245849609375\n",
      "Batch 49/56\n",
      "Batch Loss: 81.509033203125\n",
      "Batch 50/56\n",
      "Batch Loss: 83.89332580566406\n",
      "Batch 51/56\n",
      "Batch Loss: 73.2261734008789\n",
      "Batch 52/56\n",
      "Batch Loss: 99.40171813964844\n",
      "Batch 53/56\n",
      "Batch Loss: 91.08744812011719\n",
      "Batch 54/56\n",
      "Batch Loss: 78.95870208740234\n",
      "Batch 55/56\n",
      "Batch Loss: 63.22209930419922\n",
      "Batch 56/56\n",
      "Batch Loss: 40.28913497924805\n",
      "Training Loss: 86.870254107884\n",
      "----------Validation----------\n",
      "Batch 1/56\n",
      "Batch Loss: 6.1442790031433105\n",
      "Batch 2/56\n",
      "Batch Loss: 13.840015411376953\n",
      "Batch 3/56\n",
      "Batch Loss: 56.92357635498047\n",
      "Batch 4/56\n",
      "Batch Loss: 47.40483474731445\n",
      "Batch 5/56\n",
      "Batch Loss: 109.89473724365234\n",
      "Batch 6/56\n",
      "Batch Loss: 146.2579345703125\n",
      "Batch 7/56\n",
      "Batch Loss: 156.11265563964844\n",
      "Batch 8/56\n",
      "Batch Loss: 115.40713500976562\n",
      "Batch 9/56\n",
      "Batch Loss: 61.624053955078125\n",
      "Batch 10/56\n",
      "Batch Loss: 29.642168045043945\n",
      "Batch 11/56\n",
      "Batch Loss: 21.81201171875\n",
      "Batch 12/56\n",
      "Batch Loss: 11.782023429870605\n",
      "Batch 13/56\n",
      "Batch Loss: 3.2957494258880615\n",
      "Batch 14/56\n",
      "Batch Loss: 22.342979431152344\n",
      "Batch 15/56\n",
      "Batch Loss: 71.94332885742188\n",
      "Batch 16/56\n",
      "Batch Loss: 162.32199096679688\n",
      "Batch 17/56\n",
      "Batch Loss: 123.63066864013672\n",
      "Batch 18/56\n",
      "Batch Loss: 186.44046020507812\n",
      "Batch 19/56\n",
      "Batch Loss: 160.3208770751953\n",
      "Batch 20/56\n",
      "Batch Loss: 75.36276245117188\n",
      "Batch 21/56\n",
      "Batch Loss: 24.794414520263672\n",
      "Batch 22/56\n",
      "Batch Loss: 15.691883087158203\n",
      "Batch 23/56\n",
      "Batch Loss: 12.372865676879883\n",
      "Batch 24/56\n",
      "Batch Loss: 9.834375381469727\n",
      "Batch 25/56\n",
      "Batch Loss: 28.300918579101562\n",
      "Batch 26/56\n",
      "Batch Loss: 41.4434814453125\n",
      "Batch 27/56\n",
      "Batch Loss: 139.14816284179688\n",
      "Batch 28/56\n",
      "Batch Loss: 130.08682250976562\n",
      "Batch 29/56\n",
      "Batch Loss: 163.30575561523438\n",
      "Batch 30/56\n",
      "Batch Loss: 138.30853271484375\n",
      "Batch 31/56\n",
      "Batch Loss: 86.70466613769531\n",
      "Batch 32/56\n",
      "Batch Loss: 57.088111877441406\n",
      "Batch 33/56\n",
      "Batch Loss: 14.135799407958984\n",
      "Batch 34/56\n",
      "Batch Loss: 10.130985260009766\n",
      "Batch 35/56\n",
      "Batch Loss: 6.965538024902344\n",
      "Batch 36/56\n",
      "Batch Loss: 9.176549911499023\n",
      "Batch 37/56\n",
      "Batch Loss: 23.200769424438477\n",
      "Batch 38/56\n",
      "Batch Loss: 90.5191650390625\n",
      "Batch 39/56\n",
      "Batch Loss: 199.60617065429688\n",
      "Batch 40/56\n",
      "Batch Loss: 270.2727355957031\n",
      "Batch 41/56\n",
      "Batch Loss: 169.2718963623047\n",
      "Batch 42/56\n",
      "Batch Loss: 105.77831268310547\n",
      "Batch 43/56\n",
      "Batch Loss: 65.54914855957031\n",
      "Batch 44/56\n",
      "Batch Loss: 25.334732055664062\n",
      "Batch 45/56\n",
      "Batch Loss: 13.662614822387695\n",
      "Batch 46/56\n",
      "Batch Loss: 13.22945785522461\n",
      "Batch 47/56\n",
      "Batch Loss: 28.919403076171875\n",
      "Batch 48/56\n",
      "Batch Loss: 21.209827423095703\n",
      "Batch 49/56\n",
      "Batch Loss: 80.41255950927734\n",
      "Batch 50/56\n",
      "Batch Loss: 113.55805969238281\n",
      "Batch 51/56\n",
      "Batch Loss: 162.9408721923828\n",
      "Batch 52/56\n",
      "Batch Loss: 253.80007934570312\n",
      "Batch 53/56\n",
      "Batch Loss: 159.75286865234375\n",
      "Batch 54/56\n",
      "Batch Loss: 102.82978820800781\n",
      "Batch 55/56\n",
      "Batch Loss: 26.148221969604492\n",
      "Batch 56/56\n",
      "Batch Loss: 7.2713494300842285\n",
      "Validation Loss: 78.62966317364148\n",
      "----------Epoch 6/10----------\n",
      "----------Training----------\n",
      "Batch 1/56\n",
      "Batch Loss: 67.62649536132812\n",
      "Batch 2/56\n",
      "Batch Loss: 86.23458862304688\n",
      "Batch 3/56\n",
      "Batch Loss: 89.5362319946289\n",
      "Batch 4/56\n",
      "Batch Loss: 91.0951919555664\n",
      "Batch 5/56\n",
      "Batch Loss: 95.88938903808594\n",
      "Batch 6/56\n",
      "Batch Loss: 85.45805358886719\n",
      "Batch 7/56\n",
      "Batch Loss: 73.41621398925781\n",
      "Batch 8/56\n",
      "Batch Loss: 69.32438659667969\n",
      "Batch 9/56\n",
      "Batch Loss: 72.61038208007812\n",
      "Batch 10/56\n",
      "Batch Loss: 80.88672637939453\n",
      "Batch 11/56\n",
      "Batch Loss: 74.96588134765625\n",
      "Batch 12/56\n",
      "Batch Loss: 86.63804626464844\n",
      "Batch 13/56\n",
      "Batch Loss: 91.25609588623047\n",
      "Batch 14/56\n",
      "Batch Loss: 83.81538391113281\n",
      "Batch 15/56\n",
      "Batch Loss: 102.18604278564453\n",
      "Batch 16/56\n",
      "Batch Loss: 57.633338928222656\n",
      "Batch 17/56\n",
      "Batch Loss: 61.06061935424805\n",
      "Batch 18/56\n",
      "Batch Loss: 86.35810089111328\n",
      "Batch 19/56\n",
      "Batch Loss: 66.47806549072266\n",
      "Batch 20/56\n",
      "Batch Loss: 79.263671875\n",
      "Batch 21/56\n",
      "Batch Loss: 70.21363830566406\n",
      "Batch 22/56\n",
      "Batch Loss: 58.67579650878906\n",
      "Batch 23/56\n",
      "Batch Loss: 73.90505981445312\n",
      "Batch 24/56\n",
      "Batch Loss: 66.37015533447266\n",
      "Batch 25/56\n",
      "Batch Loss: 51.86834716796875\n",
      "Batch 26/56\n",
      "Batch Loss: 80.78080749511719\n",
      "Batch 27/56\n",
      "Batch Loss: 66.42176818847656\n",
      "Batch 28/56\n",
      "Batch Loss: 86.03755187988281\n",
      "Batch 29/56\n",
      "Batch Loss: 68.00820922851562\n",
      "Batch 30/56\n",
      "Batch Loss: 75.70405578613281\n",
      "Batch 31/56\n",
      "Batch Loss: 73.20892333984375\n",
      "Batch 32/56\n",
      "Batch Loss: 79.98474884033203\n",
      "Batch 33/56\n",
      "Batch Loss: 61.68461990356445\n",
      "Batch 34/56\n",
      "Batch Loss: 86.89309692382812\n",
      "Batch 35/56\n",
      "Batch Loss: 74.1957778930664\n",
      "Batch 36/56\n",
      "Batch Loss: 87.71502685546875\n",
      "Batch 37/56\n",
      "Batch Loss: 73.80125427246094\n",
      "Batch 38/56\n",
      "Batch Loss: 47.34239959716797\n",
      "Batch 39/56\n",
      "Batch Loss: 68.19353485107422\n",
      "Batch 40/56\n",
      "Batch Loss: 73.79704284667969\n",
      "Batch 41/56\n",
      "Batch Loss: 74.32583618164062\n",
      "Batch 42/56\n",
      "Batch Loss: 60.099456787109375\n",
      "Batch 43/56\n",
      "Batch Loss: 52.983001708984375\n",
      "Batch 44/56\n",
      "Batch Loss: 76.2197494506836\n",
      "Batch 45/56\n",
      "Batch Loss: 52.3799934387207\n",
      "Batch 46/56\n",
      "Batch Loss: 64.86038208007812\n",
      "Batch 47/56\n",
      "Batch Loss: 78.73004913330078\n",
      "Batch 48/56\n",
      "Batch Loss: 86.47584533691406\n",
      "Batch 49/56\n",
      "Batch Loss: 50.38653564453125\n",
      "Batch 50/56\n",
      "Batch Loss: 76.58088684082031\n",
      "Batch 51/56\n",
      "Batch Loss: 75.019775390625\n",
      "Batch 52/56\n",
      "Batch Loss: 51.992393493652344\n",
      "Batch 53/56\n",
      "Batch Loss: 67.36929321289062\n",
      "Batch 54/56\n",
      "Batch Loss: 63.30089569091797\n",
      "Batch 55/56\n",
      "Batch Loss: 59.56056213378906\n",
      "Batch 56/56\n",
      "Batch Loss: 45.90596008300781\n",
      "Training Loss: 72.54866674968174\n",
      "----------Validation----------\n",
      "Batch 1/56\n",
      "Batch Loss: 5.496440887451172\n",
      "Batch 2/56\n",
      "Batch Loss: 8.871992111206055\n",
      "Batch 3/56\n",
      "Batch Loss: 44.71498489379883\n",
      "Batch 4/56\n",
      "Batch Loss: 35.73551559448242\n",
      "Batch 5/56\n",
      "Batch Loss: 91.37675476074219\n",
      "Batch 6/56\n",
      "Batch Loss: 124.48139190673828\n",
      "Batch 7/56\n",
      "Batch Loss: 133.2207489013672\n",
      "Batch 8/56\n",
      "Batch Loss: 95.89273834228516\n",
      "Batch 9/56\n",
      "Batch Loss: 47.661781311035156\n",
      "Batch 10/56\n",
      "Batch Loss: 21.85460662841797\n",
      "Batch 11/56\n",
      "Batch Loss: 15.942390441894531\n",
      "Batch 12/56\n",
      "Batch Loss: 10.590084075927734\n",
      "Batch 13/56\n",
      "Batch Loss: 2.253575325012207\n",
      "Batch 14/56\n",
      "Batch Loss: 14.637552261352539\n",
      "Batch 15/56\n",
      "Batch Loss: 58.54085922241211\n",
      "Batch 16/56\n",
      "Batch Loss: 139.6891326904297\n",
      "Batch 17/56\n",
      "Batch Loss: 103.33244323730469\n",
      "Batch 18/56\n",
      "Batch Loss: 161.6227569580078\n",
      "Batch 19/56\n",
      "Batch Loss: 136.99838256835938\n",
      "Batch 20/56\n",
      "Batch Loss: 59.83039093017578\n",
      "Batch 21/56\n",
      "Batch Loss: 18.761741638183594\n",
      "Batch 22/56\n",
      "Batch Loss: 11.680412292480469\n",
      "Batch 23/56\n",
      "Batch Loss: 8.823365211486816\n",
      "Batch 24/56\n",
      "Batch Loss: 7.4032697677612305\n",
      "Batch 25/56\n",
      "Batch Loss: 20.53932762145996\n",
      "Batch 26/56\n",
      "Batch Loss: 30.423328399658203\n",
      "Batch 27/56\n",
      "Batch Loss: 118.01536560058594\n",
      "Batch 28/56\n",
      "Batch Loss: 109.59294891357422\n",
      "Batch 29/56\n",
      "Batch Loss: 139.95004272460938\n",
      "Batch 30/56\n",
      "Batch Loss: 116.69926452636719\n",
      "Batch 31/56\n",
      "Batch Loss: 69.89205932617188\n",
      "Batch 32/56\n",
      "Batch Loss: 43.900020599365234\n",
      "Batch 33/56\n",
      "Batch Loss: 11.919677734375\n",
      "Batch 34/56\n",
      "Batch Loss: 10.265176773071289\n",
      "Batch 35/56\n",
      "Batch Loss: 6.971101760864258\n",
      "Batch 36/56\n",
      "Batch Loss: 9.90194320678711\n",
      "Batch 37/56\n",
      "Batch Loss: 17.509897232055664\n",
      "Batch 38/56\n",
      "Batch Loss: 74.04049682617188\n",
      "Batch 39/56\n",
      "Batch Loss: 173.89349365234375\n",
      "Batch 40/56\n",
      "Batch Loss: 239.9959259033203\n",
      "Batch 41/56\n",
      "Batch Loss: 145.49407958984375\n",
      "Batch 42/56\n",
      "Batch Loss: 87.1763916015625\n",
      "Batch 43/56\n",
      "Batch Loss: 51.50776672363281\n",
      "Batch 44/56\n",
      "Batch Loss: 17.798995971679688\n",
      "Batch 45/56\n",
      "Batch Loss: 10.262273788452148\n",
      "Batch 46/56\n",
      "Batch Loss: 12.055032730102539\n",
      "Batch 47/56\n",
      "Batch Loss: 21.22967529296875\n",
      "Batch 48/56\n",
      "Batch Loss: 14.300922393798828\n",
      "Batch 49/56\n",
      "Batch Loss: 65.70690155029297\n",
      "Batch 50/56\n",
      "Batch Loss: 94.77375793457031\n",
      "Batch 51/56\n",
      "Batch Loss: 139.79470825195312\n",
      "Batch 52/56\n",
      "Batch Loss: 224.3692626953125\n",
      "Batch 53/56\n",
      "Batch Loss: 136.86636352539062\n",
      "Batch 54/56\n",
      "Batch Loss: 85.02017211914062\n",
      "Batch 55/56\n",
      "Batch Loss: 18.52914810180664\n",
      "Batch 56/56\n",
      "Batch Loss: 5.133206844329834\n",
      "Validation Loss: 65.76682217632022\n",
      "----------Epoch 7/10----------\n",
      "----------Training----------\n",
      "Batch 1/56\n",
      "Batch Loss: 72.60235595703125\n",
      "Batch 2/56\n",
      "Batch Loss: 66.1015625\n",
      "Batch 3/56\n",
      "Batch Loss: 105.64826965332031\n",
      "Batch 4/56\n",
      "Batch Loss: 54.968536376953125\n",
      "Batch 5/56\n",
      "Batch Loss: 83.84022521972656\n",
      "Batch 6/56\n",
      "Batch Loss: 62.115867614746094\n",
      "Batch 7/56\n",
      "Batch Loss: 65.3088607788086\n",
      "Batch 8/56\n",
      "Batch Loss: 47.70738220214844\n",
      "Batch 9/56\n",
      "Batch Loss: 72.31514739990234\n",
      "Batch 10/56\n",
      "Batch Loss: 59.740814208984375\n",
      "Batch 11/56\n",
      "Batch Loss: 71.99784851074219\n",
      "Batch 12/56\n",
      "Batch Loss: 82.2237777709961\n",
      "Batch 13/56\n",
      "Batch Loss: 53.23097229003906\n",
      "Batch 14/56\n",
      "Batch Loss: 61.127838134765625\n",
      "Batch 15/56\n",
      "Batch Loss: 58.61548614501953\n",
      "Batch 16/56\n",
      "Batch Loss: 75.71156311035156\n",
      "Batch 17/56\n",
      "Batch Loss: 52.434730529785156\n",
      "Batch 18/56\n",
      "Batch Loss: 70.59942626953125\n",
      "Batch 19/56\n",
      "Batch Loss: 71.29296875\n",
      "Batch 20/56\n",
      "Batch Loss: 73.51041412353516\n",
      "Batch 21/56\n",
      "Batch Loss: 75.56475067138672\n",
      "Batch 22/56\n",
      "Batch Loss: 67.91761016845703\n",
      "Batch 23/56\n",
      "Batch Loss: 82.19320678710938\n",
      "Batch 24/56\n",
      "Batch Loss: 51.38420104980469\n",
      "Batch 25/56\n",
      "Batch Loss: 53.862831115722656\n",
      "Batch 26/56\n",
      "Batch Loss: 70.74063110351562\n",
      "Batch 27/56\n",
      "Batch Loss: 63.538841247558594\n",
      "Batch 28/56\n",
      "Batch Loss: 70.47721099853516\n",
      "Batch 29/56\n",
      "Batch Loss: 55.0491943359375\n",
      "Batch 30/56\n",
      "Batch Loss: 63.976871490478516\n",
      "Batch 31/56\n",
      "Batch Loss: 41.50896072387695\n",
      "Batch 32/56\n",
      "Batch Loss: 59.53977966308594\n",
      "Batch 33/56\n",
      "Batch Loss: 50.85048294067383\n",
      "Batch 34/56\n",
      "Batch Loss: 61.645957946777344\n",
      "Batch 35/56\n",
      "Batch Loss: 40.21622848510742\n",
      "Batch 36/56\n",
      "Batch Loss: 63.84653091430664\n",
      "Batch 37/56\n",
      "Batch Loss: 65.52699279785156\n",
      "Batch 38/56\n",
      "Batch Loss: 56.666290283203125\n",
      "Batch 39/56\n",
      "Batch Loss: 68.84156036376953\n",
      "Batch 40/56\n",
      "Batch Loss: 53.67302703857422\n",
      "Batch 41/56\n",
      "Batch Loss: 42.82128143310547\n",
      "Batch 42/56\n",
      "Batch Loss: 54.340431213378906\n",
      "Batch 43/56\n",
      "Batch Loss: 45.63517761230469\n",
      "Batch 44/56\n",
      "Batch Loss: 55.064903259277344\n",
      "Batch 45/56\n",
      "Batch Loss: 54.710533142089844\n",
      "Batch 46/56\n",
      "Batch Loss: 56.33753967285156\n",
      "Batch 47/56\n",
      "Batch Loss: 59.480072021484375\n",
      "Batch 48/56\n",
      "Batch Loss: 62.334434509277344\n",
      "Batch 49/56\n",
      "Batch Loss: 40.717140197753906\n",
      "Batch 50/56\n",
      "Batch Loss: 61.438385009765625\n",
      "Batch 51/56\n",
      "Batch Loss: 45.20068359375\n",
      "Batch 52/56\n",
      "Batch Loss: 73.25360870361328\n",
      "Batch 53/56\n",
      "Batch Loss: 54.33866882324219\n",
      "Batch 54/56\n",
      "Batch Loss: 48.018802642822266\n",
      "Batch 55/56\n",
      "Batch Loss: 45.28558349609375\n",
      "Batch 56/56\n",
      "Batch Loss: 43.08780288696289\n",
      "Training Loss: 61.074647426605225\n",
      "----------Validation----------\n",
      "Batch 1/56\n",
      "Batch Loss: 6.529486656188965\n",
      "Batch 2/56\n",
      "Batch Loss: 5.973799705505371\n",
      "Batch 3/56\n",
      "Batch Loss: 35.22809600830078\n",
      "Batch 4/56\n",
      "Batch Loss: 26.739347457885742\n",
      "Batch 5/56\n",
      "Batch Loss: 76.14852905273438\n",
      "Batch 6/56\n",
      "Batch Loss: 106.2879867553711\n",
      "Batch 7/56\n",
      "Batch Loss: 114.01238250732422\n",
      "Batch 8/56\n",
      "Batch Loss: 79.7578125\n",
      "Batch 9/56\n",
      "Batch Loss: 36.579097747802734\n",
      "Batch 10/56\n",
      "Batch Loss: 16.390722274780273\n",
      "Batch 11/56\n",
      "Batch Loss: 12.223771095275879\n",
      "Batch 12/56\n",
      "Batch Loss: 11.128016471862793\n",
      "Batch 13/56\n",
      "Batch Loss: 2.9277870655059814\n",
      "Batch 14/56\n",
      "Batch Loss: 9.24840259552002\n",
      "Batch 15/56\n",
      "Batch Loss: 47.96757888793945\n",
      "Batch 16/56\n",
      "Batch Loss: 120.71651458740234\n",
      "Batch 17/56\n",
      "Batch Loss: 86.48423767089844\n",
      "Batch 18/56\n",
      "Batch Loss: 140.6619873046875\n",
      "Batch 19/56\n",
      "Batch Loss: 117.39822387695312\n",
      "Batch 20/56\n",
      "Batch Loss: 47.318965911865234\n",
      "Batch 21/56\n",
      "Batch Loss: 14.894753456115723\n",
      "Batch 22/56\n",
      "Batch Loss: 9.652652740478516\n",
      "Batch 23/56\n",
      "Batch Loss: 7.2159857749938965\n",
      "Batch 24/56\n",
      "Batch Loss: 6.813594341278076\n",
      "Batch 25/56\n",
      "Batch Loss: 15.099072456359863\n",
      "Batch 26/56\n",
      "Batch Loss: 22.01788902282715\n",
      "Batch 27/56\n",
      "Batch Loss: 100.40774536132812\n",
      "Batch 28/56\n",
      "Batch Loss: 92.56669616699219\n",
      "Batch 29/56\n",
      "Batch Loss: 120.31963348388672\n",
      "Batch 30/56\n",
      "Batch Loss: 98.65806579589844\n",
      "Batch 31/56\n",
      "Batch Loss: 56.215660095214844\n",
      "Batch 32/56\n",
      "Batch Loss: 33.52181625366211\n",
      "Batch 33/56\n",
      "Batch Loss: 11.525629043579102\n",
      "Batch 34/56\n",
      "Batch Loss: 12.009845733642578\n",
      "Batch 35/56\n",
      "Batch Loss: 8.598724365234375\n",
      "Batch 36/56\n",
      "Batch Loss: 12.184588432312012\n",
      "Batch 37/56\n",
      "Batch Loss: 13.953932762145996\n",
      "Batch 38/56\n",
      "Batch Loss: 60.66798400878906\n",
      "Batch 39/56\n",
      "Batch Loss: 152.11834716796875\n",
      "Batch 40/56\n",
      "Batch Loss: 214.0675506591797\n",
      "Batch 41/56\n",
      "Batch Loss: 125.47957611083984\n",
      "Batch 42/56\n",
      "Batch Loss: 71.87178039550781\n",
      "Batch 43/56\n",
      "Batch Loss: 40.35309600830078\n",
      "Batch 44/56\n",
      "Batch Loss: 12.564260482788086\n",
      "Batch 45/56\n",
      "Batch Loss: 8.790624618530273\n",
      "Batch 46/56\n",
      "Batch Loss: 12.608901023864746\n",
      "Batch 47/56\n",
      "Batch Loss: 15.854814529418945\n",
      "Batch 48/56\n",
      "Batch Loss: 9.636585235595703\n",
      "Batch 49/56\n",
      "Batch Loss: 53.94776916503906\n",
      "Batch 50/56\n",
      "Batch Loss: 79.3031997680664\n",
      "Batch 51/56\n",
      "Batch Loss: 120.35498809814453\n",
      "Batch 52/56\n",
      "Batch Loss: 199.21075439453125\n",
      "Batch 53/56\n",
      "Batch Loss: 117.6629638671875\n",
      "Batch 54/56\n",
      "Batch Loss: 70.4365234375\n",
      "Batch 55/56\n",
      "Batch Loss: 13.218578338623047\n",
      "Batch 56/56\n",
      "Batch Loss: 4.810120105743408\n",
      "Validation Loss: 55.68459730063166\n",
      "----------Epoch 8/10----------\n",
      "----------Training----------\n",
      "Batch 1/56\n",
      "Batch Loss: 51.0575065612793\n",
      "Batch 2/56\n",
      "Batch Loss: 49.514991760253906\n",
      "Batch 3/56\n",
      "Batch Loss: 51.48660659790039\n",
      "Batch 4/56\n",
      "Batch Loss: 75.75232696533203\n",
      "Batch 5/56\n",
      "Batch Loss: 64.10186767578125\n",
      "Batch 6/56\n",
      "Batch Loss: 38.415184020996094\n",
      "Batch 7/56\n",
      "Batch Loss: 40.556861877441406\n",
      "Batch 8/56\n",
      "Batch Loss: 36.507606506347656\n",
      "Batch 9/56\n",
      "Batch Loss: 62.52206039428711\n",
      "Batch 10/56\n",
      "Batch Loss: 45.71366882324219\n",
      "Batch 11/56\n",
      "Batch Loss: 53.05217742919922\n",
      "Batch 12/56\n",
      "Batch Loss: 73.84527587890625\n",
      "Batch 13/56\n",
      "Batch Loss: 77.01073455810547\n",
      "Batch 14/56\n",
      "Batch Loss: 52.66331481933594\n",
      "Batch 15/56\n",
      "Batch Loss: 49.261634826660156\n",
      "Batch 16/56\n",
      "Batch Loss: 56.6318473815918\n",
      "Batch 17/56\n",
      "Batch Loss: 42.56092071533203\n",
      "Batch 18/56\n",
      "Batch Loss: 64.66112518310547\n",
      "Batch 19/56\n",
      "Batch Loss: 51.79261016845703\n",
      "Batch 20/56\n",
      "Batch Loss: 31.08704948425293\n",
      "Batch 21/56\n",
      "Batch Loss: 75.3407974243164\n",
      "Batch 22/56\n",
      "Batch Loss: 82.57550048828125\n",
      "Batch 23/56\n",
      "Batch Loss: 60.92338562011719\n",
      "Batch 24/56\n",
      "Batch Loss: 51.0473518371582\n",
      "Batch 25/56\n",
      "Batch Loss: 37.94940948486328\n",
      "Batch 26/56\n",
      "Batch Loss: 43.67399215698242\n",
      "Batch 27/56\n",
      "Batch Loss: 51.69147491455078\n",
      "Batch 28/56\n",
      "Batch Loss: 67.49667358398438\n",
      "Batch 29/56\n",
      "Batch Loss: 48.00282669067383\n",
      "Batch 30/56\n",
      "Batch Loss: 65.9917984008789\n",
      "Batch 31/56\n",
      "Batch Loss: 46.53965759277344\n",
      "Batch 32/56\n",
      "Batch Loss: 43.97251510620117\n",
      "Batch 33/56\n",
      "Batch Loss: 34.83881759643555\n",
      "Batch 34/56\n",
      "Batch Loss: 36.62920379638672\n",
      "Batch 35/56\n",
      "Batch Loss: 51.62443542480469\n",
      "Batch 36/56\n",
      "Batch Loss: 76.05819702148438\n",
      "Batch 37/56\n",
      "Batch Loss: 39.6320915222168\n",
      "Batch 38/56\n",
      "Batch Loss: 63.950740814208984\n",
      "Batch 39/56\n",
      "Batch Loss: 60.822967529296875\n",
      "Batch 40/56\n",
      "Batch Loss: 56.547027587890625\n",
      "Batch 41/56\n",
      "Batch Loss: 44.0053596496582\n",
      "Batch 42/56\n",
      "Batch Loss: 47.776771545410156\n",
      "Batch 43/56\n",
      "Batch Loss: 41.94394302368164\n",
      "Batch 44/56\n",
      "Batch Loss: 47.505279541015625\n",
      "Batch 45/56\n",
      "Batch Loss: 46.55398178100586\n",
      "Batch 46/56\n",
      "Batch Loss: 38.981197357177734\n",
      "Batch 47/56\n",
      "Batch Loss: 54.98309326171875\n",
      "Batch 48/56\n",
      "Batch Loss: 51.51983642578125\n",
      "Batch 49/56\n",
      "Batch Loss: 61.519710540771484\n",
      "Batch 50/56\n",
      "Batch Loss: 38.13488006591797\n",
      "Batch 51/56\n",
      "Batch Loss: 61.79827117919922\n",
      "Batch 52/56\n",
      "Batch Loss: 39.72056198120117\n",
      "Batch 53/56\n",
      "Batch Loss: 52.18709945678711\n",
      "Batch 54/56\n",
      "Batch Loss: 42.807830810546875\n",
      "Batch 55/56\n",
      "Batch Loss: 44.58818054199219\n",
      "Batch 56/56\n",
      "Batch Loss: 38.17631149291992\n",
      "Training Loss: 52.066188301358906\n",
      "----------Validation----------\n",
      "Batch 1/56\n",
      "Batch Loss: 8.794331550598145\n",
      "Batch 2/56\n",
      "Batch Loss: 4.6843061447143555\n",
      "Batch 3/56\n",
      "Batch Loss: 27.981578826904297\n",
      "Batch 4/56\n",
      "Batch Loss: 19.93650245666504\n",
      "Batch 5/56\n",
      "Batch Loss: 63.711097717285156\n",
      "Batch 6/56\n",
      "Batch Loss: 91.1696548461914\n",
      "Batch 7/56\n",
      "Batch Loss: 97.97639465332031\n",
      "Batch 8/56\n",
      "Batch Loss: 66.50061798095703\n",
      "Batch 9/56\n",
      "Batch Loss: 27.889781951904297\n",
      "Batch 10/56\n",
      "Batch Loss: 12.781518936157227\n",
      "Batch 11/56\n",
      "Batch Loss: 10.192505836486816\n",
      "Batch 12/56\n",
      "Batch Loss: 12.945215225219727\n",
      "Batch 13/56\n",
      "Batch Loss: 4.868200778961182\n",
      "Batch 14/56\n",
      "Batch Loss: 5.70676851272583\n",
      "Batch 15/56\n",
      "Batch Loss: 39.73882293701172\n",
      "Batch 16/56\n",
      "Batch Loss: 104.89366149902344\n",
      "Batch 17/56\n",
      "Batch Loss: 72.5821533203125\n",
      "Batch 18/56\n",
      "Batch Loss: 123.0416030883789\n",
      "Batch 19/56\n",
      "Batch Loss: 101.00798797607422\n",
      "Batch 20/56\n",
      "Batch Loss: 37.33788299560547\n",
      "Batch 21/56\n",
      "Batch Loss: 12.72934341430664\n",
      "Batch 22/56\n",
      "Batch Loss: 9.150142669677734\n",
      "Batch 23/56\n",
      "Batch Loss: 7.0935516357421875\n",
      "Batch 24/56\n",
      "Batch Loss: 7.611293792724609\n",
      "Batch 25/56\n",
      "Batch Loss: 11.511232376098633\n",
      "Batch 26/56\n",
      "Batch Loss: 15.749139785766602\n",
      "Batch 27/56\n",
      "Batch Loss: 85.81902313232422\n",
      "Batch 28/56\n",
      "Batch Loss: 78.5036392211914\n",
      "Batch 29/56\n",
      "Batch Loss: 103.90209197998047\n",
      "Batch 30/56\n",
      "Batch Loss: 83.67736053466797\n",
      "Batch 31/56\n",
      "Batch Loss: 45.18129348754883\n",
      "Batch 32/56\n",
      "Batch Loss: 25.469440460205078\n",
      "Batch 33/56\n",
      "Batch Loss: 12.500200271606445\n",
      "Batch 34/56\n",
      "Batch Loss: 14.918088912963867\n",
      "Batch 35/56\n",
      "Batch Loss: 11.401142120361328\n",
      "Batch 36/56\n",
      "Batch Loss: 15.579231262207031\n",
      "Batch 37/56\n",
      "Batch Loss: 12.069731712341309\n",
      "Batch 38/56\n",
      "Batch Loss: 49.908363342285156\n",
      "Batch 39/56\n",
      "Batch Loss: 133.76165771484375\n",
      "Batch 40/56\n",
      "Batch Loss: 191.955810546875\n",
      "Batch 41/56\n",
      "Batch Loss: 108.71473693847656\n",
      "Batch 42/56\n",
      "Batch Loss: 59.36530303955078\n",
      "Batch 43/56\n",
      "Batch Loss: 31.598691940307617\n",
      "Batch 44/56\n",
      "Batch Loss: 9.162235260009766\n",
      "Batch 45/56\n",
      "Batch Loss: 8.790908813476562\n",
      "Batch 46/56\n",
      "Batch Loss: 14.440509796142578\n",
      "Batch 47/56\n",
      "Batch Loss: 12.326096534729004\n",
      "Batch 48/56\n",
      "Batch Loss: 6.750271797180176\n",
      "Batch 49/56\n",
      "Batch Loss: 44.6468505859375\n",
      "Batch 50/56\n",
      "Batch Loss: 66.64667510986328\n",
      "Batch 51/56\n",
      "Batch Loss: 104.10984802246094\n",
      "Batch 52/56\n",
      "Batch Loss: 177.79507446289062\n",
      "Batch 53/56\n",
      "Batch Loss: 101.6314468383789\n",
      "Batch 54/56\n",
      "Batch Loss: 58.58189010620117\n",
      "Batch 55/56\n",
      "Batch Loss: 9.747990608215332\n",
      "Batch 56/56\n",
      "Batch Loss: 5.848849296569824\n",
      "Validation Loss: 47.89981687068939\n",
      "----------Epoch 9/10----------\n",
      "----------Training----------\n",
      "Batch 1/56\n",
      "Batch Loss: 26.78122901916504\n",
      "Batch 2/56\n",
      "Batch Loss: 39.89012145996094\n",
      "Batch 3/56\n",
      "Batch Loss: 45.2723274230957\n",
      "Batch 4/56\n",
      "Batch Loss: 26.727787017822266\n",
      "Batch 5/56\n",
      "Batch Loss: 66.78589630126953\n",
      "Batch 6/56\n",
      "Batch Loss: 41.446861267089844\n",
      "Batch 7/56\n",
      "Batch Loss: 56.294673919677734\n",
      "Batch 8/56\n",
      "Batch Loss: 74.75588989257812\n",
      "Batch 9/56\n",
      "Batch Loss: 52.67646789550781\n",
      "Batch 10/56\n",
      "Batch Loss: 64.70589447021484\n",
      "Batch 11/56\n",
      "Batch Loss: 47.164833068847656\n",
      "Batch 12/56\n",
      "Batch Loss: 52.58501434326172\n",
      "Batch 13/56\n",
      "Batch Loss: 52.92236328125\n",
      "Batch 14/56\n",
      "Batch Loss: 28.794015884399414\n",
      "Batch 15/56\n",
      "Batch Loss: 47.325767517089844\n",
      "Batch 16/56\n",
      "Batch Loss: 37.10240173339844\n",
      "Batch 17/56\n",
      "Batch Loss: 56.34790802001953\n",
      "Batch 18/56\n",
      "Batch Loss: 60.25526428222656\n",
      "Batch 19/56\n",
      "Batch Loss: 36.14914321899414\n",
      "Batch 20/56\n",
      "Batch Loss: 35.18439865112305\n",
      "Batch 21/56\n",
      "Batch Loss: 40.256874084472656\n",
      "Batch 22/56\n",
      "Batch Loss: 43.774017333984375\n",
      "Batch 23/56\n",
      "Batch Loss: 61.74735641479492\n",
      "Batch 24/56\n",
      "Batch Loss: 46.31777572631836\n",
      "Batch 25/56\n",
      "Batch Loss: 49.42966079711914\n",
      "Batch 26/56\n",
      "Batch Loss: 26.248138427734375\n",
      "Batch 27/56\n",
      "Batch Loss: 45.006805419921875\n",
      "Batch 28/56\n",
      "Batch Loss: 43.717350006103516\n",
      "Batch 29/56\n",
      "Batch Loss: 50.04574966430664\n",
      "Batch 30/56\n",
      "Batch Loss: 48.85257339477539\n",
      "Batch 31/56\n",
      "Batch Loss: 32.365665435791016\n",
      "Batch 32/56\n",
      "Batch Loss: 53.475563049316406\n",
      "Batch 33/56\n",
      "Batch Loss: 35.755916595458984\n",
      "Batch 34/56\n",
      "Batch Loss: 37.217185974121094\n",
      "Batch 35/56\n",
      "Batch Loss: 60.80318069458008\n",
      "Batch 36/56\n",
      "Batch Loss: 53.130977630615234\n",
      "Batch 37/56\n",
      "Batch Loss: 41.37830352783203\n",
      "Batch 38/56\n",
      "Batch Loss: 43.8311653137207\n",
      "Batch 39/56\n",
      "Batch Loss: 24.456552505493164\n",
      "Batch 40/56\n",
      "Batch Loss: 39.273712158203125\n",
      "Batch 41/56\n",
      "Batch Loss: 38.71163558959961\n",
      "Batch 42/56\n",
      "Batch Loss: 56.50746154785156\n",
      "Batch 43/56\n",
      "Batch Loss: 44.16516876220703\n",
      "Batch 44/56\n",
      "Batch Loss: 43.574729919433594\n",
      "Batch 45/56\n",
      "Batch Loss: 47.95804977416992\n",
      "Batch 46/56\n",
      "Batch Loss: 30.241928100585938\n",
      "Batch 47/56\n",
      "Batch Loss: 45.220054626464844\n",
      "Batch 48/56\n",
      "Batch Loss: 41.40386199951172\n",
      "Batch 49/56\n",
      "Batch Loss: 37.190555572509766\n",
      "Batch 50/56\n",
      "Batch Loss: 38.85167694091797\n",
      "Batch 51/56\n",
      "Batch Loss: 49.95877456665039\n",
      "Batch 52/56\n",
      "Batch Loss: 41.697723388671875\n",
      "Batch 53/56\n",
      "Batch Loss: 38.21237564086914\n",
      "Batch 54/56\n",
      "Batch Loss: 54.57310485839844\n",
      "Batch 55/56\n",
      "Batch Loss: 62.784786224365234\n",
      "Batch 56/56\n",
      "Batch Loss: 28.773954391479492\n",
      "Training Loss: 45.10854687009539\n",
      "----------Validation----------\n",
      "Batch 1/56\n",
      "Batch Loss: 11.8840913772583\n",
      "Batch 2/56\n",
      "Batch Loss: 4.600897312164307\n",
      "Batch 3/56\n",
      "Batch Loss: 22.579988479614258\n",
      "Batch 4/56\n",
      "Batch Loss: 14.931001663208008\n",
      "Batch 5/56\n",
      "Batch Loss: 53.67528533935547\n",
      "Batch 6/56\n",
      "Batch Loss: 78.74042510986328\n",
      "Batch 7/56\n",
      "Batch Loss: 84.72793579101562\n",
      "Batch 8/56\n",
      "Batch Loss: 55.73293685913086\n",
      "Batch 9/56\n",
      "Batch Loss: 21.200117111206055\n",
      "Batch 10/56\n",
      "Batch Loss: 10.627168655395508\n",
      "Batch 11/56\n",
      "Batch Loss: 9.446874618530273\n",
      "Batch 12/56\n",
      "Batch Loss: 15.635334968566895\n",
      "Batch 13/56\n",
      "Batch Loss: 7.668320655822754\n",
      "Batch 14/56\n",
      "Batch Loss: 3.612741470336914\n",
      "Batch 15/56\n",
      "Batch Loss: 33.460330963134766\n",
      "Batch 16/56\n",
      "Batch Loss: 91.83548736572266\n",
      "Batch 17/56\n",
      "Batch Loss: 61.23875045776367\n",
      "Batch 18/56\n",
      "Batch Loss: 108.37866973876953\n",
      "Batch 19/56\n",
      "Batch Loss: 87.44327545166016\n",
      "Batch 20/56\n",
      "Batch Loss: 29.494985580444336\n",
      "Batch 21/56\n",
      "Batch Loss: 11.863953590393066\n",
      "Batch 22/56\n",
      "Batch Loss: 9.769319534301758\n",
      "Batch 23/56\n",
      "Batch Loss: 8.052045822143555\n",
      "Batch 24/56\n",
      "Batch Loss: 9.391244888305664\n",
      "Batch 25/56\n",
      "Batch Loss: 9.375954627990723\n",
      "Batch 26/56\n",
      "Batch Loss: 11.220458030700684\n",
      "Batch 27/56\n",
      "Batch Loss: 73.86263275146484\n",
      "Batch 28/56\n",
      "Batch Loss: 67.01652526855469\n",
      "Batch 29/56\n",
      "Batch Loss: 90.31297302246094\n",
      "Batch 30/56\n",
      "Batch Loss: 71.37100219726562\n",
      "Batch 31/56\n",
      "Batch Loss: 36.39806365966797\n",
      "Batch 32/56\n",
      "Batch Loss: 19.348407745361328\n",
      "Batch 33/56\n",
      "Batch Loss: 14.438055992126465\n",
      "Batch 34/56\n",
      "Batch Loss: 18.58224868774414\n",
      "Batch 35/56\n",
      "Batch Loss: 14.970823287963867\n",
      "Batch 36/56\n",
      "Batch Loss: 19.677627563476562\n",
      "Batch 37/56\n",
      "Batch Loss: 11.455392837524414\n",
      "Batch 38/56\n",
      "Batch Loss: 41.37042236328125\n",
      "Batch 39/56\n",
      "Batch Loss: 118.44136047363281\n",
      "Batch 40/56\n",
      "Batch Loss: 173.28314208984375\n",
      "Batch 41/56\n",
      "Batch Loss: 94.81558990478516\n",
      "Batch 42/56\n",
      "Batch Loss: 49.26782989501953\n",
      "Batch 43/56\n",
      "Batch Loss: 24.850921630859375\n",
      "Batch 44/56\n",
      "Batch Loss: 7.192846298217773\n",
      "Batch 45/56\n",
      "Batch Loss: 9.858960151672363\n",
      "Batch 46/56\n",
      "Batch Loss: 17.143489837646484\n",
      "Batch 47/56\n",
      "Batch Loss: 10.243599891662598\n",
      "Batch 48/56\n",
      "Batch Loss: 5.241289138793945\n",
      "Batch 49/56\n",
      "Batch Loss: 37.41117477416992\n",
      "Batch 50/56\n",
      "Batch Loss: 56.415245056152344\n",
      "Batch 51/56\n",
      "Batch Loss: 90.67465209960938\n",
      "Batch 52/56\n",
      "Batch Loss: 159.74383544921875\n",
      "Batch 53/56\n",
      "Batch Loss: 88.38697052001953\n",
      "Batch 54/56\n",
      "Batch Loss: 49.06635284423828\n",
      "Batch 55/56\n",
      "Batch Loss: 7.7173919677734375\n",
      "Batch 56/56\n",
      "Batch Loss: 7.843982219696045\n",
      "Validation Loss: 42.01768580504826\n",
      "----------Epoch 10/10----------\n",
      "----------Training----------\n",
      "Batch 1/56\n",
      "Batch Loss: 41.08534240722656\n",
      "Batch 2/56\n",
      "Batch Loss: 42.014225006103516\n",
      "Batch 3/56\n",
      "Batch Loss: 49.25020217895508\n",
      "Batch 4/56\n",
      "Batch Loss: 43.07973861694336\n",
      "Batch 5/56\n",
      "Batch Loss: 38.13539123535156\n",
      "Batch 6/56\n",
      "Batch Loss: 40.157875061035156\n",
      "Batch 7/56\n",
      "Batch Loss: 43.2642936706543\n",
      "Batch 8/56\n",
      "Batch Loss: 70.5118408203125\n",
      "Batch 9/56\n",
      "Batch Loss: 47.91309356689453\n",
      "Batch 10/56\n",
      "Batch Loss: 37.776153564453125\n",
      "Batch 11/56\n",
      "Batch Loss: 31.100189208984375\n",
      "Batch 12/56\n",
      "Batch Loss: 56.412227630615234\n",
      "Batch 13/56\n",
      "Batch Loss: 44.493553161621094\n",
      "Batch 14/56\n",
      "Batch Loss: 42.72243881225586\n",
      "Batch 15/56\n",
      "Batch Loss: 30.799774169921875\n",
      "Batch 16/56\n",
      "Batch Loss: 35.352962493896484\n",
      "Batch 17/56\n",
      "Batch Loss: 41.277870178222656\n",
      "Batch 18/56\n",
      "Batch Loss: 54.139747619628906\n",
      "Batch 19/56\n",
      "Batch Loss: 21.482746124267578\n",
      "Batch 20/56\n",
      "Batch Loss: 29.61119842529297\n",
      "Batch 21/56\n",
      "Batch Loss: 23.538755416870117\n",
      "Batch 22/56\n",
      "Batch Loss: 49.759925842285156\n",
      "Batch 23/56\n",
      "Batch Loss: 40.5621452331543\n",
      "Batch 24/56\n",
      "Batch Loss: 57.465023040771484\n",
      "Batch 25/56\n",
      "Batch Loss: 35.3055419921875\n",
      "Batch 26/56\n",
      "Batch Loss: 55.90568923950195\n",
      "Batch 27/56\n",
      "Batch Loss: 32.19529724121094\n",
      "Batch 28/56\n",
      "Batch Loss: 42.68084716796875\n",
      "Batch 29/56\n",
      "Batch Loss: 33.85744857788086\n",
      "Batch 30/56\n",
      "Batch Loss: 53.45376968383789\n",
      "Batch 31/56\n",
      "Batch Loss: 43.380958557128906\n",
      "Batch 32/56\n",
      "Batch Loss: 59.15723419189453\n",
      "Batch 33/56\n",
      "Batch Loss: 48.886680603027344\n",
      "Batch 34/56\n",
      "Batch Loss: 34.44622039794922\n",
      "Batch 35/56\n",
      "Batch Loss: 28.231399536132812\n",
      "Batch 36/56\n",
      "Batch Loss: 38.74781799316406\n",
      "Batch 37/56\n",
      "Batch Loss: 38.70645523071289\n",
      "Batch 38/56\n",
      "Batch Loss: 35.144317626953125\n",
      "Batch 39/56\n",
      "Batch Loss: 48.070735931396484\n",
      "Batch 40/56\n",
      "Batch Loss: 47.25704574584961\n",
      "Batch 41/56\n",
      "Batch Loss: 21.229703903198242\n",
      "Batch 42/56\n",
      "Batch Loss: 22.986406326293945\n",
      "Batch 43/56\n",
      "Batch Loss: 31.79401397705078\n",
      "Batch 44/56\n",
      "Batch Loss: 30.613473892211914\n",
      "Batch 45/56\n",
      "Batch Loss: 21.291946411132812\n",
      "Batch 46/56\n",
      "Batch Loss: 36.516578674316406\n",
      "Batch 47/56\n",
      "Batch Loss: 30.815921783447266\n",
      "Batch 48/56\n",
      "Batch Loss: 30.951351165771484\n",
      "Batch 49/56\n",
      "Batch Loss: 43.6314582824707\n",
      "Batch 50/56\n",
      "Batch Loss: 50.698246002197266\n",
      "Batch 51/56\n",
      "Batch Loss: 53.02534484863281\n",
      "Batch 52/56\n",
      "Batch Loss: 26.72853660583496\n",
      "Batch 53/56\n",
      "Batch Loss: 36.82878112792969\n",
      "Batch 54/56\n",
      "Batch Loss: 42.378414154052734\n",
      "Batch 55/56\n",
      "Batch Loss: 40.63878631591797\n",
      "Batch 56/56\n",
      "Batch Loss: 30.001434326171875\n",
      "Training Loss: 39.954724482127595\n",
      "----------Validation----------\n",
      "Batch 1/56\n",
      "Batch Loss: 15.498664855957031\n",
      "Batch 2/56\n",
      "Batch Loss: 5.378990173339844\n",
      "Batch 3/56\n",
      "Batch Loss: 18.60417938232422\n",
      "Batch 4/56\n",
      "Batch Loss: 11.309257507324219\n",
      "Batch 5/56\n",
      "Batch Loss: 45.55696105957031\n",
      "Batch 6/56\n",
      "Batch Loss: 68.48265075683594\n",
      "Batch 7/56\n",
      "Batch Loss: 73.73783874511719\n",
      "Batch 8/56\n",
      "Batch Loss: 46.96040725708008\n",
      "Batch 9/56\n",
      "Batch Loss: 16.07290267944336\n",
      "Batch 10/56\n",
      "Batch Loss: 9.554055213928223\n",
      "Batch 11/56\n",
      "Batch Loss: 9.633009910583496\n",
      "Batch 12/56\n",
      "Batch Loss: 18.892671585083008\n",
      "Batch 13/56\n",
      "Batch Loss: 11.02398681640625\n",
      "Batch 14/56\n",
      "Batch Loss: 2.5935497283935547\n",
      "Batch 15/56\n",
      "Batch Loss: 28.700664520263672\n",
      "Batch 16/56\n",
      "Batch Loss: 81.01548767089844\n",
      "Batch 17/56\n",
      "Batch Loss: 51.951576232910156\n",
      "Batch 18/56\n",
      "Batch Loss: 96.1241683959961\n",
      "Batch 19/56\n",
      "Batch Loss: 76.17047882080078\n",
      "Batch 20/56\n",
      "Batch Loss: 23.33690071105957\n",
      "Batch 21/56\n",
      "Batch Loss: 11.943037033081055\n",
      "Batch 22/56\n",
      "Batch Loss: 11.17544937133789\n",
      "Batch 23/56\n",
      "Batch Loss: 9.761490821838379\n",
      "Batch 24/56\n",
      "Batch Loss: 11.83498764038086\n",
      "Batch 25/56\n",
      "Batch Loss: 8.319889068603516\n",
      "Batch 26/56\n",
      "Batch Loss: 8.024938583374023\n",
      "Batch 27/56\n",
      "Batch Loss: 64.02751159667969\n",
      "Batch 28/56\n",
      "Batch Loss: 57.60088348388672\n",
      "Batch 29/56\n",
      "Batch Loss: 79.01837921142578\n",
      "Batch 30/56\n",
      "Batch Loss: 61.22304916381836\n",
      "Batch 31/56\n",
      "Batch Loss: 29.399423599243164\n",
      "Batch 32/56\n",
      "Batch Loss: 14.729494094848633\n",
      "Batch 33/56\n",
      "Batch Loss: 17.02294921875\n",
      "Batch 34/56\n",
      "Batch Loss: 22.71027374267578\n",
      "Batch 35/56\n",
      "Batch Loss: 19.0143985748291\n",
      "Batch 36/56\n",
      "Batch Loss: 24.193817138671875\n",
      "Batch 37/56\n",
      "Batch Loss: 11.75888729095459\n",
      "Batch 38/56\n",
      "Batch Loss: 34.59103775024414\n",
      "Batch 39/56\n",
      "Batch Loss: 105.5992660522461\n",
      "Batch 40/56\n",
      "Batch Loss: 157.44439697265625\n",
      "Batch 41/56\n",
      "Batch Loss: 83.24385070800781\n",
      "Batch 42/56\n",
      "Batch Loss: 41.09439468383789\n",
      "Batch 43/56\n",
      "Batch Loss: 19.671768188476562\n",
      "Batch 44/56\n",
      "Batch Loss: 6.285067558288574\n",
      "Batch 45/56\n",
      "Batch Loss: 11.666337966918945\n",
      "Batch 46/56\n",
      "Batch Loss: 20.41232681274414\n",
      "Batch 47/56\n",
      "Batch Loss: 9.234718322753906\n",
      "Batch 48/56\n",
      "Batch Loss: 4.745065689086914\n",
      "Batch 49/56\n",
      "Batch Loss: 31.795881271362305\n",
      "Batch 50/56\n",
      "Batch Loss: 48.1220703125\n",
      "Batch 51/56\n",
      "Batch Loss: 79.51763153076172\n",
      "Batch 52/56\n",
      "Batch Loss: 144.46055603027344\n",
      "Batch 53/56\n",
      "Batch Loss: 77.40045166015625\n",
      "Batch 54/56\n",
      "Batch Loss: 41.413108825683594\n",
      "Batch 55/56\n",
      "Batch Loss: 6.754897594451904\n",
      "Batch 56/56\n",
      "Batch Loss: 10.480074882507324\n",
      "Validation Loss: 37.61232436554773\n",
      "Baseline + feature 1 MSE: 37.61232436554773\n"
     ]
    }
   ],
   "source": [
    "train_model_x(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T02:45:35.065284Z",
     "start_time": "2024-08-15T01:25:23.428319Z"
    }
   },
   "id": "4f18e65fb5ef8194",
   "execution_count": 164
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_baseline_0 = model_x['model_baseline_0']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T03:12:40.613380Z",
     "start_time": "2024-08-15T03:12:40.609876Z"
    }
   },
   "id": "1807d79e68561257",
   "execution_count": 171
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model_x_1(i):\n",
    "    print(f\"\\nTraining baseline + spatial feature {i+1}/{n} model...\")\n",
    "    X_land_single_channel = X_land[:, :, :, i:i+1]\n",
    "\n",
    "    train_dataset_single = TensorDataset(X_land_single_channel, Y_seq, Y_target)\n",
    "    val_dataset_single = TensorDataset(X_land_single_channel, Y_seq, Y_target)\n",
    "\n",
    "    train_loader_single = DataLoader(train_dataset_single, batch_size=32, shuffle=True)\n",
    "    val_loader_single = DataLoader(val_dataset_single, batch_size=32)\n",
    "\n",
    "    results[f\"Baseline + feature {i+1}\"] = train_and_evaluate(model_x[f'model_baseline_{i}'], train_loader_single, val_loader_single)\n",
    "\n",
    "    print(f\"Baseline + feature {i+1} MSE: {results[f'Baseline + feature {i+1}']}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T03:16:10.913263Z",
     "start_time": "2024-08-15T03:16:10.909248Z"
    }
   },
   "id": "2160d8b626d3f1bd",
   "execution_count": 174
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_baseline_1 = LSTMWithSpatialFeatures(seq_length=seq_length, in_channels=1)\n",
    "X_land_single_channel_1 = X_land[:, :, :, 1:2]\n",
    "\n",
    "train_dataset_single_1 = TensorDataset(X_land_single_channel, Y_seq, Y_target)\n",
    "val_dataset_single_1 = TensorDataset(X_land_single_channel, Y_seq, Y_target)\n",
    "\n",
    "train_loader_single_1 = DataLoader(train_dataset_single_1, batch_size=32, shuffle=True)\n",
    "val_loader_single_1 = DataLoader(val_dataset_single_1, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T03:17:22.331234Z",
     "start_time": "2024-08-15T03:17:22.327762Z"
    }
   },
   "id": "4b6931c0b9f450c3",
   "execution_count": 176
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Epoch 1/10----------\n",
      "----------Training----------\n",
      "Batch 1/56\n",
      "Batch Loss: 196.18690490722656\n",
      "Batch 2/56\n",
      "Batch Loss: 206.98638916015625\n",
      "Batch 3/56\n",
      "Batch Loss: 227.02284240722656\n",
      "Batch 4/56\n",
      "Batch Loss: 164.34698486328125\n",
      "Batch 5/56\n",
      "Batch Loss: 188.54409790039062\n",
      "Batch 6/56\n",
      "Batch Loss: 183.74868774414062\n",
      "Batch 7/56\n",
      "Batch Loss: 178.96194458007812\n",
      "Batch 8/56\n",
      "Batch Loss: 181.75555419921875\n",
      "Batch 9/56\n",
      "Batch Loss: 170.31719970703125\n",
      "Batch 10/56\n",
      "Batch Loss: 235.59652709960938\n",
      "Batch 11/56\n",
      "Batch Loss: 175.6417236328125\n",
      "Batch 12/56\n",
      "Batch Loss: 163.25205993652344\n",
      "Batch 13/56\n",
      "Batch Loss: 214.7478790283203\n",
      "Batch 14/56\n",
      "Batch Loss: 205.3195037841797\n",
      "Batch 15/56\n",
      "Batch Loss: 186.35816955566406\n",
      "Batch 16/56\n",
      "Batch Loss: 163.5742950439453\n",
      "Batch 17/56\n",
      "Batch Loss: 199.640625\n",
      "Batch 18/56\n",
      "Batch Loss: 194.17361450195312\n",
      "Batch 19/56\n",
      "Batch Loss: 186.9622802734375\n",
      "Batch 20/56\n",
      "Batch Loss: 174.5210723876953\n",
      "Batch 21/56\n",
      "Batch Loss: 197.6294403076172\n",
      "Batch 22/56\n",
      "Batch Loss: 204.79241943359375\n",
      "Batch 23/56\n",
      "Batch Loss: 164.97987365722656\n",
      "Batch 24/56\n",
      "Batch Loss: 189.73631286621094\n",
      "Batch 25/56\n",
      "Batch Loss: 200.03579711914062\n",
      "Batch 26/56\n",
      "Batch Loss: 177.12722778320312\n",
      "Batch 27/56\n",
      "Batch Loss: 219.47381591796875\n",
      "Batch 28/56\n",
      "Batch Loss: 184.46475219726562\n",
      "Batch 29/56\n",
      "Batch Loss: 137.51239013671875\n",
      "Batch 30/56\n",
      "Batch Loss: 139.00335693359375\n",
      "Batch 31/56\n",
      "Batch Loss: 174.57290649414062\n",
      "Batch 32/56\n",
      "Batch Loss: 185.71734619140625\n",
      "Batch 33/56\n",
      "Batch Loss: 163.78009033203125\n",
      "Batch 34/56\n",
      "Batch Loss: 187.20608520507812\n",
      "Batch 35/56\n",
      "Batch Loss: 181.7770233154297\n",
      "Batch 36/56\n",
      "Batch Loss: 200.0815887451172\n",
      "Batch 37/56\n",
      "Batch Loss: 150.97000122070312\n",
      "Batch 38/56\n",
      "Batch Loss: 182.37168884277344\n",
      "Batch 39/56\n",
      "Batch Loss: 175.39414978027344\n",
      "Batch 40/56\n",
      "Batch Loss: 204.74331665039062\n",
      "Batch 41/56\n",
      "Batch Loss: 169.01361083984375\n",
      "Batch 42/56\n",
      "Batch Loss: 111.64285278320312\n",
      "Batch 43/56\n",
      "Batch Loss: 202.7885284423828\n",
      "Batch 44/56\n",
      "Batch Loss: 174.48342895507812\n",
      "Batch 45/56\n",
      "Batch Loss: 183.7861328125\n",
      "Batch 46/56\n",
      "Batch Loss: 140.63633728027344\n",
      "Batch 47/56\n",
      "Batch Loss: 158.98387145996094\n",
      "Batch 48/56\n",
      "Batch Loss: 162.2182159423828\n",
      "Batch 49/56\n",
      "Batch Loss: 130.45932006835938\n",
      "Batch 50/56\n",
      "Batch Loss: 131.40582275390625\n",
      "Batch 51/56\n",
      "Batch Loss: 152.7286376953125\n",
      "Batch 52/56\n",
      "Batch Loss: 150.337158203125\n",
      "Batch 53/56\n",
      "Batch Loss: 178.22035217285156\n",
      "Batch 54/56\n",
      "Batch Loss: 155.7512664794922\n",
      "Batch 55/56\n",
      "Batch Loss: 195.07691955566406\n",
      "Batch 56/56\n",
      "Batch Loss: 198.6962890625\n",
      "Training Loss: 178.84386934552873\n",
      "----------Validation----------\n",
      "Batch 1/56\n",
      "Batch Loss: 31.307870864868164\n",
      "Batch 2/56\n",
      "Batch Loss: 58.07033157348633\n",
      "Batch 3/56\n",
      "Batch Loss: 133.1094512939453\n",
      "Batch 4/56\n",
      "Batch Loss: 121.210693359375\n",
      "Batch 5/56\n",
      "Batch Loss: 213.9264373779297\n",
      "Batch 6/56\n",
      "Batch Loss: 264.6709899902344\n",
      "Batch 7/56\n",
      "Batch Loss: 279.44830322265625\n",
      "Batch 8/56\n",
      "Batch Loss: 223.83639526367188\n",
      "Batch 9/56\n",
      "Batch Loss: 145.54962158203125\n",
      "Batch 10/56\n",
      "Batch Loss: 86.31623077392578\n",
      "Batch 11/56\n",
      "Batch Loss: 70.02145385742188\n",
      "Batch 12/56\n",
      "Batch Loss: 39.346946716308594\n",
      "Batch 13/56\n",
      "Batch Loss: 30.199703216552734\n",
      "Batch 14/56\n",
      "Batch Loss: 78.65457916259766\n",
      "Batch 15/56\n",
      "Batch Loss: 153.39828491210938\n",
      "Batch 16/56\n",
      "Batch Loss: 284.5142517089844\n",
      "Batch 17/56\n",
      "Batch Loss: 235.51934814453125\n",
      "Batch 18/56\n",
      "Batch Loss: 318.275390625\n",
      "Batch 19/56\n",
      "Batch Loss: 285.5567932128906\n",
      "Batch 20/56\n",
      "Batch Loss: 166.2178192138672\n",
      "Batch 21/56\n",
      "Batch Loss: 73.72344970703125\n",
      "Batch 22/56\n",
      "Batch Loss: 55.70055389404297\n",
      "Batch 23/56\n",
      "Batch Loss: 50.34267044067383\n",
      "Batch 24/56\n",
      "Batch Loss: 42.86825942993164\n",
      "Batch 25/56\n",
      "Batch Loss: 84.8603744506836\n",
      "Batch 26/56\n",
      "Batch Loss: 112.38427734375\n",
      "Batch 27/56\n",
      "Batch Loss: 254.7201385498047\n",
      "Batch 28/56\n",
      "Batch Loss: 242.83901977539062\n",
      "Batch 29/56\n",
      "Batch Loss: 288.688232421875\n",
      "Batch 30/56\n",
      "Batch Loss: 255.9833221435547\n",
      "Batch 31/56\n",
      "Batch Loss: 183.2098846435547\n",
      "Batch 32/56\n",
      "Batch Loss: 137.59690856933594\n",
      "Batch 33/56\n",
      "Batch Loss: 46.22087860107422\n",
      "Batch 34/56\n",
      "Batch Loss: 31.84316635131836\n",
      "Batch 35/56\n",
      "Batch Loss: 29.245403289794922\n",
      "Batch 36/56\n",
      "Batch Loss: 28.27951431274414\n",
      "Batch 37/56\n",
      "Batch Loss: 70.6213150024414\n",
      "Batch 38/56\n",
      "Batch Loss: 185.550537109375\n",
      "Batch 39/56\n",
      "Batch Loss: 335.3908996582031\n",
      "Batch 40/56\n",
      "Batch Loss: 426.2008972167969\n",
      "Batch 41/56\n",
      "Batch Loss: 296.517333984375\n",
      "Batch 42/56\n",
      "Batch Loss: 210.1804656982422\n",
      "Batch 43/56\n",
      "Batch Loss: 149.8238525390625\n",
      "Batch 44/56\n",
      "Batch Loss: 80.89741516113281\n",
      "Batch 45/56\n",
      "Batch Loss: 50.97411346435547\n",
      "Batch 46/56\n",
      "Batch Loss: 40.71709060668945\n",
      "Batch 47/56\n",
      "Batch Loss: 85.16170501708984\n",
      "Batch 48/56\n",
      "Batch Loss: 74.00605010986328\n",
      "Batch 49/56\n",
      "Batch Loss: 167.6189727783203\n",
      "Batch 50/56\n",
      "Batch Loss: 218.76510620117188\n",
      "Batch 51/56\n",
      "Batch Loss: 287.3985290527344\n",
      "Batch 52/56\n",
      "Batch Loss: 405.9942626953125\n",
      "Batch 53/56\n",
      "Batch Loss: 283.06451416015625\n",
      "Batch 54/56\n",
      "Batch Loss: 203.73526000976562\n",
      "Batch 55/56\n",
      "Batch Loss: 82.07870483398438\n",
      "Batch 56/56\n",
      "Batch Loss: 39.012264251708984\n",
      "Validation Loss: 157.7029685633523\n",
      "----------Epoch 2/10----------\n",
      "----------Training----------\n",
      "Batch 1/56\n",
      "Batch Loss: 182.55404663085938\n",
      "Batch 2/56\n",
      "Batch Loss: 171.8626708984375\n",
      "Batch 3/56\n",
      "Batch Loss: 133.55860900878906\n",
      "Batch 4/56\n",
      "Batch Loss: 154.68450927734375\n",
      "Batch 5/56\n",
      "Batch Loss: 160.9255828857422\n",
      "Batch 6/56\n",
      "Batch Loss: 139.2709197998047\n",
      "Batch 7/56\n",
      "Batch Loss: 102.2757797241211\n",
      "Batch 8/56\n",
      "Batch Loss: 96.48945617675781\n",
      "Batch 9/56\n",
      "Batch Loss: 166.0471954345703\n",
      "Batch 10/56\n",
      "Batch Loss: 162.72454833984375\n",
      "Batch 11/56\n",
      "Batch Loss: 145.4103240966797\n",
      "Batch 12/56\n",
      "Batch Loss: 186.5130615234375\n",
      "Batch 13/56\n",
      "Batch Loss: 164.375732421875\n",
      "Batch 14/56\n",
      "Batch Loss: 182.11666870117188\n",
      "Batch 15/56\n",
      "Batch Loss: 143.4959716796875\n",
      "Batch 16/56\n",
      "Batch Loss: 164.28121948242188\n",
      "Batch 17/56\n",
      "Batch Loss: 188.5877227783203\n",
      "Batch 18/56\n",
      "Batch Loss: 143.31105041503906\n",
      "Batch 19/56\n",
      "Batch Loss: 118.35191345214844\n",
      "Batch 20/56\n",
      "Batch Loss: 135.20046997070312\n",
      "Batch 21/56\n",
      "Batch Loss: 136.4885711669922\n",
      "Batch 22/56\n",
      "Batch Loss: 173.39239501953125\n",
      "Batch 23/56\n",
      "Batch Loss: 156.12669372558594\n",
      "Batch 24/56\n",
      "Batch Loss: 162.90318298339844\n",
      "Batch 25/56\n",
      "Batch Loss: 102.19255065917969\n",
      "Batch 26/56\n",
      "Batch Loss: 117.9124984741211\n",
      "Batch 27/56\n",
      "Batch Loss: 134.24002075195312\n",
      "Batch 28/56\n",
      "Batch Loss: 141.23876953125\n",
      "Batch 29/56\n",
      "Batch Loss: 161.0528564453125\n",
      "Batch 30/56\n",
      "Batch Loss: 194.5640106201172\n",
      "Batch 31/56\n",
      "Batch Loss: 144.83688354492188\n",
      "Batch 32/56\n",
      "Batch Loss: 158.9777069091797\n",
      "Batch 33/56\n",
      "Batch Loss: 140.3118133544922\n",
      "Batch 34/56\n",
      "Batch Loss: 157.2427520751953\n",
      "Batch 35/56\n",
      "Batch Loss: 162.7188262939453\n",
      "Batch 36/56\n",
      "Batch Loss: 132.7377166748047\n",
      "Batch 37/56\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m<timed exec>:1\u001B[0m\n",
      "Cell \u001B[1;32mIn[126], line 16\u001B[0m, in \u001B[0;36mtrain_and_evaluate\u001B[1;34m(model, train_loader, val_loader, num_epochs)\u001B[0m\n\u001B[0;32m     14\u001B[0m output \u001B[38;5;241m=\u001B[39m model(land_data, temp_seq)\n\u001B[0;32m     15\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output, target)\n\u001B[1;32m---> 16\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     17\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     18\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    520\u001B[0m     )\n\u001B[1;32m--> 521\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[0;32m    522\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[0;32m    523\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:289\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    284\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    286\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 289\u001B[0m _engine_run_backward(\n\u001B[0;32m    290\u001B[0m     tensors,\n\u001B[0;32m    291\u001B[0m     grad_tensors_,\n\u001B[0;32m    292\u001B[0m     retain_graph,\n\u001B[0;32m    293\u001B[0m     create_graph,\n\u001B[0;32m    294\u001B[0m     inputs,\n\u001B[0;32m    295\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    296\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    297\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\graph.py:768\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    766\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    767\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 768\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    769\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    770\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    771\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    772\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results[f\"Baseline + feature {1}\"] = train_and_evaluate(model_baseline_1, train_loader_single_1, val_loader_single_1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T03:29:41.206510Z",
     "start_time": "2024-08-15T03:17:36.898487Z"
    }
   },
   "id": "7d2f4f2c28f3fa48",
   "execution_count": 177
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_baseline_2 = LSTMWithSpatialFeatures(seq_length=seq_length, in_channels=1)\n",
    "X_land_single_channel_2 = X_land[:, :, :, 1:2]\n",
    "\n",
    "train_dataset_single_2 = TensorDataset(X_land_single_channel, Y_seq, Y_target)\n",
    "val_dataset_single_2 = TensorDataset(X_land_single_channel, Y_seq, Y_target)\n",
    "\n",
    "train_loader_single_2 = DataLoader(train_dataset_single_2, batch_size=32, shuffle=True)\n",
    "val_loader_single_2 = DataLoader(val_dataset_single_2, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79331c6af4e65432",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%time\n",
    "results[f\"Baseline + feature {2}\"] = train_and_evaluate(model_baseline_2, train_loader_single_2, val_loader_single_2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30e73965a1121773",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_baseline_3 = LSTMWithSpatialFeatures(seq_length=seq_length, in_channels=1)\n",
    "X_land_single_channel_3 = X_land[:, :, :, 1:2]\n",
    "\n",
    "train_dataset_single_3 = TensorDataset(X_land_single_channel, Y_seq, Y_target)\n",
    "val_dataset_single_3 = TensorDataset(X_land_single_channel, Y_seq, Y_target)\n",
    "\n",
    "train_loader_single_3 = DataLoader(train_dataset_single_3, batch_size=32, shuffle=True)\n",
    "val_loader_single_3 = DataLoader(val_dataset_single_3, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ed679d409fc8f1f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%time\n",
    "results[f\"Baseline + feature {3}\"] = train_and_evaluate(model_baseline_3, train_loader_single_3, val_loader_single_3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6700913414f114c1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training baseline + spatial feature 2/4 model...\n",
      "----------Epoch 1/10----------\n",
      "----------Training----------\n",
      "Batch 1/56\n",
      "Batch Loss: nan\n",
      "Batch 2/56\n",
      "Batch Loss: nan\n",
      "Batch 3/56\n",
      "Batch Loss: nan\n",
      "Batch 4/56\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m<timed exec>:1\u001B[0m\n",
      "Cell \u001B[1;32mIn[163], line 14\u001B[0m, in \u001B[0;36mtrain_model_x\u001B[1;34m(i)\u001B[0m\n\u001B[0;32m     11\u001B[0m train_loader_single \u001B[38;5;241m=\u001B[39m DataLoader(train_dataset_single, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     12\u001B[0m val_loader_single \u001B[38;5;241m=\u001B[39m DataLoader(val_dataset_single, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m)\n\u001B[1;32m---> 14\u001B[0m results[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBaseline + feature \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m train_and_evaluate(model_x[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_baseline_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m], train_loader_single, val_loader_single)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBaseline + feature \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m MSE: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresults[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBaseline + feature \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[126], line 16\u001B[0m, in \u001B[0;36mtrain_and_evaluate\u001B[1;34m(model, train_loader, val_loader, num_epochs)\u001B[0m\n\u001B[0;32m     14\u001B[0m output \u001B[38;5;241m=\u001B[39m model(land_data, temp_seq)\n\u001B[0;32m     15\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output, target)\n\u001B[1;32m---> 16\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     17\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     18\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    520\u001B[0m     )\n\u001B[1;32m--> 521\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[0;32m    522\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[0;32m    523\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:289\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    284\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    286\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 289\u001B[0m _engine_run_backward(\n\u001B[0;32m    290\u001B[0m     tensors,\n\u001B[0;32m    291\u001B[0m     grad_tensors_,\n\u001B[0;32m    292\u001B[0m     retain_graph,\n\u001B[0;32m    293\u001B[0m     create_graph,\n\u001B[0;32m    294\u001B[0m     inputs,\n\u001B[0;32m    295\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    296\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    297\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\graph.py:768\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    766\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    767\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 768\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    769\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    770\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    771\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    772\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_model_x(1)\n",
    "model_baseline_1 = model_x['model_baseline_1']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T03:12:40.608869Z",
     "start_time": "2024-08-15T03:12:16.947956Z"
    }
   },
   "id": "9cd78236b358bd4b",
   "execution_count": 170
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%time\n",
    "train_model_x(2)\n",
    "model_baseline_2 = model_x['model_baseline_2']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-15T03:11:52.042731Z"
    }
   },
   "id": "66e3cef0faf0baf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%time\n",
    "train_model_x(3)\n",
    "model_baseline_3 = model_x['model_baseline_3']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-15T03:11:52.043955Z"
    }
   },
   "id": "160cbc875c5f90e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Baseline + 单个空间数据\n",
    "# for i in range(n):\n",
    "#     print(f\"\\nTraining baseline + spatial feature {i+1}/{n} model...\")\n",
    "#     model = LSTMWithSpatialFeatures(seq_length=seq_length, in_channels=1)\n",
    "#     X_land_single_channel = X_land[:, :, :, i:i+1]\n",
    "#     train_dataset_single = TensorDataset(X_land_single_channel, Y_seq, Y_target)\n",
    "#     train_loader_single = DataLoader(train_dataset_single, batch_size=32, shuffle=True)\n",
    "#     results[f\"Baseline + feature {i+1}\"] = train_and_evaluate(model, train_loader_single, val_loader)\n",
    "#     print(f\"Baseline + feature {i+1} MSE: {results[f'Baseline + feature {i+1}']}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T03:12:00.522803Z",
     "start_time": "2024-08-15T03:12:00.518805Z"
    }
   },
   "id": "ba7c3e9ea37dcf0",
   "execution_count": 167
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Baseline + 指定的j个空间数据\n",
    "if specific_indices is not None:\n",
    "    print(f\"\\nTraining baseline + specified features {specific_indices} model...\")\n",
    "    \n",
    "    selected_channels = torch.cat([X_land[:, :, :, i:i+1] for i in specific_indices], dim=1)\n",
    "    \n",
    "    model = LSTMWithSpatialFeatures(seq_length=seq_length, in_channels=len(specific_indices))\n",
    "    \n",
    "    \n",
    "    train_dataset_specified = TensorDataset(selected_channels, Y_seq, Y_target)\n",
    "    train_loader_specified = DataLoader(train_dataset_specified, batch_size=32, shuffle=True)\n",
    "    results[f\"Baseline + specified features {specific_indices}\"] = train_and_evaluate(model, train_loader_specified, val_loader)\n",
    "    print(f\"Baseline + specified features {specific_indices} MSE: {results[f'Baseline + specified features {specific_indices}']}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3ec369838a590df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # 比较不同模型的函数\n",
    "# def compare_models(X_land, Y_seq, Y_target, train_loader, val_loader, n, specific_indices=None):\n",
    "#     results = {}\n",
    "# \n",
    "#     # Baseline\n",
    "#     print(\"Training baseline model...\")\n",
    "#     model = LSTMWithSpatialFeatures(seq_length=seq_length, in_channels=0)  # 无空间数据\n",
    "#     results[\"Baseline\"] = train_and_evaluate(model, train_loader, val_loader)\n",
    "#     print(f\"Baseline MSE: {results['Baseline']}\")\n",
    "# \n",
    "#     # Baseline + n种所有空间数据\n",
    "#     print(f\"\\nTraining baseline + all {n} spatial features model...\")\n",
    "#     model = LSTMWithSpatialFeatures(seq_length=seq_length, in_channels=n)\n",
    "#     results[f\"Baseline + all {n} features\"] = train_and_evaluate(model, train_loader, val_loader)\n",
    "#     print(f\"Baseline + all {n} features MSE: {results[f'Baseline + all {n} features']}\")\n",
    "# \n",
    "#     # Baseline + 单个空间数据\n",
    "#     for i in range(n):\n",
    "#         print(f\"\\nTraining baseline + spatial feature {i+1}/{n} model...\")\n",
    "#         model = LSTMWithSpatialFeatures(seq_length=seq_length, in_channels=1)\n",
    "#         X_land_single_channel = X_land[:, i:i+1, :, :]\n",
    "#         train_dataset_single = TensorDataset(X_land_single_channel, Y_seq, Y_target)\n",
    "#         train_loader_single = DataLoader(train_dataset_single, batch_size=32, shuffle=True)\n",
    "#         results[f\"Baseline + feature {i+1}\"] = train_and_evaluate(model, train_loader_single, val_loader)\n",
    "#         print(f\"Baseline + feature {i+1} MSE: {results[f'Baseline + feature {i+1}']}\")\n",
    "# \n",
    "#     # Baseline + 指定的j个空间数据\n",
    "#     if specific_indices is not None:\n",
    "#         print(f\"\\nTraining baseline + specified features {specific_indices} model...\")\n",
    "#         selected_channels = torch.cat([X_land[:, i:i+1, :, :] for i in specific_indices], dim=1)\n",
    "#         model = LSTMWithSpatialFeatures(seq_length=seq_length, in_channels=len(specific_indices))\n",
    "#         train_dataset_specified = TensorDataset(selected_channels, Y_seq, Y_target)\n",
    "#         train_loader_specified = DataLoader(train_dataset_specified, batch_size=32, shuffle=True)\n",
    "#         results[f\"Baseline + specified features {specific_indices}\"] = train_and_evaluate(model, train_loader_specified, val_loader)\n",
    "#         print(f\"Baseline + specified features {specific_indices} MSE: {results[f'Baseline + specified features {specific_indices}']}\")\n",
    "# \n",
    "#     return results\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-14T23:10:02.559191Z",
     "start_time": "2024-08-14T23:10:02.553910Z"
    }
   },
   "id": "9e440e78cd44556f",
   "execution_count": 127
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # 调用比较函数\n",
    "# results = compare_models(X_land, Y_seq, Y_target, train_loader, val_loader, n, specific_indices=[0, 2])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1eaee0e95a501bcd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "691803927ea2d784",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ConvLSTM Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "891711670cebda49"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0513da04df443de",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(400, 400, 5)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T00:13:44.764559Z",
     "start_time": "2024-08-17T00:13:44.759102Z"
    }
   },
   "id": "2f77610f7b57e6c7",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(1795,)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T00:13:48.265839Z",
     "start_time": "2024-08-17T00:13:48.262195Z"
    }
   },
   "id": "ad419c2ac13a789",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# 假设 X_original 是形状为 [400, 400, 5] 的空间数据\n",
    "# 假设 Y_original 是形状为 [1795,] 的温度时间序列数据\n",
    "X_original = X\n",
    "Y_original = Y\n",
    "\n",
    "seq_length = 20  # 设定时间序列的长度\n",
    "\n",
    "# 将时间序列拆分为输入序列和预测值\n",
    "def create_sequences(data, seq_length):\n",
    "    X_seq = []\n",
    "    Y_seq = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X_seq.append(data[i:i+seq_length])\n",
    "        Y_seq.append(data[i+seq_length])\n",
    "    return np.array(X_seq), np.array(Y_seq)\n",
    "\n",
    "# 拆分时间序列数据\n",
    "Y_seq, Y_target = create_sequences(Y_original, seq_length)\n",
    "Y_seq = np.expand_dims(Y_seq, axis=-1)  # [1795-seq_length, seq_length, 1]\n",
    "Y_target = np.expand_dims(Y_target, axis=-1)  # [1795-seq_length, 1]\n",
    "\n",
    "# 转换为 PyTorch 张量\n",
    "X_land = torch.tensor(X_original, dtype=torch.float32).unsqueeze(0).repeat(Y_seq.shape[0], 1, 1, 1)\n",
    "Y_seq = torch.tensor(Y_seq, dtype=torch.float32)\n",
    "Y_target = torch.tensor(Y_target, dtype=torch.float32)\n",
    "\n",
    "# 调整 X_land 的形状以适应 ConvLSTM 的输入格式\n",
    "X_land = X_land.permute(0, 3, 1, 2).unsqueeze(1)  # 变成 (batch_size, seq_length, channels, height, width)\n",
    "\n",
    "# 创建 DataLoader\n",
    "dataset = TensorDataset(X_land, Y_seq, Y_target)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T00:15:55.539429Z",
     "start_time": "2024-08-17T00:15:51.328586Z"
    }
   },
   "id": "b4cf70e66a16b322",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def inspect_first_and_last_batch(dataloader):\n",
    "    \"\"\"\n",
    "    只检查并打印 DataLoader 中第一批和最后一批的数据形状和批大小。\n",
    "    \n",
    "    参数:\n",
    "    dataloader (DataLoader): PyTorch 的 DataLoader 对象。\n",
    "    \n",
    "    返回:\n",
    "    None: 直接打印信息。\n",
    "    \"\"\"\n",
    "    total_batches = len(dataloader)\n",
    "    first_batch = None\n",
    "    last_batch = None\n",
    "    \n",
    "    print(f\"Total Batches: {total_batches}\")\n",
    "\n",
    "    for batch_idx, (X, Y_seq, Y_target) in enumerate(dataloader):\n",
    "        if batch_idx == 0:\n",
    "            first_batch = (X, Y_seq, Y_target)\n",
    "        if batch_idx == total_batches - 1:\n",
    "            last_batch = (X, Y_seq, Y_target)\n",
    "\n",
    "    if first_batch:\n",
    "        X, Y_seq, Y_target = first_batch\n",
    "        print(f\"First Batch (Batch 1):\")\n",
    "        print(f\"Batch size (X): {X.shape[0]}\")\n",
    "        print(f\"X shape: {X.shape}\")\n",
    "        print(f\"Y_seq shape: {Y_seq.shape}\")\n",
    "        print(f\"Y_target shape: {Y_target.shape}\")\n",
    "\n",
    "    if last_batch:\n",
    "        X, Y_seq, Y_target = last_batch\n",
    "        print(f\"\\nLast Batch (Batch {total_batches}):\")\n",
    "        print(f\"Batch size (X): {X.shape[0]}\")\n",
    "        print(f\"X shape: {X.shape}\")\n",
    "        print(f\"Y_seq shape: {Y_seq.shape}\")\n",
    "        print(f\"Y_target shape: {Y_target.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T00:22:32.862448Z",
     "start_time": "2024-08-17T00:22:32.857377Z"
    }
   },
   "id": "f54b4851944d925",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.float32"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtype"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T00:25:46.859549Z",
     "start_time": "2024-08-17T00:25:46.856024Z"
    }
   },
   "id": "db8c05d4996dbc66",
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "dtype('float64')"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.dtype"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T00:25:51.279286Z",
     "start_time": "2024-08-17T00:25:51.275138Z"
    }
   },
   "id": "a51629a122c747d5",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 1, 5, 400, 400])"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T00:26:03.969023Z",
     "start_time": "2024-08-17T00:26:03.965253Z"
    }
   },
   "id": "44aa06122dc72800",
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(1795,)"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T00:26:08.306175Z",
     "start_time": "2024-08-17T00:26:08.302170Z"
    }
   },
   "id": "51ae8030c52e41ce",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 保存 X 张量\n",
    "torch.save(X, 'X_tensor.pt')\n",
    "\n",
    "# 保存 Y 数据\n",
    "Y.to_pickle('Y_series.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T00:28:19.216784Z",
     "start_time": "2024-08-17T00:28:19.112655Z"
    }
   },
   "id": "71803e365fa55457",
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Batches: 45\n",
      "First Batch (Batch 1):\n",
      "Batch size (X): 32\n",
      "X shape: torch.Size([32, 1, 5, 400, 400])\n",
      "Y_seq shape: torch.Size([32, 20, 1])\n",
      "Y_target shape: torch.Size([32, 1])\n",
      "\n",
      "Last Batch (Batch 45):\n",
      "Batch size (X): 12\n",
      "X shape: torch.Size([12, 1, 5, 400, 400])\n",
      "Y_seq shape: torch.Size([12, 20, 1])\n",
      "Y_target shape: torch.Size([12, 1])\n"
     ]
    }
   ],
   "source": [
    "inspect_first_and_last_batch(train_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T00:22:34.371080Z",
     "start_time": "2024-08-17T00:22:33.313261Z"
    }
   },
   "id": "de3b4192bd924e0f",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Batches: 12\n",
      "First Batch (Batch 1):\n",
      "Batch size (X): 32\n",
      "X shape: torch.Size([32, 1, 5, 400, 400])\n",
      "Y_seq shape: torch.Size([32, 20, 1])\n",
      "Y_target shape: torch.Size([32, 1])\n",
      "\n",
      "Last Batch (Batch 12):\n",
      "Batch size (X): 3\n",
      "X shape: torch.Size([3, 1, 5, 400, 400])\n",
      "Y_seq shape: torch.Size([3, 20, 1])\n",
      "Y_target shape: torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "inspect_first_and_last_batch(val_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T00:22:34.774237Z",
     "start_time": "2024-08-17T00:22:34.552214Z"
    }
   },
   "id": "9b4e2418bec4a98e",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = tuple(k // 2 for k in kernel_size)  # 对每个维度分别计算 padding\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, batch_first=False, bias=True):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        if not self.batch_first:\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "        if hidden_state is None:\n",
    "            hidden_state = self._init_hidden(batch_size=b, image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :], cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or isinstance(kernel_size, list)):\n",
    "            raise ValueError('`kernel_size` must be a tuple or list')\n",
    "        if isinstance(kernel_size, list) and len(kernel_size) != len(kernel_size):\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "class ConvLSTMModel(nn.Module):\n",
    "    def __init__(self, seq_length, input_dim, hidden_dim, kernel_size, num_layers):\n",
    "        super(ConvLSTMModel, self).__init__()\n",
    "        self.conv_lstm = ConvLSTM(input_dim=input_dim, hidden_dim=hidden_dim, kernel_size=kernel_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim[-1] * 400 * 400, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c, h, w = x.size()\n",
    "        conv_lstm_out, _ = self.conv_lstm(x)\n",
    "        last_time_step = conv_lstm_out[-1][:, -1, :, :, :]\n",
    "        last_time_step = last_time_step.view(last_time_step.size(0), -1)  # flatten\n",
    "        output = self.fc(last_time_step)\n",
    "        return output\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T00:24:42.661918Z",
     "start_time": "2024-08-17T00:24:42.650902Z"
    }
   },
   "id": "a5f30d5dbbe401b4",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[51], line 30\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# 初始化并训练模型\u001B[39;00m\n\u001B[0;32m     29\u001B[0m conv_lstm_model \u001B[38;5;241m=\u001B[39m ConvLSTMModel(seq_length\u001B[38;5;241m=\u001B[39mseq_length, input_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, hidden_dim\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m64\u001B[39m, \u001B[38;5;241m128\u001B[39m], kernel_size\u001B[38;5;241m=\u001B[39m[(\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m3\u001B[39m), (\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m3\u001B[39m)], num_layers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m---> 30\u001B[0m train_model(conv_lstm_model, train_loader, val_loader, num_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n",
      "Cell \u001B[1;32mIn[51], line 11\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, train_loader, val_loader, num_epochs)\u001B[0m\n\u001B[0;32m      9\u001B[0m X, _, Y \u001B[38;5;241m=\u001B[39m batch\n\u001B[0;32m     10\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 11\u001B[0m output \u001B[38;5;241m=\u001B[39m model(X)\n\u001B[0;32m     12\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output, Y)\n\u001B[0;32m     13\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[50], line 109\u001B[0m, in \u001B[0;36mConvLSTMModel.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    107\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m    108\u001B[0m     b, t, c, h, w \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39msize()\n\u001B[1;32m--> 109\u001B[0m     conv_lstm_out, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv_lstm(x)\n\u001B[0;32m    110\u001B[0m     last_time_step \u001B[38;5;241m=\u001B[39m conv_lstm_out[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :, :, :]\n\u001B[0;32m    111\u001B[0m     last_time_step \u001B[38;5;241m=\u001B[39m last_time_step\u001B[38;5;241m.\u001B[39mview(last_time_step\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# flatten\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[50], line 77\u001B[0m, in \u001B[0;36mConvLSTM.forward\u001B[1;34m(self, input_tensor, hidden_state)\u001B[0m\n\u001B[0;32m     75\u001B[0m output_inner \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(seq_len):\n\u001B[1;32m---> 77\u001B[0m     h, c \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcell_list[layer_idx](input_tensor\u001B[38;5;241m=\u001B[39mcur_layer_input[:, t, :, :, :], cur_state\u001B[38;5;241m=\u001B[39m[h, c])\n\u001B[0;32m     78\u001B[0m     output_inner\u001B[38;5;241m.\u001B[39mappend(h)\n\u001B[0;32m     80\u001B[0m layer_output \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(output_inner, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[50], line 20\u001B[0m, in \u001B[0;36mConvLSTMCell.forward\u001B[1;34m(self, input_tensor, cur_state)\u001B[0m\n\u001B[0;32m     17\u001B[0m h_cur, c_cur \u001B[38;5;241m=\u001B[39m cur_state\n\u001B[0;32m     19\u001B[0m combined \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([input_tensor, h_cur], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# concatenate along channel axis\u001B[39;00m\n\u001B[1;32m---> 20\u001B[0m combined_conv \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv(combined)\n\u001B[0;32m     21\u001B[0m cc_i, cc_f, cc_o, cc_g \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msplit(combined_conv, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_dim, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     22\u001B[0m i \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msigmoid(cc_i)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:458\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    457\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 458\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conv_forward(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:454\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    450\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    451\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    452\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    453\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 454\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(\u001B[38;5;28minput\u001B[39m, weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    455\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=10):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            X, _, Y = batch\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                X, _, Y = batch\n",
    "                output = model(X)\n",
    "                loss = criterion(output, Y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-17T00:25:06.675793Z",
     "start_time": "2024-08-17T00:24:43.196057Z"
    }
   },
   "id": "16b3ad7755eded99",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f97f105bd67b6d39",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
